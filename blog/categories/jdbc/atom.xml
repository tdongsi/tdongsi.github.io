<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Jdbc | Personal Programming Notes]]></title>
  <link href="http://tdongsi.github.io/blog/categories/jdbc/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/"/>
  <updated>2015-12-09T19:06:29-08:00</updated>
  <id>http://tdongsi.github.io/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Programming Hive (Pt. 4): Data Types]]></title>
    <link href="http://tdongsi.github.io/blog/2015/11/26/programming-hive-data-types/"/>
    <updated>2015-11-26T18:01:37-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/11/26/programming-hive-data-types</id>
    <content type="html"><![CDATA[<p><img class="center" src="/images/hive/cat.gif" title="Cover" ></p>

<p>Chapter 3 of the book covers different data types and file formats supported by Hive.</p>

<h3>Data Types</h3>

<p>The following primitive data types are supported:</p>

<ul>
<li>TINYINT: 1 byte signed integer</li>
<li>SMALLINT: 2 bytes</li>
<li>INT: 4 bytes</li>
<li>BIGINT: 8 bytes</li>
<li>BOOLEAN</li>
<li>FLOAT</li>
<li>DOUBLE</li>
<li>STRING: Single or doublbe quotes can be used for literals.</li>
<li>TIMESTAMP: Integer, float, or string.

<ul>
<li>Integer: For seconds from Unix epoch.</li>
<li>Float: Seconds from Unix epoch and nanoseconds.</li>
<li>String: JDBC-compliant java.sql.Timestamp format convention, i.e. YYYY-MM-DD hh:mm:ss.fffffffff</li>
</ul>
</li>
<li>BINARY: array of bytes. Used to include arbitrary bytes and prevent Hive from attempting to parse them.</li>
</ul>


<p>As you can see, Hive supports most basic primitive data types conventionally found in relational databases. Moreover, it helps to remember that these data types are implemented in Java, so their behaviors will be similar to their Java counterparts.</p>

<p>NOTE: Not metioned in the book, but the types <code>DECIMAL</code> and <code>DATE</code> are introduced since Hive 0.13.0. In addition, the book claimed &ldquo;Hive does not support character arrays with maximum-allowed lengths, as is common in other SQL dialects&rdquo; but <code>VARCHAR</code> type, introduced in Hive 0.12.0, does exactly that.</p>

<p>Besides primitive data types, Hive supports the following collection data types:</p>

<ul>
<li>STRUCT: Analogous to a C <code>struct</code> or POJO (Plain Old Java Object). The elements can be accessed using the DOT (.) notation.

<ul>
<li>Example: Declaration <code>struct&lt;name:string,id:int&gt;</code>. Literal <code>struct('John',1)</code>.</li>
</ul>
</li>
<li>MAP: A collection of key-value tuples. The elements can be accessed using array notation, e.g. persons[&lsquo;John&rsquo;].

<ul>
<li>Example: Declaration <code>map&lt;string,int&gt;</code>. Literal <code>map('John',1)</code>.</li>
</ul>
</li>
<li>ARRAY: Ordered sequences of the same type. The elements can be accessed using array notation, e.g. person[2].

<ul>
<li>Example: Declaration <code>array&lt;string&gt;</code>. Literal <code>array('John','Peter')</code>.</li>
</ul>
</li>
</ul>


<p>Relational databases don&rsquo;t usually support such collection types because they tend to break <strong>normal form</strong>. In Hive/Hadoop, sacrificing normal form is pretty common as it can give benefit of higher processing throughput, especially with large amount of data (tens of terarbytes).</p>

<h3>Text File Formats</h3>

<p>Hive can use comma-separated values (CSV) or tab-separated values (TSV) text file format. A Hive table declaration with all row format specified (with default values, however) looks like this:</p>

<pre><code class="sql">CREATE TABLE employees (
  name         STRING,
  salary       FLOAT,
  subordinates ARRAY&lt;STRING&gt;,
  deductions   MAP&lt;STRING, FLOAT&gt;,
  address      STRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\001'
COLLECTION ITEMS TERMINATED BY '\002'
MAP KEYS TERMINATED BY '\003'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
</code></pre>

<h3>Schema on Read</h3>

<p>Different from databases, Hive has no control over the underlying storage: for example, you can modify files on HDFS that Hive will query. Hive tries its best to read the data and match the schema. If the file content does not match the schema such as non-numeric strings found when numbers expected, you may get null values.</p>

<h3>Additional References</h3>

<p>As of November 2015, the book uses slightly a outdated Hive version 0.9.0 (Chapter 2, Installing Hive). Information in the following links are used when writing this post:</p>

<ol>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/Tutorial">https://cwiki.apache.org/confluence/display/Hive/Tutorial</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Programming Hive (Pt. 3): Runtime Modes]]></title>
    <link href="http://tdongsi.github.io/blog/2015/11/24/programming-hive-getting-started/"/>
    <updated>2015-11-24T18:24:30-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/11/24/programming-hive-getting-started</id>
    <content type="html"><![CDATA[<p><img class="center" src="/images/hive/cat.gif" title="Cover" ></p>

<p>In addition to <a href="/blog/2015/11/23/programming-hive-hive-cli/">Hive CLI</a>, chapter 2 of the <strong>Programming Hive</strong> book also covers some lower-level details of Hive such as Hadoop runtime modes and metastore.</p>

<h3>Runtime Modes</h3>

<p>There are different runtime modes for Hadoop. Because Hive uses Hadoop jobs for most of its work, its behavior is dependent on Hadoop runtime mode that you are using. However, even in distributed mode, Hive can decide on a per-query basis if it can perform the query using just local mode to provide better turnaround.</p>

<table>
<thead>
<tr>
<th> Local Mode </th>
<th> Distributed Mode </th>
<th> Pseudodistributed Mode </th>
</tr>
</thead>
<tbody>
<tr>
<td> Filesystem references use local filesystem. </td>
<td> Filesystem referenes use HDFS. </td>
<td> Similar to distributed mode. </td>
</tr>
<tr>
<td> MapReduce tasks in same process. </td>
<td>  MapReduce tasks in separate <br>processes, managed by JobTracker service. </td>
<td> Similar to distributed mode.</td>
</tr>
<tr>
<td> Default mode. </td>
<td> Usually configured for server clusters. </td>
<td> Like a cluster of one node.</td>
</tr>
</tbody>
</table>


<p><br></p>

<p>Pseudodistributed mode is mainly for developers working on personal machines or VM&rsquo;s when testing their applications since local mode doesn’t fully reflect the behavior of a real cluster. Changes to configuration are done by editing the <code>hive-site.xml</code> file in <code>$HIVE_HOME/conf</code> folder (e.g., <code>/usr/lib/hive/conf</code> on Cloudera VM). Create one if it doesn’t already exist.</p>

<h3>Metastore Using JDBC</h3>

<p>Hive requires only one extra component that Hadoop does not already have; the metastore component. The metastore stores metadata such as table schema and partition information that you specify when you run commands such as create table x&hellip;, or alter table y&hellip;, etc. Any JDBC-compliant database can be used for the metastore. In practice, most installations of Hive use MySQL. In <code>hive-site.xml</code> file, the metastore database configuration looks like this:</p>

<pre><code class="xml">  &lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
    &lt;value&gt;jdbc:mysql://127.0.0.1/metastore?createDatabaseIfNotExist=true&lt;/value&gt;
    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
    &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
    &lt;value&gt;hive&lt;/value&gt;
  &lt;/property&gt;
</code></pre>

<p>The information stored in metastore is typically much smaller than the data stored in Hive. Therefore, you typically don’t need a powerful dedicated database server for the metastore. However since it represents a Single Point of Failure (SPOF), it is strongly recommended that you replicate and back up this database using the best practices like any other database instances.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pushing Local Jar File Into Your Local Maven (M2) Repository]]></title>
    <link href="http://tdongsi.github.io/blog/2015/11/17/pushing-local-jar-file-into-your-local-maven-m2-repository/"/>
    <updated>2015-11-17T16:46:49-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/11/17/pushing-local-jar-file-into-your-local-maven-m2-repository</id>
    <content type="html"><![CDATA[<h4>Problem:</h4>

<p>I want to use Vertica JDBC driver in my Eclipse Maven project. I have the jar file from the vendor (i.e., downloaded from HP-Vertica support website) but, obviously, that file is not in Maven central repository. My Maven build will not work without that dependency.</p>

<p>This post will also apply if you are behind a firewall and/or do not have external access for some reason.</p>

<h4>Solution:</h4>

<ul>
<li>Download the jar file (e.g., the Vertica JDBC jar file).</li>
<li>At the same directory as the jar file, run the following command to install the jar to the local Maven repository (running in a different directory seems not work).</li>
</ul>


<pre><code class="bash General Maven command">$ mvn install:install-file -DgroupId=&lt;GROUP_ID&gt; -DartifactId=&lt;ARTIFACT_ID&gt; -Dversion=&lt;VERSION&gt; -Dpackaging=jar -Dfile=&lt;LOCAL_PATH_FOR_JAR&gt; -DgeneratePom=true
</code></pre>

<p>Example:</p>

<pre><code class="bash Example Maven command for Vertica JDBC">$ mvn install:install-file -DgroupId=vertica -DartifactId=vertica-jdbc -Dversion=7.0.1 -Dpackaging=jar -Dfile=~/Downloads/vertica/vertica-jdbc-7.0.1.jar -DgeneratePom=true
</code></pre>

<ul>
<li>Now when you run your maven goals, it will not look for this particular jar file in any external repository such as Maven Central Repository since Maven checks and perceives that it is already in your local repository (your ~/.m2 directory).</li>
</ul>


<p>If you want your Eclipse to start using this jar from your local repository:</p>

<ul>
<li>In Eclipse Luna on a Mac/Windows, go to Navigate > Show View > Other > Maven > Maven Repository.</li>
<li>Open Local Repositories > Local Repository.</li>
<li>Right click for the context menu > Rebuild Index.</li>
</ul>


<p>Now it should show up in “Add…” dialog in pom.xml edit.</p>
]]></content>
  </entry>
  
</feed>
