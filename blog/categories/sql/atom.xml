<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Sql | Personal Programming Notes]]></title>
  <link href="http://tdongsi.github.io/blog/categories/sql/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/"/>
  <updated>2015-12-20T01:38:06-08:00</updated>
  <id>http://tdongsi.github.io/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Vertica Tip: Find Empty Tables]]></title>
    <link href="http://tdongsi.github.io/blog/2015/12/18/vertica-tip-find-empty-tables-in-a-schema/"/>
    <updated>2015-12-18T21:39:56-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/12/18/vertica-tip-find-empty-tables-in-a-schema</id>
    <content type="html"><![CDATA[<p>This post is a reminder of using Vertica&rsquo;s system tables for administrating and monitoring our own tables. One common house-cleaning operation when developing/testing in Vertica is to find and drop tables that are empty (truncated) and never used again.</p>

<p>You might ask why the tables are not dropped directly when I truncated the table in the first place. The answer is that all those tables have some specific designs on projection segmentation and partition, and those information will be lost if I drop the tables. These tables are frequently populated with data and cleared for testing purposes, and truncating and inserting with <code>direct</code> <a href="https://my.vertica.com/docs/7.1.x/HTML/Content/Authoring/SQLReferenceManual/Statements/INSERT.htm">hint</a> will give a significant performance boost (see <a href="/blog/2015/12/16/vertica-tip-best-practices/">Best practices</a>).</p>

<h3>v_monitor schema and COLUMN_STORAGE system table</h3>

<p>The <a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/SystemTables/MONITOR/COLUMN_STORAGE.htm">COLUMN_STORAGE system table</a> in <code>v_monitor</code> schema returns the &ldquo;amount of disk storage used by each column of each projection on each node&rdquo;. Therefore, to get the size of each table, you only need to aggregate the <code>used_byte</code> data, grouped by schema name and table name.</p>

<pre><code class="sql Query to list tables' sizes in a schema">select anchor_table_schema, anchor_table_name, sum(used_bytes)
FROM v_monitor.column_storage
where anchor_table_schema = 'some_schema'
group by anchor_table_schema, anchor_table_name
</code></pre>

<p>According to <a href="http://vertica.tips/2014/01/25/table-size/">here</a>, the number from the above query is the <em>compressed</em> size of the Vertica tables. To get the <em>raw</em> size of the tables, which probably only matters for license limit, perform a <em>license audit</em>, and query the system table <code>license_audits</code> in <code>v_catalog</code> schema. However, the most important takeaway is that empty tables will not appear in this <code>COLUMN_STORAGE</code> system table.</p>

<h3>v_catalog schema and TABLES system table</h3>

<p>The <a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/SystemTables/CATALOG/TABLES.htm">TABLES system table</a> is probably more well-known. It contains all the information about all the tables in all the schemas. For example, to list all the tables in some schema:</p>

<pre><code class="sql Query to list all tables in a schema">select table_schema, table_name from tables
where table_schema = 'some_schema'
</code></pre>

<p>Another useful system table in <code>v_catalog</code> shema is <code>USER_FUNCTIONS</code> which lists all user-defined functions and their function signatures in the database.</p>

<h3>Find all the empty (truncated) tables</h3>

<p>Having all the tables in <code>v_catalog.tables</code> table and only non-empty tables in <code>v_monitor.column_storage</code> table, finding empty tables is pretty straight-forward in SQL:</p>

<pre><code class="sql Query to find empty tables in a schema">select table_name
from v_catalog.tables
where table_schema = 'some_schema'
EXCEPT
select anchor_table_name
from v_monitor.column_storage
where anchor_table_schema = 'some_schema' 
</code></pre>

<h3>External Links</h3>

<ol>
<li><a href="http://vertica.tips/2014/01/25/table-size/">Finding table&rsquo;s compressed size</a></li>
<li><a href="http://vertica.tips/2014/01/24/license-audit-utilization-raw-size/">Vertica License audit</a></li>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/SystemTables/MONITOR/COLUMN_STORAGE.htm">COLUMN_STORAGE system table</a></li>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/SystemTables/CATALOG/TABLES.htm">TABLES system table</a></li>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/SystemTables/CATALOG/USER_FUNCTIONS.htm">USER_FUNCTIONS system table</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vertica Tip: Best Practices]]></title>
    <link href="http://tdongsi.github.io/blog/2015/12/16/vertica-tip-best-practices/"/>
    <updated>2015-12-16T23:12:06-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/12/16/vertica-tip-best-practices</id>
    <content type="html"><![CDATA[<p>This post lists some tips and tricks that I learnt when working with Vertica database.</p>

<h3>General Tips and Tricks</h3>

<h4>CREATE (INSERT)</h4>

<ul>
<li><p>If you want to write data directly to disk and bypass memory, then you should include <code>/*+ direct */</code> as a &ldquo;hint&rdquo; in your <code>INSERT</code> statement. This is especially helpful when you are loading data from big files into Vertica. If you don&rsquo;t use <code>/*+ direct */</code>, then <code>INSERT</code> statement first uses memory, which may be more useful when you want to optimally do inserts and run queries.</p></li>
<li><p>ALWAYS include <code>COMMIT</code> in your SQL statements when you are creating or updating Vertica schemas, because there is NO auto commit in Vertica.</p></li>
<li><p>If you are copying a table, <strong>DO NOT</strong> use <code>CREATE TABLE copy AS SELECT * FROM source</code>. This will give you a copy table with default projections and storage policy. Instead, you should use <code>CREATE TABLE</code> statement with the <a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/AdministratorsGuide/Tables/CreatingATableLikeAnother.htm"><code>LIKE existing_table</code> clause</a> and use <code>INSERT /*+ direct */</code> statement. Creating a table with <code>LIKE</code> option replicates the table definition and storage policy associated with the source table, which can make significant difference in data loading performance. Note that the <code>LIKE</code> clause does not work if the existing source table is a temporary table.</p></li>
</ul>


<pre><code class="sql DO NOT do this">create table to_schema.to_table_name
as select * from from_schema.from_table_name;
</code></pre>

<pre><code class="sql DO this">CREATE TABLE to_schema.to_table_name LIKE from_schema.from_table_name INCLUDING PROJECTIONS;
INSERT /*+ direct */ INTO to_schema.to_table_name SELECT * from from_schema.from_table_name;
</code></pre>

<ul>
<li>Before making a copy of a table, be sure to consider alternatives in order to execute optimal queries: create views, rewrite queries, use sub-queries, limit queries to only a subset of data for analysis.</li>
</ul>


<h4>READ</h4>

<ul>
<li><p>Avoid joining large tables (e.g., > 50M records). Run a <code>count(*)</code> on tables before joining and use <code>MERGE JOIN</code> to optimally join tables. When you use smaller subsets of data, the Vertica Optimizer will pick the <code>MERGE JOIN</code> algorithm instead of the <code>HASH JOIN</code> one, which is less optimal.</p></li>
<li><p>When an approximate value will be enough, Vertica offers an alternative to <code>COUNT(DISTINCT)</code>: <code>APPROXIMATE_COUNT_DISTINCT</code>. This function is recommended when you have a large data set and you do not require an exact count of distinct values: e.g., sanity checks that verify the tables are populated. According to <a href="http://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/AnalyzingData/Optimizations/OptimizingCOUNTDISTINCTByCalculatingApproximateCounts.htm">this documentation</a>, you can get much better performance than <code>COUNT(DISTINCT)</code>. <a href="http://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/Functions/Aggregate/APPROXIMATE_COUNT_DISTINCT.htm">Here</a> is an example of the <code>APPROXIMATE_COUNT_DISTINCT</code> syntax that you should use.</p></li>
</ul>


<h4>UPDATE &amp; DELETE</h4>

<ul>
<li><p>Deletes and updates take exclusive locks on the table. Hence, only one <code>DELETE</code> or <code>UPDATE</code> transaction on that table can be in progress at a time and only when no <code>INSERTs</code> are in progress. Deletes and updates on different tables can be run concurrently.</p></li>
<li><p>Try to avoid <code>DELETE</code> or <code>UPDATE</code> as much as you can, especially on shared Vertica databases. Instead, it may work better to move the data you want to update to a new temporary table, work on that copy, drop the original table, and rename the temporary table with the original table name. For example:</p></li>
</ul>


<pre><code class="sql">CREATE temp_table LIKE src_table INCLUDING PROJECTIONS;
INSERT INTO temp_table (SELECT statement based on the updated data or the needed rows);
DROP TABLE src_table;
ALTER TABLE temp_table RENAME TO src_table;
</code></pre>

<ul>
<li>Delete from tables marks rows with delete vectors and stores them so data can be rolled back to a previous epoch. The data must be eventually purged before the database can reclaim disk space.</li>
</ul>


<h3>Query plan</h3>

<p>A query plan is a sequence of step-like paths that the HP Vertica cost-based query optimizer selects to access or alter information in your HP Vertica database. You can get information about query plans by prefixing the SQL query with the <code>EXPLAIN</code> command.</p>

<pre><code class="sql EXPLAIN statement">EXPLAIN SELECT customer_name, customer_state FROM customer_dimension
WHERE customer_state in ('MA','NH') AND customer_gender = 'Male'     
ORDER BY customer_name LIMIT 10;
</code></pre>

<p>The output from a query plan is presented in a tree-like structure, where each step path represents a single operation in the database that the optimizer uses for its execution strategy. The following example output is based on the previous query:</p>

<pre><code class="bash Query Plan description">EXPLAIN SELECT
customer_name,
customer_state
FROM customer_dimension
WHERE customer_state in ('MA','NH')
AND customer_gender = 'Male'
ORDER BY customer_name
LIMIT 10;
Access Path:
+-SELECT  LIMIT 10 [Cost: 370, Rows: 10] (PATH ID: 0)
|  Output Only: 10 tuples
|  Execute on: Query Initiator
| +---&gt; SORT [Cost: 370, Rows: 544] (PATH ID: 1)
| |      Order: customer_dimension.customer_name ASC
| |      Output Only: 10 tuples
| |      Execute on: Query Initiator
| | +---&gt; STORAGE ACCESS for customer_dimension [Cost: 331, Rows: 544] (PATH ID: 2) 
| | |      Projection: public.customer_dimension_DBD_1_rep_vmartdb_design_vmartdb_design_node0001
| | |      Materialize: customer_dimension.customer_state, customer_dimension.customer_name
| | |      Filter: (customer_dimension.customer_gender = 'Male')
| | |      Filter: (customer_dimension.customer_state = ANY (ARRAY['MA', 'NH']))
| | |      Execute on: Query Initiator
</code></pre>

<p>If you want to understand the details of the query plan, observe the real-time flow of data through the plan to identify possible query bottlenecks, you can:</p>

<ol>
<li>query the <a href="https://my.vertica.com/docs/7.1.x/HTML/Content/Authoring/SQLReferenceManual/SystemTables/MONITOR/QUERY_PLAN_PROFILES.htm">V_MONITOR.QUERY_PLAN_PROFILES</a> system table.</li>
<li>review <a href="https://my.vertica.com/docs/7.1.x/HTML/Content/Authoring/AdministratorsGuide/Profiling/ProfilingQueryPlanProfiles.htm">Profiling Query Plans</a>.</li>
<li>use <a href="https://my.vertica.com/docs/7.1.x/HTML/Content/Authoring/SQLReferenceManual/Statements/PROFILE.htm">PROFILE</a> statement to view further detailed analysis of your query.</li>
</ol>


<h3>External Links</h3>

<ol>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm">Vertica documentation</a></li>
<li><a href="http://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/Functions/Aggregate/APPROXIMATE_COUNT_DISTINCT.htm">APPROXIMATE_COUNT_DISTINCT</a></li>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/AdministratorsGuide/Tables/CreatingATableLikeAnother.htm">Create a Table Like Another</a></li>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/Content/Authoring/SQLReferenceManual/SystemTables/MONITOR/QUERY_PLAN_PROFILES.htm">V_MONITOR.QUERY_PLAN_PROFILES</a> system table.</li>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/Content/Authoring/AdministratorsGuide/Profiling/ProfilingQueryPlanProfiles.htm">Profiling Query Plans</a>.</li>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/Content/Authoring/SQLReferenceManual/Statements/PROFILE.htm">PROFILE</a> statement.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Some NZSQL Tips for New Netezza Users]]></title>
    <link href="http://tdongsi.github.io/blog/2015/12/07/some-netezzas-nzsql-tips/"/>
    <updated>2015-12-07T11:11:06-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/12/07/some-netezzas-nzsql-tips</id>
    <content type="html"><![CDATA[<ul>
<li>By default, identifiers are treated as UPPERCASE, even if you type them as LOWERCASE. So, for example, these create statements:</li>
</ul>


<pre><code class="sql">create table my_table (name varchar(80), address varchar(80));
create table my_table (NaMe varchar(80), adDresS varchar(80));
</code></pre>

<p>  are equivalent to:</p>

<pre><code class="sql">create table my_table (NAME varchar(80), ADDRESS varchar(80));
</code></pre>

<p>  The same is true for <code>SELECT</code> statements. These two SQL statements:</p>

<pre><code class="sql">select name from my_table;
select NaMe from my_table;
</code></pre>

<p>  are equivalent to:</p>

<pre><code class="sql">select NAME from my_table;
</code></pre>

<ul>
<li>The best practice is that you should never care or override the above default behavior: your identifiers should be case-insensitive. Unfortunately, if you have to override the above default behavior, then you must surround the identifier with double-quotes whenever you reference it. For example, if you create a table using this statement:</li>
</ul>


<pre><code class="sql">create table my_table ("Name" varchar(80), "Address" varchar(80));
</code></pre>

<p>then you must reference the identifiers by surrounding them with double-quotes. For example:</p>

<pre><code class="sql">select "Name" from my_table;
</code></pre>

<ul>
<li>The most perplexing feature for new Netezza users when reading a NZSQL script is probably the &ldquo;dot dot&rdquo; notation of database object names, i.e., the two dots in <code>my_dwh..companies</code>. It is simply the short-hand notation for database object names, <code>database-name..object-name</code>. The fully qualified form of object names in Netezza has <strong>three-level</strong> as <code>database-name.schema.object-name</code>. One example of using such notation is shown below:</li>
</ul>


<pre><code class="sql">select count(*) from (select company_name from my_dwh..companies where company_name like '%e%') as x;
</code></pre>

<h4>External Links</h4>

<ol>
<li><a href="https://www-304.ibm.com/support/knowledgecenter/SSULQD_7.2.0/com.ibm.nz.dbu.doc/c_dbuser_database_object_naming.html">Database Object Naming</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Programming Hive (Pt. 7): Partitioned Tables]]></title>
    <link href="http://tdongsi.github.io/blog/2015/12/06/programming-hive-partitioned-tables/"/>
    <updated>2015-12-06T21:09:51-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/12/06/programming-hive-partitioned-tables</id>
    <content type="html"><![CDATA[<p><img class="center" src="/images/hive/cat.gif" title="Cover" ></p>

<p>Chapter 4 of the book, continued from the <a href="/blog/2015/12/02/programming-hive-hiveql-ddl/">previous</a> <a href="/blog/2015/12/05/programming-hive-ddl-table/">posts</a>.</p>

<h3>Partitioned Managed Tables</h3>

<p>In general, paritioning data means distributing data load horizontally, moving data physically closer to its most frequent users. In Hive, partitioning tables changes how Hive structures its data storage for some performance gain.</p>

<p>The book presents a hypothetical problem where one will regularly query some <code>employees</code> table by country and state, e.g., all employees in California, US or Alberta, Canada. Therefore, partitioning this table by country and state is a logical thing to do.</p>

<pre><code class="sql">CREATE TABLE employees (
  name         STRING,
  salary       FLOAT,
  subordinates ARRAY&lt;STRING&gt;,
  deductions   MAP&lt;STRING, FLOAT&gt;,
  address      STRUCT&lt;street:STRING, city:STRING, state:STRING, zip:INT&gt;
)
PARTITIONED BY (country STRING, state STRING);
</code></pre>

<p>Without <code>PARTITIONED BY</code> clause, Hive will store data for these tables in a subdirectory <code>employees</code> under the directory defined by <code>hive.metastore.warehouse.dir</code> (see <a href="/blog/2015/12/05/programming-hive-ddl-table/">Managed tables</a>). However, Hive will now create subdirectories inside <code>employees</code> directory for the above partitioning structure:</p>

<pre><code>...
.../employees/country=CA/state=AB
.../employees/country=CA/state=BC
...
.../employees/country=US/state=AL
.../employees/country=US/state=AK
...
</code></pre>

<p>The actual directory names depends on values of <em>partition keys</em> (e.g., country and state). For very large data sets, partitioning can improve query performance, but only if the partitioning scheme reflects common range filtering (e.g., by countries or states). When we add predicates to WHERE clauses that filter on partition values, these predicates are called <em>partition filters</em> (e.g., <code>WHERE state = 'CA'</code>).</p>

<p>You can view the partitions in a table with <code>SHOW PARTITIONS</code>, as shown in examples below:</p>

<pre><code>SHOW PARTITIONS college;

/* DESCRIBE EXTENDED also shows partition keys */
SHOW PARTITIONS employees PARTITION( country=‘US’);
</code></pre>

<h4>Strict mode</h4>

<p>Given a partitioned table, a query across all partitions can result in a enormous MapReduce job, especially for a huge data set. It is probably desirable to put in place a safety measure which prohibits queries without any filter on partitions. Hive has a &ldquo;strict&rdquo; mode for that.</p>

<pre><code>hive&gt; set hive.mapred.mode;
hive.mapred.mode=nonstrict

hive&gt; set hive.mapred.mode = strict;
hive&gt; SELECT e.name FROM employees e; /* does not work */
</code></pre>

<h3>Partitioned External Tables</h3>

<p>You can use partitioning with external tables. The combination gives you a way to “share” data with other tools, while still optimizing query performance. While LOCATION clause is required for non-partitioned external table to specify data location, it is not required for external partitioned tables. Instead, <code>ALTER TABLE</code> statement is used to add data in each partition separately.</p>

<pre><code class="sql">CREATE EXTERNAL TABLE IF NOT EXISTS log_messages (
  hms             INT,
  severity        STRING,
  server          STRING,
  process_id      INT,
  message         STRING)
PARTITIONED BY (year INT, month INT, day INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

ALTER TABLE log_messages ADD PARTITION(year = 2012, month = 1, day = 2)
LOCATION 'hdfs://master_server/data/log_messages/2012/01/02';

DESCRIBE EXTENDED log_messages PARTITION (year=2012);
</code></pre>

<p>Note that <code>ALTER TABLE … ADD PARTITION</code> is not limited to external tables. You can use it with managed tables, too. However, it is not recommended since you have to manually keep track of this partition and remember to delete data in case you want to completely drop the managed table.</p>

<h4>Example use case of partitioned external tables</h4>

<p>For example, each day we might use the following procedure to move data older than a month to S3:</p>

<p>1) Copy the data for the partition being moved to S3. For example, you can use the hadoop distcp command:
<code>
     hadoop distcp /data/log_messages/2011/12/02 s3n://ourbucket/logs/2011/12/02
</code>
2) Alter the table to point the partition to the S3 location:
<code>
     ALTER TABLE log_messages PARTITION(year = 2011, month = 12, day = 2)
     SET LOCATION 's3n://ourbucket/logs/2011/01/02';
</code>
3) Remove the HDFS copy of the partition using the hadoop fs -rmr command:
<code>
     hadoop fs -rmr /data/log_messages/2011/01/02
</code></p>

<h3>Altering Partitioned Tables</h3>

<p>Some basic <code>ALTER TABLE</code> statements for manipulating table partitions are shown in the following examples:</p>

<pre><code>/* Add partition */
ALTER TABLE log_messages ADD IF NOT EXISTS
PARTITION (year = 2011, month = 1)
LOCATION ‘/logs/2011/01';

/* Change partition location */
ALTER TABLE log_messages PARTITION (year = 2011, month = 1)
SET LOCATION ‘/bucket/logs/2011/01’;

/* Drop partition */
ALTER TABLE log_messages DROP IF EXISTS PARTITION (year = 2011, month = 1);

/* Alter storage properties of partition */
ALTER TABLE log_messages
PARTITION(year = 2012, month = 1, day = 1)
SET FILEFORMAT SEQUENCEFILE;

/* Archive partition */
ALTER TABLE log_messages ARCHIVE
PARTITION(year = 2012, month = 1, day = 1);
</code></pre>

<p>The <code>ALTER TABLE ... ARCHIVE PARTITION</code> statement captures the partition files into a Hadoop archive (HAR) file. This only reduces the number of files in the filesystem, reducing the load on the NameNode, but doesn’t provide any space savings. To reverse the operation, substitute UNARCHIVE for ARCHIVE. This feature is only available for individual partitions of partitioned tables.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Programming Hive (Pt. 6): HiveQL Data Definition]]></title>
    <link href="http://tdongsi.github.io/blog/2015/12/05/programming-hive-ddl-table/"/>
    <updated>2015-12-05T20:15:56-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/12/05/programming-hive-ddl-table</id>
    <content type="html"><![CDATA[<p><img class="center" src="/images/hive/cat.gif" title="Cover" ></p>

<p>Chapter 4 of the book, continued from the previous <a href="/blog/2015/12/02/programming-hive-hiveql-ddl/">post</a>.</p>

<h3>Creating Tables</h3>

<p>Some basic HiveQL&rsquo;s table DDL commands are shown in the following examples:</p>

<pre><code class="sql">/* NOTE: the LOCATION clause uses the default location */
CREATE TABLE IF NOT EXISTS college.student (
  name STRING COMMENT 'Student name',
  sid INT COMMENT 'Student ID',
)
COMMENT 'Description of the table' TBLPROPERTIES ( 'creator' = 'me' )
LOCATION '/user/hive/warehouse/college.db/student';

/* 
 * copy the schema of an existing table
 * you can specify optional LOCATION but no other can be defined
 */
CREATE TABLE IF NOT EXISTS mydb.clone LIKE mydb.employees;

/*
* Create external table
* Read all data files with comma-delimited format
* from /data/stocks
* LOCATION is required for external table
*/
CREATE EXTERNAL TABLE IF NOT EXISTS stocks (
  exchange        STRING,
  symbol          STRING,
  volume          INT,
  price_adj_close FLOAT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/data/stocks';

/*
* Copy external table schema.
*/
CREATE EXTERNAL TABLE IF NOT EXISTS ext_clone
LIKE stocks
LOCATION '/path/to/data';

/*
* Drop table
* For managed tables, the table metadata and data are deleted.
* For external tables, the metadata is deleted but the data is not.
*/
DROP TABLE IF EXISTS college;
</code></pre>

<p>Note that in the first <code>CREATE TABLE</code> command, you can prefix a database name, e.g. <code>mydb</code>, even when it is not your current working database. As usual, the optional <code>IF NOT EXISTS</code> clause will ignore the statement if the table already exists, even when the schema does not match (no warning from Hive). The second <code>CREATE TABLE</code> command is useful to copy the schema of an existing table. The corresponding commands for <strong>external</strong> table are also shown above (note <code>EXTERNAL TABLE</code>). The concept of external table in Hive will be discussed shortly.</p>

<p>The <code>SHOW TABLES</code> command lists the tables. You use different variants of that command to find tables of interest as shown below:</p>

<pre><code>hive&gt; use college;
OK
Time taken: 0.048 seconds


/* Show list of tables in current database */
hive&gt; show tables;
OK
apply
college
student
Time taken: 0.031 seconds, Fetched: 3 row(s)

/* Show list of tables in the specified database */
hive&gt; show tables in college;
OK
apply
college
student
Time taken: 0.034 seconds, Fetched: 3 row(s)

/* use regex to search tables in current database */
hive&gt; show tables '.*e.*';
OK
college
student
Time taken: 0.025 seconds, Fetched: 2 row(s)


/* Show table properties */
hive&gt; show tblproperties student;        
OK
COLUMN_STATS_ACCURATE   true
comment List of students
numFiles    1
numRows 0
rawDataSize 0
totalSize   213
transient_lastDdlTime   1421796179
Time taken: 0.28 seconds, Fetched: 7 row(s)
</code></pre>

<p>You can use <code>DESCRIBE</code> command to display table information as shown below:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hive&gt; describe extended student;      &lt;br/&gt;
</span><span class='line'>OK
</span><span class='line'>sid                     int                     Student ID        &lt;br/&gt;
</span><span class='line'>sname                   string                  Student name      &lt;br/&gt;
</span><span class='line'>gpa                     float                   Student GPA       &lt;br/&gt;
</span><span class='line'>sizehs                  int                     Size of student highschool&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Detailed Table Information  Table(tableName:student, dbName:college, owner:cloudera, createTime:1421796178, lastAccessTime:0, retention:0,
</span><span class='line'>sd:StorageDescriptor(cols:[FieldSchema(name:sid, type:int, comment:Student ID), FieldSchema(name:sname, type:string, comment:Student name), &hellip;
</span><span class='line'>Time taken: 0.119 seconds, Fetched: 6 row(s)&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;/&lt;em&gt; more readable and verbose &lt;/em&gt;/
</span><span class='line'>hive&gt; describe formatted student;
</span><span class='line'>OK&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;col_name              data_type               comment&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;sid                     int                     Student ID        &lt;br/&gt;
</span><span class='line'>sname                   string                  Student name      &lt;br/&gt;
</span><span class='line'>gpa                     float                   Student GPA       &lt;br/&gt;
</span><span class='line'>sizehs                  int                     Size of student highschool&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Detailed Table Information&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Database:               college                &lt;br/&gt;
</span><span class='line'>Owner:                  cloudera               &lt;br/&gt;
</span><span class='line'>CreateTime:             Tue Jan 20 15:22:58 PST 2015   &lt;br/&gt;
</span><span class='line'>LastAccessTime:         UNKNOWN                &lt;br/&gt;
</span><span class='line'>Protect Mode:           None                   &lt;br/&gt;
</span><span class='line'>Retention:              0                      &lt;br/&gt;
</span><span class='line'>Location:               hdfs://quickstart.cloudera:8020/user/hive/warehouse/college.db/student &lt;br/&gt;
</span><span class='line'>Table Type:             MANAGED_TABLE          &lt;br/&gt;
</span><span class='line'>Table Parameters:      &lt;br/&gt;
</span><span class='line'>    COLUMN_STATS_ACCURATE   true              &lt;br/&gt;
</span><span class='line'>    comment                 List of students  &lt;br/&gt;
</span><span class='line'>    numFiles                1                 &lt;br/&gt;
</span><span class='line'>    numRows                 0                 &lt;br/&gt;
</span><span class='line'>    rawDataSize             0                 &lt;br/&gt;
</span><span class='line'>    totalSize               213               &lt;br/&gt;
</span><span class='line'>    transient_lastDdlTime   1421796179&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Storage Information&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe &lt;br/&gt;
</span><span class='line'>InputFormat:            org.apache.hadoop.mapred.TextInputFormat   &lt;br/&gt;
</span><span class='line'>OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat &lt;br/&gt;
</span><span class='line'>Compressed:             No                     &lt;br/&gt;
</span><span class='line'>Num Buckets:            -1                     &lt;br/&gt;
</span><span class='line'>Bucket Columns:         []                     &lt;br/&gt;
</span><span class='line'>Sort Columns:           []                     &lt;br/&gt;
</span><span class='line'>Storage Desc Params:       &lt;br/&gt;
</span><span class='line'>    field.delim             ;                 &lt;br/&gt;
</span><span class='line'>    serialization.format    ;                 &lt;br/&gt;
</span><span class='line'>Time taken: 0.108 seconds, Fetched: 36 row(s)&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;/&lt;em&gt; see schema for a column &lt;/em&gt;/
</span><span class='line'>hive&gt; describe student.sid;
</span><span class='line'>OK
</span><span class='line'>sid                     int                     from deserializer &lt;br/&gt;
</span><span class='line'>Time taken: 0.315 seconds, Fetched: 1 row(s)</span></code></pre></td></tr></table></div></figure></p>

<h3>Managed tables vs External tables</h3>

<p><code>CREATE TABLE</code> commands (without <code>EXTERNAL</code>) create <em>managed tables</em> or <em>internal tables</em>. It is internal/managed because the life cycle of their data is managed by Hive. By default, Hive stores data for these tables in a subdirectory under the directory defined by <code>hive.metastore.warehouse.dir</code>, as illustrated below (see <a href="/blog/2015/11/23/programming-hive-hive-cli/">Hive CLI</a> for <code>SET</code> and <code>dfs</code> commands). When we drop a mananged table with <code>DROP TABLE</code> command, the data in the table will be deleted.</p>

<pre><code>hive&gt; SET hive.metastore.warehouse.dir;
hive.metastore.warehouse.dir=/user/hive/warehouse
hive&gt; dfs -ls /user/hive/warehouse/college.db;
Found 3 items
drwxrwxrwx   - hive hive          0 2015-01-21 11:29 /user/hive/warehouse/college.db/apply
drwxrwxrwx   - hive hive          0 2015-12-03 15:16 /user/hive/warehouse/college.db/college
drwxrwxrwx   - hive hive          0 2015-01-28 15:26 /user/hive/warehouse/college.db/student
</code></pre>

<p>As mentioned in <a href="/blog/2015/11/26/programming-hive-data-types/">Schema on Read</a>, Hive does not have control over the underlying storage, even for <em>managed table</em>: for example, you can totally use another <code>dfs</code> command in the last example to modify files on HDFS.</p>

<p>Managed tables are not convenient for sharing data with other tools. Instead, <em>external tables</em> can be defined to point to that data, but don&rsquo;t take ownership of data. In the <code>CREATE EXTERNAL TABLE</code> command example at the beginning of this post, the data files are in HDFS at <code>/data/stocks</code> and the external table will be created and populated by reading all comma-delimited data files in that location. The <code>LOCATION</code> clause is required for external table, to tell Hive where it is located. Dropping an external table does not delete the data since Hive does not <em>own</em> the data. However, the <em>metadata</em> for that table will be deleted.</p>

<p>To tell whether if a table is managed or external, use the command <code>DESCRIBE FORMATTED</code>. In the example in the last section, we see that the table <code>college.student</code> is a managed table because of its output:</p>

<pre><code>Location:               hdfs://quickstart.cloudera:8020/user/hive/warehouse/college.db/student   
Table Type:             MANAGED_TABLE 
</code></pre>

<p>For external tables, the output will be like <code>Table Type: EXTERNAL_TABLE</code>.</p>

<h3>Altering Tables</h3>

<p>The <code>ALTER TABLE</code> statements <em>only</em> change <em>metadata</em> of the table, but not the data in the table. It&rsquo;s up to us to ensure that any schema modifications are consistent with the actual data.</p>

<p>Some basic <code>ALTER TABLE</code> statements for renaming table and changing table columns are shown in the following examples:</p>

<pre><code>/* Renaming table */
ALTER TABLE college RENAME TO university;

/*
 * Change columns: rename, change its position, type, or comment.
 * The keyword COLUMN is optional, as well as COMMENT clause.
 * This command changes metadata only. 
 * The data has to be moved to match the new columns if needed.
 * Use FIRST, instead of AFTER, if the column is moved to first.
 */
ALTER TABLE log_messages
CHANGE COLUMN hms hours_minutes_seconds INT
COMMENT 'New comment'
AFTER severity;

/*
 * Add columns, to the end of existing columns.
 * Use CHANGE COLUMN to rearrange if needed.
 */
ALTER TABLE log_messages
ADD COLUMNS (
app_name STRING COMMENT 'New column 1',
session_id INT COMMENT 'New column 2');

/*
 * Remove all the existing columns and replaces with new 
 * specified columns.
 */
ALTER TABLE log_messages
REPLACE COLUMNS (
app_name STRING COMMENT 'New column 1',
session_id INT COMMENT 'New column 2');

/* 
 * You can add table properties or set current properties,
 * but not remove them 
 */
ALTER TABLE log_messages
SET TBLPROPERTIES (
'some_key' = 'some_value'
);
</code></pre>
]]></content>
  </entry>
  
</feed>
