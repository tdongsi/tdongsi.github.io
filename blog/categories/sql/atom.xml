<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Sql | Personal Programming Notes]]></title>
  <link href="http://tdongsi.github.io/blog/categories/sql/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/"/>
  <updated>2016-11-06T21:43:06-08:00</updated>
  <id>http://tdongsi.github.io/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Analytic Functions in MySQL]]></title>
    <link href="http://tdongsi.github.io/blog/2016/08/17/analytic-functions-in-mysql/"/>
    <updated>2016-08-17T23:12:54-07:00</updated>
    <id>http://tdongsi.github.io/blog/2016/08/17/analytic-functions-in-mysql</id>
    <content type="html"><![CDATA[<p>MySQL has traditionally lagged behind in support for the SQL standard.
Unfortunately, from my experience, MySQL is often used as the sandbox for SQL code challenges and interviews.
If you are used to work with <a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/SQLReferenceManual.htm">Vertica SQL</a>, writing SQL statements in MySQL can be challenging exercises, NOT necessarily in a good way, because many useful features are not supported.</p>

<h3>WITH clause</h3>

<p>As discussed in this <a href="/blog/2016/02/03/vertica-6-with-clause/">blog post</a>, <code>WITH</code> clause syntax, also known as <em>Common Table Expressions</em> (CTE), is thankfully supported in Vertica.
In summary, <code>WITH</code> clause allows us to arrange sub-queries, usually intermediate steps, in a complex SQL query in sequential, logical order.
This will make the complex queries easier to compose and read: we can write steps by steps of the query from top to bottom like a story (i.e., <a href="https://en.wikipedia.org/wiki/Literate_programming">literate programming</a>).
Unfortunately, <code>WITH</code> clause is not supported by MySQL although this feature has been requested since <a href="https://bugs.mysql.com/bug.php?id=16244">2006</a>.
There are <a href="http://guilhembichot.blogspot.fr/2013/11/with-recursive-and-mysql.html">work-arounds</a> for MySQL&rsquo;s lack of CTE, but the easiest way is probably to revert back to using nested subqueries.</p>

<p>Personally, lack of <code>WITH</code> clause support in MySQL is my greatest hindrance as I often ended up writing queries using <code>WITH</code> clauses as first draft before rewriting those queries using nested subqueries.
This might appear clumsy in SQL interviews even though writing SQL codes with CTE instead of subqueries is the recommended practice for maintainable code.</p>

<h3>Analytic functions</h3>

<p>Another regrettable hindrance when working in MySQL is its lack of analytic functions such as <code>ROW_NUMBER</code>, <code>RANK</code> and <code>DENSE_RANK</code>.
Those <a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/Functions/Analytic/AnalyticFunctions.htm">analytic functions</a> are supported in Vertica.
The difference between these three functions can be a bit subtle, and would be best described in the following example:</p>

<pre><code class="sql Example of ROW_NUMBER, RANK, and DENSE_RANK functions">SELECT customer_name, SUM(annual_income),
ROW_NUMBER () OVER (ORDER BY TO_CHAR(SUM(annual_income),'100000') DESC) row_number, 
RANK () OVER (ORDER BY TO_CHAR(SUM(annual_income),'100000') DESC) rank, 
DENSE_RANK () OVER (ORDER BY TO_CHAR(SUM(annual_income),'100000') DESC) dense_rank 
FROM customer_dimension
GROUP BY customer_name
LIMIT 15;
</code></pre>

<p>The outputs of these functions are only different if there are duplicates in <code>SUM(annual_income)</code> value, as seen in rows 75-81 in the example output below:</p>

<table border="1"><tr BGCOLOR="#CCCCFF"><th>customer_name</th><th>SUM</th><th>row_number</th><th>rank</th><th>dense_rank</th></tr>
<tr><td>Theodore R. King</td><td>97444</td><td>71</td><td>71</td><td>71</td></tr>
<tr><td>Laura Y. Pavlov</td><td>97417</td><td>72</td><td>72</td><td>72</td></tr>
<tr><td>Carla . Garcia</td><td>97371</td><td>73</td><td>73</td><td>73</td></tr>
<tr><td>Jack Z. Miller</td><td>97356</td><td>74</td><td>74</td><td>74</td></tr>
<tr><td>Steve W. Williams</td><td>97343</td><td>75</td><td>75</td><td>75</td></tr>
<tr><td>Lauren Y. Rodriguez</td><td>97343</td><td>76</td><td>75</td><td>75</td></tr>
<tr><td>Lucas . Webber</td><td>97318</td><td>77</td><td>77</td><td>76</td></tr>
<tr><td>Sarah N. Moore</td><td>97243</td><td>78</td><td>78</td><td>77</td></tr>
<tr><td>Lucas O. Li</td><td>97184</td><td>79</td><td>79</td><td>78</td></tr>
<tr><td>Doug K. Reyes</td><td>97166</td><td>80</td><td>80</td><td>79</td></tr>
<tr><td>Michael . Weaver</td><td>97162</td><td>81</td><td>81</td><td>80</td></tr>
</table>


<p><br/></p>

<p>Sadly, these useful analytic functions are not supported in MySQL.
Fortunately, MySQL supports user variables in SQL queries and we can reproduce those functionalities in MySQL using variables and subqueries as follows:</p>

<pre><code class="sql ROW_NUMBER, RANK, and DENSE_RANK functions in MySQL">-- In Vertica
SELECT 
ROW_NUMBER () OVER (PARTITION BY col_1, col_2 ORDER BY col_3 DESC) AS row_number,
RANK () OVER (PARTITION BY col_1, col_2 ORDER BY col_3 DESC) AS rank,
DENSE_RANK () OVER (PARTITION BY col_1, col_2 ORDER BY col_3 DESC) AS dense_rank,
t.* 
FROM table_1 t

-- In MySQL
SELECT
@row_num:=IF(@prev_col_1=t.col_1 AND @prev_col_2=t.col_2, @row_num+1, 1) AS row_number,
@rank:=IF(@prev_col_1=t.col_1 AND @prev_col_2=t.col_2 AND @prev_col_3=col_3, @rank, @row_num) AS rank,
@dense:=IF(@prev_col_1=t.col_1 AND @prev_col_2=t.col_2, IF(@prev_col_3=col_3, @dense, @dense+1), 1) AS dense_rank,
@prev_col_1 = t.col_1,
@prev_col_2 = t.col_2,
@prev_col_3 = t.col_3,
t.*
FROM (SELECT * FROM table_1 ORDER BY col_1, col_2, col_3 DESC) t,
     (SELECT @row_num:=1, @dense:=1, @rank:=1, @prev_col_1:=NULL, @prev_col_2:=NULL, @prev_col_3:=NULL) var
</code></pre>

<p>The MySQL work-around is intentionally generic so that I can adapt it to any use case.
In addition, it intentionally has a single pass (no <code>SET</code> statements, temporary table) since most SQL code challenges expect a single query.
Finally, note that the above MySQL solution is intentionally incomplete to make it less convoluted.
You need to put that solution in a subquery and <code>SELECT</code> only relevant columns from it.</p>

<p>As an example, the above code template is used to solve <a href="https://leetcode.com/problems/rank-scores/">this Rank Scores problem</a>.
In summary, the question asks for <code>DENSE_RANK</code> functionality to be applied on Score column.</p>

<pre><code class="plain Input table">+----+-------+
| Id | Score |
+----+-------+
| 1  | 3.50  |
| 2  | 3.65  |
| 3  | 4.00  |
| 4  | 3.85  |
| 5  | 4.00  |
| 6  | 3.65  |
+----+-------+
</code></pre>

<pre><code class="plain Expected output">+-------+------+
| Score | Rank |
+-------+------+
| 4.00  | 1    |
| 4.00  | 1    |
| 3.85  | 2    |
| 3.65  | 3    |
| 3.65  | 3    |
| 3.50  | 4    |
+-------+------+
</code></pre>

<p>The solution in Vertica SQL would be straight-forward as follows:</p>

<pre><code class="sql Solution in Vertica SQL">select Score,
DENSE_RANK() OVER (ORDER BY Score DESC) AS Rank
FROM Scores;
</code></pre>

<p>In MySQL, apply the above code template and note that there is no <code>partition clause</code> to arrive at the following solution:</p>

<pre><code class="sql Solution in MySQL">SELECT Score, Rank FROM
( SELECT t.Score,
@dense:=IF(@prev_col2=t.Score, @dense, @dense+1) AS Rank,
@prev_col2:=t.Score
FROM (SELECT Score FROM Scores ORDER BY Score DESC) t, 
(SELECT @dense:=0, @prev_col2:=NULL) var ) x
</code></pre>

<p>Note that the outer <code>SELECT</code> is used to only expose only columns of interest while the main SQL code is enclosed in a subquery.</p>

<h3>Reference</h3>

<ul>
<li><a href="http://www.folkstalk.com/2013/03/grouped-row-number-function-mysql.html">ROW_NUMBER in MySQL</a></li>
<li><a href="http://www.folkstalk.com/2013/03/grouped-dense-rank-function-mysql-sql-query.html">DENSE_RANK in MySQL</a>: this link actually shows <code>RANK</code> implementation.</li>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/Functions/Analytic/AnalyticFunctions.htm">Vertica Analytic Functions</a></li>
<li><a href="http://dev.mysql.com/doc/refman/5.7/en/user-variables.html">MySQL user variables</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[(Pt. 7) Extending for Data Parity Checks]]></title>
    <link href="http://tdongsi.github.io/blog/2016/04/17/sql-unit-data-parity/"/>
    <updated>2016-04-17T16:39:19-07:00</updated>
    <id>http://tdongsi.github.io/blog/2016/04/17/sql-unit-data-parity</id>
    <content type="html"><![CDATA[<p>Navigation: <a href="/blog/2016/03/16/sql-unit-overview/">Overview</a>,
<a href="/blog/2016/03/20/sql-unit-functional-tests/">Pt 1</a>,
<a href="/blog/2016/03/28/sql-unit-test-runner/">Pt 2</a>,
<a href="/blog/2016/04/10/sql-unit-incremental-data-update/">Pt 3</a>,
<a href="/blog/2016/04/12/sql-unit-testing/">Pt 4</a>,
<a href="/blog/2016/04/14/sql-unit-vs-functional/">Pt 5</a>,
<a href="/blog/2016/04/16/sql-unit-extension/">Pt 6</a>.</p>

<p>As an example to discussion in <a href="/blog/2016/04/16/sql-unit-extension/">this post</a>, I will discuss how I recently added a new functionality to handle a new kind of tests.</p>

<h3>Background of data parity checks</h3>

<p>Recently, I had to do lots of data parity checks to verify changes in Extract-Load processes (i.e., EL with no Transform).
In those data parity checks, we want to make sure data in some columns of two tables (i.e., two projections) must be the same.
In other words, we want to verify if the two following SQL queries return completely matching rows and columns:</p>

<pre><code class="plain Data parity checks">select col1, col2 from old_table_name

matches

select col3, col4 from new_table_name
</code></pre>

<p>The straight-forward test would be to get all the rows and columns of those two projections, and perform equality check one by one.
It would be very time-consuming to write and execute such test cases in Java and TestNG.
Even when the query returns can be managed within the memory limit, it is still time-consuming to do data transfer for the two query returns, join the columns to prepare for comparison row by row.
Moreover, note that these expensive operations are carried out on the client side, our computers.</p>

<p>The more efficient way for this data parity check is to use these two SQL test queries in these test blocks (read <a href="/blog/2016/03/28/sql-unit-test-runner/">this post</a> for more introduction):</p>

<pre><code class="plain Test blocks for data parity check">/* @Test
{
    "name" : "parity_check",
    "query" : "select col1, col2 from old_table_name
                EXCEPT
                select col3, col4 from new_table_name
                limit 20",
    "expected" : ""
}
*/

/* @Test
{
    "name" : "parity_check_reverse",
    "query" : "select col3, col4 from new_table_name
                EXCEPT
                select col1, col2 from old_table_name
                limit 20",
    "expected" : ""
}
*/
</code></pre>

<p>The two SQL test queries is based on the following <a href="https://en.wikipedia.org/wiki/Algebra_of_sets">set theory identities</a>:</p>

<p><span class="math display">\[A = B \Leftrightarrow A \subseteq B \mbox{ and } B \subseteq A\]</span></p>




<p><span class="math display">\[A \subseteq B \Leftrightarrow A \setminus B = \varnothing\]</span></p>


<p>If the query <code>Table_A EXCEPT Table_B</code> returns nothing, it indicates that data in <code>Table_A</code> is a subset of data in <code>Table_B</code>.
Similarly for <code>Table_B EXCEPT Table_A</code> query.
Therefore, if two test cases pass, it means that the data in <code>Table_A</code> is equal to the data in <code>Table_B</code>.</p>

<p>Using these two queries, we shift most of computing works (<code>EXCEPT</code> operations) to the database server side, which is faster since the server cluster is usually much more powerful than our computers.
Moreover, in most of the cases when the tests pass, the data transfer would be minimal (zero row).
In short, these <code>EXCEPT</code>-based checks will save us lots of computation time and data transfer time.</p>

<p>The <code>limit 20</code> clause is also for minimizing data transfer and local computing works.
When the expected return of the SQL query is nothing (i.e., <code>"expected" : ""</code>), we should always add LIMIT clause to the query.
This will save some waiting time and make our log files cleaner when something went wrong and caused the test to fail.
For example, using the above test blocks, if there are one million additional, erroneous rows of data in <code>new_table_name</code> for some reason, the test case &ldquo;parity_check_reverse&rdquo; will fail.
However, instead of transferring one million rows, only 20 of those will be sent to the local host (test machine), thanks to the <code>LIMIT</code> clauses.
In addition, the log file of the Test Runner will NOT be flooded with one million rows of erroneous data while 20 sample rows are probably enough to investigate what happened.</p>

<h3>Extending SQL Test Runner</h3>

<p>If we only need to do a few simple data parity checks, a few (&ldquo;name&rdquo;, &ldquo;query&rdquo;, &ldquo;expected&rdquo;) test blocks as shown above will suffice.
However, there were tens of table pairs to be checked and many tables are really wide, about 100 columns.
For wide tables, for easy investigation if data parity checks fail, we check data in group of 6-10 columns.
Writing test blocks like above can become a daunting task, and such test blocks for wide tables can become hard to read (<a href="/blog/2016/03/20/sql-unit-functional-tests/">readability matters</a>).
Therefore, I create a new test block construct that is more friendly to write and read, as shown below.</p>

<pre><code class="plain New test block">/* @Test
{
    "name" : "parity_check",
    "query" : "select col1, col2 from old_table_name",
    "equal" : "select col3, col4 from new_table_name"
}
*/
</code></pre>

<p>Under the hood, this test block should be equivalent to the two test blocks shown in the last section.
That is, based on the two projection queries found in &ldquo;query&rdquo; and &ldquo;equal&rdquo; clauses, the SQL Test Runner will generate two test blocks with <code>EXCEPT</code>-based test queries as shown above.</p>

<p>Implementation of this new feature is summarized in the following steps:</p>

<ol>
<li>Define new JSON block.</li>
<li>Define new POJO (named <code>NameQueryEqual</code>) that maps to new JSON block.</li>
<li>Create a new class (named <code>NewTestHandler</code> for easy reference) that implements TestStrategy interface to handle the new POJO. Specifically:

<ol>
<li>From <code>NameQueryEqual</code> POJO, generate two <code>NameQueryExpected</code> POJOs with relevant queries (using <code>EXCEPT</code> operations).</li>
<li>Reuse the old TestHandler class to process two <code>NameQueryExpected</code> POJOs.</li>
</ol>
</li>
<li>Create a new test runner that extends the <code>BaseTestRunner</code> and uses the new <code>TestStrategy</code>.</li>
</ol>


<p>For step 1, the new JSON block is already defined as above.
From JSON, the corresponding POJO in step 2 can be easily defined:</p>

<pre><code class="java">/**
 * POJO for JSON test block comparing two projections
 * 
 * @author tdongsi
 */
public class NameQueryEqual {
    // Test name.
    public String name;
    // File lists to run
    public List&lt;String&gt; file;
    // Test query in SQL
    public String query;
    // Equivalent query in SQL
    public String equal;
}
</code></pre>

<p>For step 3, as emphasized in the <a href="/2016/04/16/sql-unit-extension/">last post</a>, we should NOT modify the old test runner to handle this new POJO.
Instead, we should create a new class <code>NewTestHandler</code> that implements TestStrategy interface to handle the new POJO and create a new test runner that uses the new TestStrategy (Strategy pattern).</p>

<p>The implementation of the new test block handler is NOT really complex, thanks to modular design of SQL Test Runner.
We only need to extract two projections from <code>NameQueryEqual</code>&rsquo;s attributes, generate two <code>EXCEPT</code>-based queries for those two projections (with <code>LIMIT</code> clauses), and create two  <code>NameQueryExpected</code> POJOs for those test queries.
Since we already have a TestHanlder class that can run and verify those <code>NameQueryExpected</code> objects, we only need to include a TestHandler object into the <code>NewTestHandler</code> class and delegate handling <code>NameQueryExpected</code> objects to it.
Note that this approach is recommended over subclassing <code>TestHandler</code> to include new code for handling the new <code>NameQueryEqual</code> POJO (i.e., &ldquo;composition over inheritance&rdquo;).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[(Pt. 6) Extending SQL Test Runner]]></title>
    <link href="http://tdongsi.github.io/blog/2016/04/16/sql-unit-extension/"/>
    <updated>2016-04-16T17:49:34-07:00</updated>
    <id>http://tdongsi.github.io/blog/2016/04/16/sql-unit-extension</id>
    <content type="html"><![CDATA[<p>Navigation: <a href="/blog/2016/03/16/sql-unit-overview/">Overview</a>,
<a href="/blog/2016/03/20/sql-unit-functional-tests/">Pt 1</a>,
<a href="/blog/2016/03/28/sql-unit-test-runner/">Pt 2</a>,
<a href="/blog/2016/04/10/sql-unit-incremental-data-update/">Pt 3</a>,
<a href="/blog/2016/04/12/sql-unit-testing/">Pt 4</a>,
<a href="/blog/2016/04/14/sql-unit-vs-functional/">Pt 5</a>.</p>

<p>In this post, I will discuss the design of the SQL Test Runner.
From that, I explain how to easily extend the Test Runner to add new capability for new testing needs.
In the <a href="http://localhost:4000/blog/2016/04/17/sql-unit-data-parity/">next post</a>, I will give an example on how I added a new functionality to handle a new kind of tests.</p>

<h3>Design Overview of SQL Test Runner</h3>

<p>When designing the SQL Test Runner, the following requirements should be taken into account:</p>

<p>1) Test frameworks should be closed to modifications.
If we have added a few hundred test cases that are running fine in the current test suite, we don&rsquo;t want them to suddenly fail just because a new feature must be added into the test framework.
That could be confusing and counter-productive for anyone who are using it.</p>

<p>2) At the same time, the test framework should be open to extension: ability to add new capability, to address new testing needs.
SQL Unit Testing in ETL context is a pretty new area for us.
Therefore, while the current SQL Unit Test framework appears adequate for most testing now, it must be able to support any new testing needs should they arise in the future.
The test framework should be flexible enough to add new capability to support different kinds of ETLs.</p>

<p>These two are also known as <a href="https://en.wikipedia.org/wiki/Open/closed_principle">Open/Closed principle</a>.
Besides that principle, SQL Test Runner codes also use <a href="https://en.wikipedia.org/wiki/Template_method_pattern"><strong>Template Method</strong></a> and <a href="https://en.wikipedia.org/wiki/Strategy_pattern"><strong>Strategy</strong></a> design patterns.
Knowing these design patterns will make it easier to understand the overall code structure and package organization of SQL Test Runner.</p>

<p>At the top level, there is a TestRunner interface that any SQL Test Runner class should implement.
For convenience, an abstract class BaseTestRunner is provided as a template with simple processing flow and naive parsing provided in its <code>runScript</code> method, as shown below (Template Method design pattern).
The template method <code>runScript</code> extracts the SQL statements and test blocks (<code>/* @Test ... */</code> blocks), then delegates to <code>codeHandler</code> and <code>testHandler</code> to process them, respectively.</p>

<pre><code class="java Template Method for running test scripts in BaseTestRunner">private CodeStrategy codeHandler;
private TestStrategy testHandler;
private JdbcConnection connection;

@Override
public final void runScript(String filePath) throws IOException, SQLException {
    SoftAssert sAssert = new SoftAssert();

    // Read in the SQL script
    String content = SqlTestUtility.readFile(filePath);

    // Remove comments
    String sqlCode = TestBlockUtility.removeComments(content);

    Matcher m = TestBlockUtility.testBlockRegex.matcher(sqlCode);
    int startIndex = 0;
    while (m.find()) {

        String currentSql = sqlCode.substring(startIndex, m.start());
        if ( currentSql.trim().length() &gt; 0 )
            codeHandler.runSqlCode(currentSql, connection);;

        testHandler.runTest( m.group(), connection, sAssert );

        startIndex = m.end();
    }

    codeHandler.runSqlCode(sqlCode.substring(startIndex), connection );
    sAssert.assertAll();
}
</code></pre>

<p>In the <code>BaseTestRunner</code> class, the <code>codeHandler</code> attribute can be any object that implements <code>CodeStrategy</code> interface (Strategy design pattern).
It will handle executing SQL statements that are found in the unit test scripts, such as the first two <code>INSERT</code> statements in the example script below.
Similarly, the <code>testHandler</code> attribute in the <code>BaseTestRunner</code> can be any object that implements <code>TestStrategy</code> interface.
It will handle test blocks (<code>/* @Test ... */</code> blocks) such as the two test blocks in the example script below.
There are many different ways to process a test block: the first test block might be executed using a Vertica-specific interface, while the second one is executed with a generic JDBC interface.
By using the Strategy design pattern, if there is a necessary change in executing SQL code or test blocks, the test framework is flexible enough to easily integrate that change.</p>

<pre><code class="sql Example unit test script">-- This will be handled by some CodeStrategy class
INSERT INTO stg_company_id (company_id,last_modify_date,region_id) 
VALUES (123,current_timestamp-19,'US');

INSERT INTO stg_company_contact (company_id,master_email,last_modify_date) 
VALUES (123,'before@mockdata.com', current_timestamp-15);

-- This will be handled by some TestStrategy class
/* @Test
-- First ETL run
{
    "name" : "Day1_etl_run",
    "vsql_file" : ["repo_home/sql/my_etl.sql"]
}
*/

/* @Test
{
    "name" : "Day1_check_email_address",
    "query" : "select company_id, email_address from dim_company",
    "expected" : "123 before@mockdata.com"
}
*/
</code></pre>

<p>The <code>codeHandler</code> and <code>testHandler</code> attributes are undefined in the abstract class BaseTestRunner, leaving the actual test runners to provide with concrete classes when they subclass the BaseTestRunner.
In this way, when another team needs to run a new format of test blocks or run test blocks in a different way, it will only need to define a new class that implements TestStrategy interface to handle those new test blocks.
Then, a new test runner class can be created by simply subclassing the BaseTestRunner, and provide the new TestStrategy class instead.
In the following example TestRunner class, a new <code>VerticaTestHandler</code> class is created to handle test blocks that are specific to Vertica, as opposed to generic JDBC-compatible databases.
Other components such as SqlCodeHandler to process SQL statements can be reused for this new TestRunner.</p>

<pre><code class="java Example TestRunner">/**
 * Test runner that uses Vertica JDBC connection.
 * It can handle test block of NameVsqlfile format that runs ETL scripts using local vsql.
 * 
 * @author tdongsi
 */
public class VerticaRunner extends BaseTestRunner implements TestRunner {
    public VerticaRunner(JdbcConnection jdbcConn, String vsqlPath) {
        this.setCodeHandler(new SqlCodeHandler());
        this.setTestHandler(new VerticaTestHandler(vsqlPath));
        this.setConnection(jdbcConn);
    }
}
</code></pre>

<h3>Extending Test Runner</h3>

<p>When extending a test runner, the behaviors of the test runners should NOT be inherited.
Instead, they should be encapsulated in classes that specify how to handle SQL statements (CodeStrategy interface) or test blocks <code>/* @Test {...} */</code> (TestStrategy interface).
When a new test runner is created to meet new testing needs, we should not subclass the previous test runner.
Instead, we can delegate the old behaviors to the old handlers while adding new classes to handle new behaviors or new functionality.
In other words, &ldquo;composition over inheritance&rdquo; principle applies here to separate test runner classes and test processing behaviors that each test runner uses.</p>

<p>Implementation of a new feature can be summarized in the following steps:</p>

<ol>
<li>Design new JSON block for the new test block.</li>
<li>Define new POJO that maps to new JSON block.</li>
<li>Create a new class that implements TestStrategy/CodeStrategy interface to handle the new POJO.</li>
<li>Create a new test runner that uses the new TestStrategy/CodeStrategy.</li>
</ol>


<p>For example, our current test runner that can run an ETL script in Vertica database using <code>vsql</code> command-line tool.
If we need a test runner that is able to run an ETL script in <strong>Netezza</strong> database, we should not modify our <em>current</em> test runner.
It will break the current suite of tests for Vertica.
Instead, we should create a new test runner class with new class extend TestStrategy to handle running ETL in Netezza.</p>

<p>In <a href="/blog/2016/04/17/sql-unit-data-parity/">another example</a>, I give more detailed steps of implementation when we need to add new capability to SQL Test Runner.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[(Pt. 5) Big Data: Functional Tests vs. Unit Tests]]></title>
    <link href="http://tdongsi.github.io/blog/2016/04/14/sql-unit-vs-functional/"/>
    <updated>2016-04-14T17:21:12-07:00</updated>
    <id>http://tdongsi.github.io/blog/2016/04/14/sql-unit-vs-functional</id>
    <content type="html"><![CDATA[<p>Navigation: <a href="/blog/2016/03/16/sql-unit-overview/">Overview</a>,
<a href="/blog/2016/03/20/sql-unit-functional-tests/">Pt 1</a>,
<a href="/blog/2016/03/28/sql-unit-test-runner/">Pt 2</a>,
<a href="/blog/2016/04/10/sql-unit-incremental-data-update/">Pt 3</a>,
<a href="/blog/2016/04/12/sql-unit-testing/">Pt 4</a>.</p>

<p>In the context of Big Data projects, the differences between functional tests and unit tests can be summarized as follows:</p>

<table>
<thead>
<tr>
<th>       </th>
<th> Functional tests      </th>
<th> Unit tests </th>
</tr>
</thead>
<tbody>
<tr>
<td> Data         </td>
<td> Production-like data </td>
<td> Mock (synthetic) data </td>
</tr>
<tr>
<td> Environment  </td>
<td> Pre-production. All tables deployed at once. </td>
<td> Local VM. Regular setup/teardown. </td>
</tr>
<tr>
<td> Coverage     </td>
<td> Passive: Coverage depends on diverse real data. </td>
<td> Active: Mock data created to force corner cases. </td>
</tr>
<tr>
<td> Example usage </td>
<td> Snapshot testing </td>
<td> Incrementa data update testing </td>
</tr>
</tbody>
</table>


<p><br></p>

<p>It should be noted that functional and unit tests are complementary to each other.
Certain aspects of ETLs can be better verified as functional tests while others of the same ETLs should be verified as unit tests.
For example, as discussed in <a href="/blog/2016/04/10/sql-unit-incremental-data-update/">this post</a>, unit tests are better suited for testing incremental data update in ETL scripts.</p>

<p>On the other hand, for example, an ETL that performs some kind of classification, such as categorizing user types based on some clickstream patterns, should be tested in functional tests.
If there are more than 20 categories, it could become a daunting task to generate and maintain synthetic data for each of those categories.
Furthermore, synthetic data generation requires careful consideration and proper execution to have adequate coverage.
Otherwise, the synthetic data might not be as diverse as production data and we end up with less corner cases than production data.
Instead, in this particular case, we could use production-like data directly and write test queries in functional tests to check for corner cases for each category.</p>

<!-- Analogy:
Functional tests:
- In Web App, running the web application on webserver, running automated Selenium WebDriver tests to verify the web application from browser.
- In Big Data, running the DDL/DML/ETL scripts to populate the dimension and fact tables in schema, running automated SQL test queries to verify logics between tables.

Unit tests:
- In Web App, using mocking framework to mock out database, test behavior of a class/method using synthetic inputs, especially for corner cases.
- In Big Data, using a local VM, test behavior of a column modified by an ETL script using synthetic data, especially for corner cases.
-->

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[(Pt. 4) SQL Unit Testing]]></title>
    <link href="http://tdongsi.github.io/blog/2016/04/12/sql-unit-testing/"/>
    <updated>2016-04-12T17:45:42-07:00</updated>
    <id>http://tdongsi.github.io/blog/2016/04/12/sql-unit-testing</id>
    <content type="html"><![CDATA[<p>Navigation: <a href="/blog/2016/03/16/sql-unit-overview/">Overview</a>,
<a href="/blog/2016/03/20/sql-unit-functional-tests/">Pt 1</a>,
<a href="/blog/2016/03/28/sql-unit-test-runner/">Pt 2</a>,
<a href="/blog/2016/04/10/sql-unit-incremental-data-update/">Pt 3</a>.</p>

<!-- 
Changes I made:
1. Mix of SQL code and test blocks.
1. New JSON block to run ETL script using VSQL

I would also discuss some guidelines of unit testing for ETL and when it makes sense to focus.

Running ETL script through JDBC is probably not a good idea.

Requirements of unit tests:

Readability:

#### Single-node VM

Remove KSAFE.

Add a new test.
  
Revert in Git.

#### Adding  unit test

Show SBG strategy.

#### Other usages

You can insert into the ETL script to verify step by step.
However, there is only one set of mock data. 
In unit testing, you might want multiple setup of mock data for different scenarios.
=> the other way is actually more flexible

Assumptions:

1. No ;
1. ETL is simple enough: the same tables are not updated and transformed multiple times in multiple steps. 


-->


<p>TODO indefinitely.</p>

<p>The idea is to use a local Vertica VM as a sandbox test environment.
It could be a <a href="/blog/2016/01/10/find-and-replace-a-string-in-multiple-files/">single cluster VM</a> or <a href="/blog/2016/03/12/set-up-three-node-vertica-sandbox-vms-on-mac/">three-node cluster VM</a>.</p>

<p>The following changes in SQL Test Runner are critical to enable unit testing:</p>

<ol>
<li>Mix of SQL code and test blocks: We can use SQL code to do necessary data setup before running SQL queries and verifying expected outputs.</li>
<li>New test block to run ETL script using VSQL CLI: The ETL scripts are considered (large) classes/functions under test, and this new kind of test block simplify running those &ldquo;functions&rdquo; again and again with different synthetic data. Running using VSQL CLI is required since we execute ETL scripts in production using that tool.</li>
<li>Automated execution of DDL/DML files for loading other static dimension tables.</li>
</ol>


<p>In the following example, two <code>INSERT</code> statements is used to set up data in two input staging tables.
They are followed by a new test block to run the ETL script.
After the ETL is executed, the output data, <code>email_address</code> column for example, in the target dimension table is verified using the <a href="/blog/2016/03/28/sql-unit-test-runner/">standard test block</a>.
Other static dimension tables such as <code>dim_country</code> that the ETL script <code>my_etl.sql</code> depends on, can be created and populated using Java code.</p>

<pre><code class="sql Example unit test">/****************************
* Day 1
****************************/

INSERT INTO stg_company_id (company_id,last_modify_date,region_id)
VALUES (123,current_timestamp-19,'US');

INSERT INTO stg_company_contact (company_id,master_email,last_modify_date)
VALUES (123,'before@mockdata.com', current_timestamp-15);

/* @Test
-- First ETL run
{
 "name" : "Day1_etl_run",
 "vsql_file" : ["repo_home/sql/my_etl.sql"]
}
*/

/* @Test
{
 "name" : "Day1_check_email_address",
 "query" : "select company_id, email_address from dim_company",
 "expected" : "123 before@mockdata.com"
}
*/
</code></pre>

<pre><code class="java Calling unit test script">@BeforeClass
public void setup() {
    testRunner = new SqlTestRunner(getJdbcConnection());
    setupSchema("UNITTEST");
}

@AfterClass
public void teardown() {
    teardownSchema("UNITTEST");
}

@Test(enabled = true)
public void validate_dim_region() throws Exception {
        testRunner.runScript("unittests/etl_incremental_update_email.test");
}
</code></pre>

<p>For full unit test script, see <a href="/blog/2016/04/10/sql-unit-incremental-data-update/">here</a>.</p>
]]></content>
  </entry>
  
</feed>
