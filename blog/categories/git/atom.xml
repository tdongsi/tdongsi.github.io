<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Git | Personal Programming Notes]]></title>
  <link href="http://tdongsi.github.io/blog/categories/git/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/"/>
  <updated>2018-01-17T15:42:34-08:00</updated>
  <id>http://tdongsi.github.io/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Github REST API Cookbook]]></title>
    <link href="http://tdongsi.github.io/blog/2017/08/06/github-rest-api/"/>
    <updated>2017-08-06T21:37:20-07:00</updated>
    <id>http://tdongsi.github.io/blog/2017/08/06/github-rest-api</id>
    <content type="html"><![CDATA[<p>The blog post shows some useful snippets for interacting with Github API.
Jenkins pipelines regularly interacts with Github (public or Enterprise) API to perform some query/posting, for example, regarding the current pull request.
For that reason, some of the following snippets are either in Groovy or <code>curl</code> commands embedded in Groovy-based Jenkinsfile code with some Jenkinsfile DSLs.</p>

<!--more-->


<h3>Working with Pull Requests</h3>

<h4>Extracting Pull Request details</h4>

<pre><code class="groovy Get PR body text">import groovy.json.JsonSlurper

def getPrBody(String githubUsername, String githubToken, String repo, String id) {
  String GITHUB_API = 'https://git.enterprise.com/api/v3/repos'

  String url = "${GITHUB_API}/${githubUsername}/${repo}/pulls/${id}"
  println "Querying ${url}"
  def text = url.toURL().getText(requestProperties: ['Authorization': "token ${githubToken}"])
  def json = new JsonSlurper().parseText(text)
  def bodyText = json.body

  return bodyText
}
</code></pre>

<p>The equivalent <code>curl</code> command is as follows, with JSON processing is done in <code>jq</code>:</p>

<pre><code class="groovy Equivalent curl | jq command in Jenkisfile">sh """
curl -s -H "Authorization: token ${env.GITHUB_TOKEN}" ${GITHUB_API}/${org}/${repo}/pulls/${env.CHANGE_ID} | jq '.body' -r &gt; pr_body.txt
"""
</code></pre>

<h4>Posting comment on the Pull Request</h4>

<p>Reference: <a href="https://developer.github.com/v3/issues/comments/#create-a-comment">Create a comment</a>.</p>

<pre><code class="groovy Equivalent curl in Jenkinsfile">sh """
curl -s -X POST -H "Authorization: token ${env.GITHUB_TOKEN}" --data '{"body": "${comment}"}' ${GITHUB_API}/${org}/${repo}/issues/${env.CHANGE_ID}/comments
"""
</code></pre>

<h4>Merge Pull Request</h4>

<p>Based on <a href="http://www.cloudypoint.com/Tutorials/discussion/jenkins-solved-how-to-merge-a-successful-build-of-a-pull-request-using-a-jenkinsfile/">this article</a>.</p>

<pre><code class="groovy Merge pull request">stage ("Merge PR") {
    steps { 
        withCredentials([usernamePassword(credentialsId: 'credential-value', usernameVariable: 'ACCESS_TOKEN_USERNAME', passwordVariable: 'ACCESS_TOKEN_PASSWORD',)]) {
            def GITHUB = 'https://github.ibm.com/api/v3/repos'
            sh "curl -X PUT -d '{\"commit_title\": \"Merge pull request\"}' ${GITHUB}/org-name/repo-name/pulls/${env.CHANGE_ID}/merge?access_token=${env.ACCESS_TOKEN_PASSWORD}"
        }
    }
}
</code></pre>

<p>The Jenkins-provided environment variable <code>$CHANGE_ID</code>, in the case of a pull request, is the pull request number.</p>

<h3>Working with Branches</h3>

<h4>Getting email of branch maintainer</h4>

<p>At the end of a Jenkins build for a feature branch (NOT <code>develop</code>/<code>master</code>), you may want to email some developer of its status, as opposed to blasting a whole distribution list.
Note that in Git, there is no such metadata for branch creator, as discussed <a href="https://stackoverflow.com/questions/12055198/find-out-git-branch-creator/19135644">here</a>.
Instead, it makes more sense to notify the latest/active committer which is likely the owner of the branch.</p>

<!--
Furthermore, while most of the branches in Git has short lifetime, some branches such as `master` and `develop` can stay around for a long time.
That person may be not active or leave the project entirely.
-->


<pre><code class="groovy Get email of branch maintainer.">def getBranchCreator(String githubUsername, String githubToken, String repo, String branch) {
    String GITHUB_API = 'https://git.enterprise.com/api/v3/repos'

    String url = "${GITHUB_API}/${githubUsername}/${repo}/branches/${branch}"
    println "Querying ${url}"
    def text = url.toURL().getText(requestProperties: ['Authorization': "token ${githubToken}"])
    def json = new JsonSlurper().parseText(text)

    // Get last committer.
    def creator = json.commit.commit.committer.email
    // TRICKY: json.commit.commit.committer.email is not valid if someone commits from Github web interface.
    // In the case, committer name is 'GitHub Enterprise'.
    if (json.commit.commit.committer.name == 'GitHub Enterprise') {
    // Use author's email instead
    creator = json.commit.commit.author.email
    }
    // TRICKY: the following can return inconsistent data, including "null".
    // def author = json.author
    return creator
}

// Calling from Jenkinsfile
withCredentials([
    [$class: 'UsernamePasswordMultiBinding', credentialsId: 'my-credentials', 
        passwordVariable: 'GITHUB_PASSWORD', usernameVariable: 'GITHUB_USERNAME']
]) {
    if (env.BRANCH_NAME ==~ /PR-\d+/ ) {
        // If it is a PR build, use some distribution list
        email = 'someemail@enterprise.com'
    } else {
        // TODO: Replace env.GITHUB_USERNAME with the correct Github org name.
        email = getBranchCreator(env.GITHUB_USERNAME, env.GITHUB_PASSWORD, 'my_repo', env.BRANCH_NAME)
    }
}  
</code></pre>

<h4>Deleting a branch</h4>

<p>Searching how to delete a branch in Github API&rsquo;s <a href="https://developer.github.com/v3/repos/branches/">Branches reference</a> does not return anything.
In fact, to delete a branch, we have to delete its HEAD reference as shown <a href="https://developer.github.com/v3/git/refs/#delete-a-reference">here</a>.</p>

<pre><code class="plain Deleting a branch">DELETE /repos/octocat/Hello-World/git/refs/heads/feature-a
</code></pre>

<h3>More tips on Github API</h3>

<p>1) When processing data from Github API, note that any commit has an author and a committer, as shown below.</p>

<pre><code class="json Example commit data">        "commit": {
            "author": {
                "name": "Cuong Dong-Si",
                "email": "tdongsi@example.com",
                "date": "2017-08-17T05:33:46Z"
            },
            "committer": {
                "name": "Tue-Cuong Dong-Si",
                "email": "tdongsi@example.com",
                "date": "2017-08-17T05:33:46Z"
            },
            "message": "@JIRA-4214772@: Add function.",
            "tree": {
                "sha": "xxx",
                "url": "xxx"
            },
            "url": "xxx",
            "comment_count": 0
        },
</code></pre>

<p>While the two fields are usually the same in normal commits (with same associated email and timestamp), they have different meanings.
In summary, the author is the one who created the content, and the committer is the one who committed it.
The two fields can be different in some common Github workflows:</p>

<ul>
<li>Commit a change from Github web interface: The author is the logged-in user (e.g., tdongsi) but the &ldquo;committer&rdquo; field usually has the Github default name and email, e.g., &ldquo;Github Enterprise&rdquo; and &ldquo;<a href="&#x6d;&#97;&#105;&#108;&#116;&#x6f;&#58;&#110;&#x6f;&#x2d;&#114;&#101;&#112;&#x6c;&#121;&#64;&#x67;&#105;&#116;&#x68;&#117;&#98;&#x2e;&#99;&#x6f;&#109;">&#110;&#111;&#x2d;&#114;&#101;&#112;&#108;&#121;&#x40;&#x67;&#105;&#x74;&#x68;&#x75;&#x62;&#46;&#x63;&#x6f;&#x6d;</a>&rdquo;.</li>
<li>Make and/or merge a pull request from Github: For example, Alice submitted a pull request which was accepted and then merged by Betty (the repository owner). In that case, the author is Alice and the committer is Betty.</li>
</ul>


<p>Due to that subtle difference in committer and author in different scenarios, one has to be careful when using data sent by Github API in a Jenkins pipeline.
For example, you want to send email to the repository owner (committer) at the end of a Pull Request build, but what if someone adds a commit via Github web interface (commiter email would be &ldquo;<a href="&#x6d;&#x61;&#x69;&#108;&#116;&#111;&#58;&#110;&#x6f;&#x2d;&#x72;&#x65;&#x70;&#x6c;&#121;&#x40;&#x67;&#x69;&#x74;&#104;&#117;&#98;&#46;&#x63;&#111;&#x6d;">&#110;&#111;&#x2d;&#114;&#x65;&#112;&#x6c;&#121;&#64;&#x67;&#x69;&#x74;&#x68;&#117;&#98;&#46;&#99;&#x6f;&#x6d;</a>&rdquo; which is not helpful).</p>

<p>2) There is an API rate limit for the free public Github API (note &ldquo;X-RateLimit-Limit&rdquo; and &ldquo;X-RateLimit-Remaining&rdquo; in output below).</p>

<pre><code class="plain Github API limit">tdongsi-mac:dev tdongsi$ curl -i https://api.github.com/users/tdongsi
HTTP/1.1 200 OK
Date: Fri, 09 Jun 2017 16:16:49 GMT
Content-Type: application/json; charset=utf-8
Content-Length: 1236
Server: GitHub.com
Status: 200 OK
X-RateLimit-Limit: 60
X-RateLimit-Remaining: 55
X-RateLimit-Reset: 1497025098
Cache-Control: public, max-age=60, s-maxage=60
Vary: Accept
ETag: "4d7770cf5c2478bf64d23bc908494172"
Last-Modified: Thu, 01 Jun 2017 01:09:00 GMT
X-GitHub-Media-Type: github.v3; format=json
Access-Control-Expose-Headers: ETag, Link, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval
Access-Control-Allow-Origin: *
Content-Security-Policy: default-src 'none'
Strict-Transport-Security: max-age=31536000; includeSubdomains; preload
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-Runtime-rack: 0.030687
Vary: Accept-Encoding
X-Served-By: 62cdcc2d03a2f173f1c58590d1a90077
Vary: Accept-Encoding
X-GitHub-Request-Id: FADF:2CB6E:44F743B:56EDC6F:593AC9F1

...
</code></pre>

<p>You are likely to hit this rate limit quickly if you are polling the repos for updates.
Instead of polling from your CI (e.g., Jenkins) system, it is recommended to use <a href="https://developer.github.com/webhooks/creating/">Github webhooks</a>.</p>

<h3>Reference</h3>

<ul>
<li><a href="/blog/2015/08/04/curl-cookbook/">curl cookbook</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Jenkins Pipeline Shared Libraries]]></title>
    <link href="http://tdongsi.github.io/blog/2017/03/17/jenkins-pipeline-shared-libraries/"/>
    <updated>2017-03-17T15:38:14-07:00</updated>
    <id>http://tdongsi.github.io/blog/2017/03/17/jenkins-pipeline-shared-libraries</id>
    <content type="html"><![CDATA[<p>When you have multiple Pipeline jobs, you often want to share some parts of the Jenkinsfiles between them to keep Jenkinfiles <a href="https://en.wikipedia.org/wiki/Don't_repeat_yourself">DRY</a>.
A very common use case is that you have many projects that are built in the similar way, such as Nexus authentication step in Gradle build.
One way is to use <a href="https://github.com/jenkinsci/workflow-cps-global-lib-plugin">Workflow plugin</a>.
Comprehensive user documentation can be found in <a href="https://jenkins.io/doc/book/pipeline/shared-libraries/">this section</a> of Jenkins handbook.</p>

<p>In the following sections, we review a couple <strong>older</strong>, but not necessarily worse, ways of updating shared Groovy code which are still used in some Jenkins system.</p>

<!--more-->


<h3>Simple copying</h3>

<p>A quick and dirty way of updating shared Groovy codes in Jenkinsfile is to overwrite Groovy files on Jenkins in its <code>$JENKINS_HOME</code>.
All such Groovy files are stored in <em>$JENKINS_HOME/workflow-libs</em> folder, following this directory structure:</p>

<pre><code class="plain Directory structure of a Shared Library repository">(root)
+- src                     # Groovy source files
|   +- org
|       +- foo
|           +- Bar.groovy  # for org.foo.Bar class
+- vars
|   +- foo.groovy          # for global 'foo' variable
|   +- foo.txt             # help for 'foo' variable
+- resources               # resource files (external libraries only)
|   +- org
|       +- foo
|           +- bar.json    # static helper data for org.foo.Bar
</code></pre>

<p>By manually modifying the Groovy files (e.g., <em>vars/foo.groovy</em>) and restarting Jenkins, you can update their behaviors accordingly.
This method is dirty and definitely bad since it requires a Jenkins restart and modifications to Groovy codes are not tracked (and code-reviewed) anywhere.</p>

<h3>Git-based update</h3>

<p>A more scalable alternative for updating Groovy codes is to use <code>git push</code>, exposed by Jenkins.</p>

<p>As a side note, this method is no longer mentioned in documentation, as of March 2017.
In fact, you have to look into a <a href="https://github.com/jenkinsci/workflow-cps-global-lib-plugin/tree/ce1177278d4cb05ac6b01f723177cc4b2e0aec8d">very old commit</a>
or <a href="https://github.com/cloudbees/workflow-plugin/tree/master/cps-global-lib">outdated, unofficial fork</a> to find this method briefly mentioned at all.
It is also occasionally mentioned in support articles such as <a href="https://support.cloudbees.com/hc/en-us/articles/218162277-Unable-to-Clone-workflowLibs">this</a>.</p>

<p>In this method, the directory <em>$JENKINS_HOME/workflow-libs</em> is exposed by Jenkins as a Git repository.
You can deploy new changes to this directory through <code>git push</code> and any such event will trigger Jenkins to recompile Groovy files.
There is no Jenkins restart required for this method, which makes it much more suitable for production Jenkins.
The Git repository is exposed in two endpoints:</p>

<ul>
<li><a href="http://server/jenkins/workflowLibs.git">http://server/jenkins/workflowLibs.git</a> (when your Jenkins is <code>http://server/jenkins/</code>).</li>
<li>ssh://USERNAME@server:PORT/workflowLibs.git (when Jenkins acts as <a href="https://wiki.jenkins-ci.org/display/JENKINS/Jenkins+SSH">an SSH server</a>)</li>
</ul>


<p>This method also means that the shared Jenkins library scripts in Groovy are stored in another Git repository (e.g., &ldquo;shared-lib&rdquo; on Github) and only <code>git push</code> to the <code>workflowLibs.git</code> repository in the event of deployment.
Having the shared scripts in Git allows you to track changes, perform tested deployments, and reuse the same shared library across a large number of instances.</p>

<h4>Jenkinsfile to update global library</h4>

<p>In this Git-based update approach, all Groovy files should be in some Git repository (e.g., &ldquo;shared-lib&rdquo;) with certain directory structure (shown in the last section).
Since Jenkinsfile has been extensively used for creating CI/CD pipelines, it is only appropriate to add a Jenkinsfile for deploying Groovy files in this Git repository to update Jenkins.
The Jenkinsfile for such workflow-libs should be as follows:</p>

<pre><code class="plain Jenkinsfile for deployment">  stage 'Checkout'
  checkout scm

  if (env.BRANCH_NAME == 'master') {
    stage 'Update'
    println "Updating Jenkins workflow-libs"
    sshagent(['jenkins_ssh_key']) {
      sh """
         git branch master
         git checkout master
         ssh-keyscan -H -p 12222 \${JENKINS_ADDR} &gt;&gt; ~/.ssh/known_hosts
         git remote add jenkins ssh://tdongsi@\${JENKINS_ADDR}:12222/workflowLibs.git
         git push --force jenkins master
      """
    }
  }
</code></pre>

<p>Some comments on this Jenkinsfile:</p>

<ul>
<li><code>sshagent(['jenkins_ssh_key'])</code> indicates that the current node/slave is known as <a href="https://wiki.jenkins-ci.org/display/JENKINS/SSH+Agent+Plugin">an SSH agent</a> to Jenkins master, using Jenkins credentials with ID <code>jenkins_ssh_key</code>.</li>
<li><code>git remote add</code> uses the currently checked out Git repo and branch as a remote branch (named &ldquo;jenkins&rdquo;) to the <code>workflowLibs</code> repository.</li>
<li>The <code>workflowLibs</code> repository is managed by Jenkins, exposed at that location <em>ssh://tdongsi@\${JENKINS_ADDR}:12222/workflowLibs.git</em>.</li>
<li>Then we force push any new changes to the Git repository on Jenkins.</li>
</ul>


<p>After the push, the Git repository <code>workflowLibs</code> on Jenkins should have latest change, same as the current &ldquo;shared-lib&rdquo; repository.
Upon a <code>git push</code> event, the Jenkins will automatically update its global library with the latest changes, without the need of restarting.
Note that for this SSH push to work, a public-private key pair must be generated and configured accordingly.</p>

<pre><code class="plain Key pair generation">mymac:jenkins tdongsi$ kubectl --namespace=jenkins exec -ti jenkins-ideb4 -- bash

jenkins@jenkins-4076880321-ideb4:~$ ssh-keygen -t rsa -b 4096 -C "example@gmail.com"
Generating public/private rsa key pair.
Enter file in which to save the key (/var/jenkins_home/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
</code></pre>

<p>The generated public key should be added to the user via <em>jenkinsurl.com/user/tdongsi/configure</em> URL and private key should be added to the credentials ID <code>jenkins_ssh_key</code>.</p>

<h3>References</h3>

<ul>
<li><a href="https://github.com/cloudbees/workflow-plugin/tree/master/cps-global-lib">Git-based update</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Git: Allow-empty When Squashing]]></title>
    <link href="http://tdongsi.github.io/blog/2016/07/05/git-allow-empty-when-squashing/"/>
    <updated>2016-07-05T00:15:57-07:00</updated>
    <id>http://tdongsi.github.io/blog/2016/07/05/git-allow-empty-when-squashing</id>
    <content type="html"><![CDATA[<p>Many times in Git, we commit some work only to realize that is a mistake, and we should do another way.
The easy way to fix that is to revert the previous commit, a process in which Git creates another commit that undoes exactly everything in the last commit.
After that, we move on with the other way and check in commits for that.
Before pushing everything to the remote branch, as responsible software engineers :), we sometimes want to &ldquo;squash&rdquo; the commits to erase the mistake to keep the commit log clean.</p>

<!--more-->


<p>In the example shown below, my commit <code>daefc6e</code> was a mistake, and I reverted it with <code>f3886c2</code> commit, and then I checked in my correct solution in <code>b4cb02d</code> commit.
I wanted to squash those commits in an interactive rebase session, as seen in the following:</p>

<pre><code class="plain Rebase commands shown in text editor">pick daefc6e KSAFE REMOVAL.
squash f3886c2 Revert "KSAFE REMOVAL."
squash b4cb02d Update constants.

# Rebase 41ab184..b4cb02d onto 41ab184
#
# Commands:
#  p, pick = use commit
#  r, reword = use commit, but edit the commit message
#  e, edit = use commit, but stop for amending
#  s, squash = use commit, but meld into previous commit
#  f, fixup = like "squash", but discard this commit's log message
#  x, exec = run command (the rest of the line) using shell
#
# These lines can be re-ordered; they are executed from top to bottom.
#
# If you remove a line here THAT COMMIT WILL BE LOST.
#
# However, if you remove everything, the rebase will be aborted.
#
# Note that empty commits are commented out
</code></pre>

<p>However, <code>git rebase</code> always fail in such situations with the following &ldquo;error&rdquo; message:</p>

<pre><code class="plain git rebase fails">$ git rebase -i origin/feature/foobar
You asked to amend the most recent commit, but doing so would make
it empty. You can repeat your command with --allow-empty, or you can
remove the commit entirely with "git reset HEAD^".
rebase in progress; onto 41ab184
You are currently rebasing branch 'feature/foobar' on '41ab184'.

No changes

Could not apply f3886c23589e0964a4483f6454c6edeba7d63fb7... KSAFE REMOVAL.
</code></pre>

<p>The error message is very confusing.
When <code>daefc6e</code> and <code>f3886c2</code> commits are squashed, the net effect is nothing, which is the &ldquo;empty commit&rdquo; mentioned in that error message.
However, retrying the <code>git rebase</code> command with <code>--allow-empty</code> as said does not work.</p>

<pre><code class="plain">$ git rebase --interactive --allow-empty 
error: unknown option `allow-empty' 
</code></pre>

<p>Using <code>git rebase --continue</code> does not work as expected: it does not squash three commits into one.</p>

<p>After some Google searching, it turns out that the above error message comes from <code>git commit --amend</code>, which is delegated by <code>git rebase</code> to handle the squash.
When the message says &ldquo;repeat your command&rdquo;, it means repeating the <code>git commit --amend</code> command, something would never occurs to us.
Therefore, the right thing to do here is repeat <code>commit</code> and continue with the interactive rebase session:</p>

<pre><code class="plain">$ git commit --amend --allow-empty
[detached HEAD 706f662] Revert "KSAFE REMOVAL."

$ git rebase --continue
[detached HEAD 923477f] Revert "KSAFE REMOVAL."
 1 file changed, 3 insertions(+), 3 deletions(-)
Successfully rebased and updated refs/heads/feature/foobar.
</code></pre>

<p>By doing that, we will now have all three commits squashed into one and help cleaning up the commit log.</p>

<!--
http://git.661346.n2.nabble.com/Confusing-error-message-in-rebase-when-commit-becomes-empty-td7612948.html
-->

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Symlinks in Git]]></title>
    <link href="http://tdongsi.github.io/blog/2016/02/20/symlinks-in-git/"/>
    <updated>2016-02-20T11:28:11-08:00</updated>
    <id>http://tdongsi.github.io/blog/2016/02/20/symlinks-in-git</id>
    <content type="html"><![CDATA[<p>Let&rsquo;s say we have folders with many symbolic links in them, linking to other files in the same Git repository.</p>

<pre><code class="bash Before">$ ls -l link
... link -&gt; /path/to/target
</code></pre>

<p>Unfortunately after committing into Git, they&rsquo;ve turned into plain text files.
Note that even after committing and pushing into Git, the symlinks still work fine.
However, after some branch switches and code merges, the symlinks become actual text files with the link target as the contents.</p>

<pre><code class="bash After">$ cat link
/path/to/target
</code></pre>

<!--more-->


<h3>Restoring the symlinks</h3>

<p>Before going into lengthy discussion on how Git handles symlinks and hard links, the quick solution for the above problem is the following Bash script:</p>

<pre><code class="bash">folder=/Users/tdongsi/Github/my_repo/scripts/sql
ls -d1 $folder/* | while read f; do
  ln -sf "$(cat $f)" "$f"
done
</code></pre>

<p>where <code>ls -d1 $folder/*</code> should be replaced with some command that will list exactly the files you want, preferably in full path.
Note that <code>-f</code> option of <code>ln</code> command is required to replace the file with the symlink. For examples:</p>

<pre><code class="bash Examples">ls -d1 vertica/*.sql | while read f; do
  ln -sf "$(cat $f)" "$f"
done

ls -d1 bash/* | while read f; do
  ln -sf "$(cat $f)" "$f"
done
</code></pre>

<p><strong>Best practice note</strong>: I think that the following template is preferred to the more commonly seen <code>for f in $(ls *);</code> <code>do...done</code>:</p>

<pre><code class="bash">ls * | while read f; do
  # command executed for each file
done
</code></pre>

<p>I think it is the better way to handle all file names, especially with spaces, since <code>"$f"</code> will still work.
In addition, <code>$(cmd)</code> is the same as <code>'cmd'</code> (backticks) but it can be nested, unlike using backticks.
It fact, it&rsquo;s the main reason why the backticks have been <a href="http://wiki.bash-hackers.org/scripting/obsolete">deprecated</a> from Bash scripting.</p>

<h3>How Git deals with symlinks</h3>

<p>How Git deals with symlinks is defined in the <a href="https://git-scm.com/docs/git-config">git config</a> <code>core.symlinks</code>.
If false, symbolic links are checked out as small plain files that contain the link text.
<a href="http://stackoverflow.com/questions/954560/how-does-git-handle-symbolic-links">Otherwise</a>, Git just stores the contents of the link (i.e., the path of the file system) in a &lsquo;blob&rsquo; just like it would for a normal file.
It also stores the name, mode and type (e.g., symlink) in the tree object that represents its containing directory.
When you checkout a tree containing the link, it restores the object as a symlink.</p>

<p>After the symlinks are checked out as plain text files, I believe it is pretty much no way for Git to restore symlinks again (i.e., follow symlinks inside text files).
It would be an insecure, undefined behavior: what if the symlink as text file is modified? What if the target is changed when moving between versions of that text file?</p>

<h3>Use hard links?</h3>

<p>You can use hard links instead of symlinks (a.k.a., soft links).
Git will handle a hard link like a copy of the file, except that the contents of the linked files change at the same time.
Git may see changes in both files if both the original file and the hard link are in the same repository.</p>

<p>One of the disadvantages is that the file will be created as a normal file during <code>git checkout</code>, because there is no way Git understand it as a link.
Moreover, hard link itself has many limitations, compared to symlinks, such as files have to reside on the same file-system or partition.
In Mac OSX, hard links to directories are not supported. There is a <a href="https://github.com/selkhateeb/hardlink">tool</a> to do that, but use it with caution.</p>

<p>Finally, it is important to note that hard links to files can be lost when moving between different versions/branches in Git, even if they are in the same repository.
When you switch branches back and forth, Git remove the old files and create new ones.
You still have the copies of the previous files, but they might have totally different inodes, while others (if not in the same Git repo) still refers to the old inodes.
Eventually, the file and its hard links may be out of sync, and appear like totally unrelated files to Git.
Therefore, using hard links, at best, is just a temporary solution.</p>

<h3>Links</h3>

<ol>
<li><a href="http://superuser.com/questions/638998/easiest-way-to-restore-symbolic-links-turned-into-text-files">Alternative ways to restore symlinks</a></li>
<li><a href="http://stackoverflow.com/questions/246215/how-can-i-list-files-with-their-absolute-path-in-linux">Alternative ways to list files</a></li>
<li><a href="https://git.wiki.kernel.org/index.php/Git">Git design overview</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Virtual Machine for ETL Testing]]></title>
    <link href="http://tdongsi.github.io/blog/2016/01/10/find-and-replace-a-string-in-multiple-files/"/>
    <updated>2016-01-10T23:49:15-08:00</updated>
    <id>http://tdongsi.github.io/blog/2016/01/10/find-and-replace-a-string-in-multiple-files</id>
    <content type="html"><![CDATA[<p>When developing data-warehouse solutions in Vertica, you want to set up some test environment.
Ideally, you should have separate schema for each developer.
However, it is usually NOT possible in my experience: developers and test engineers have to share very few schemas in development environment.
The explanation that I usually get is that having a schema for each developer will not scale in database maintenance and administration, and there are likely some limits in Vertica&rsquo;s commercial license.
If that is the case, I recommend that we look into using Vertica Community Edition on <strong>Virtual Machines (VMs)</strong> for sandbox test environment, as a cheap alternative.</p>

<!--more-->


<h3>Vertica Virtual Machine as sandbox test environment</h3>

<p>Are VMs really necessary in data-warehouse testing? When testing Extract-Transform-Load (ETL) processes, I find that many of test cases require regular set-up and tear-down, adding mock records to force rare logical branches and corner cases, and/or running ETLs multiple times to simulate daily runs of those processes.
Regular tear-down requires dropping multiple tables regularly, which requires much greater care and drains much mental energy when working with others' data and tables.
Similarly, adding mock records into some commonly shared tables might affect others when they assume the data is production-like.
Running ETL scripts regularly, which could be computationally intensive, on a shared Vertica cluster might affect the performance or get affected by others' processes.
In short, for these tests, I cannot use the common schema that is shared with others since it might interfere others and/or destroy valuable common data.
Using a Vertica VM as the sandbox test environment helps us minimize interference to and from others' data and activities.</p>

<h3>Single-node VM and KSAFE clause</h3>

<p>I have been using a <strong>single-node</strong> Vertica VM to run tests for sometime. And it works wonderfully for testing purpose, especially when you want to isolate issues, for example, a corner case. The Vertica VM can be downloaded from HP Vertica&rsquo;s support website (NOTE: As of 2016 Jan 1st, the Vertica 7.1 VM is taken down while the Vertica 7.2 VM is not available).</p>

<p>The only minor problem is when we add <code>KSAFE 1</code> in our DDL scripts (i.e., <code>CREATE TABLE</code> statements) for production purposes. This gives error on single-node VM when running DDL scripts to set up schema.
The reason is that Vertica database with one or two hosts cannot be <em>k-safe</em> (i.e., it may lose data if it crashes) and three-node cluster is the minimum requirement to have <code>KSAFE 1</code> in <code>CREATE TABLE</code> statements to work.</p>

<p>Even then, the workaround for running those DDL scripts in tests is easy enough if all DDL scripts are all located in a single folder. The idea is that since <code>KSAFE 1</code> does not affect ETL processes' transform logics, we can remove those KSAFE clauses to set up the test schema and go ahead with our ETL testing. Specifically, in my project, my workflow for ETL testing with <strong>Git</strong> is as follows:</p>

<ul>
<li>Branch the latest code (<code>develop</code> branch) into a temporary branch (e.g., <code>local/develop</code> branch).</li>
<li>Find and remove <code>KSAFE 1</code> in all DDL files (see subsection below).</li>
<li>While still in <code>local/develop</code> branch, commit all these changes in a <strong>single</strong> commit with some unique description (e.g., &ldquo;KSAFE REMOVAL&rdquo;).</li>
<li>Add unit and functional tests to ETL scripts in this branch.</li>
<li>After tests are properly developed and checked-in, reverse the &ldquo;KSAFE REMOVAL&rdquo; commit above.

<ul>
<li>In SourceTree, it could be done by a simple right-click on that commit and selecting &ldquo;Reverse Commit&rdquo;.</li>
</ul>
</li>
<li>Merge <code>local/develop</code> branch into <code>develop</code> branch (create a pull request if needed). You will now have your tests with the latest codes in <code>develop</code> branch.</li>
</ul>


<h4>Find and replace a string in multiple files</h4>

<p>There are times and times again that you find that you have to replace every single occurrences of some string in multiple files with another string. Finding and removing <code>KSAFE 1</code> like the above workflow is an example where &ldquo;removing string&rdquo; is a special case of &ldquo;replacing string&rdquo; with nothing. This operation can be quickly done by the following bash command:</p>

<pre><code>grep -rl match_string your_dir/ | xargs sed -i 's/old_string/new_string/g'
</code></pre>

<p>If you are familiar with bash scripting, the above command is straight forward. This quick explanation is for anyone who does not understand the command:</p>

<ul>
<li><code>grep</code> command finds all files in <code>your_dir</code> directory that contain <code>match_string</code>. <code>-l</code> option makes sure it will return a list of files</li>
<li><code>sed</code> command then execute the replacement regex on all those files. A regex tip: the forward slash <code>/</code> delimiter could be another delimiter (e.g., <code>#</code>). This might be useful if you need to search HTML files.</li>
</ul>


<p>Example: In my case, all the DDL scripts are in multiple sub-directories under <code>tables</code> directory. To find and remove all <code>KSAFE 1</code> occurrences, the command is:</p>

<pre><code>grep -rl 'KSAFE 1' tables | xargs sed -i 's/KSAFE 1//g'
</code></pre>

<p>This will search for the string <code>KSAFE 1</code> in all files in the <code>tables</code> directory and replace <code>KSAFE 1</code> with nothing <code>''</code> for each occurrence of the string in each file.</p>
]]></content>
  </entry>
  
</feed>
