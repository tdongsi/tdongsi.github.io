<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Kubernetes | Personal Programming Notes]]></title>
  <link href="http://tdongsi.github.io/blog/categories/kubernetes/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/"/>
  <updated>2018-01-31T22:11:12-08:00</updated>
  <id>http://tdongsi.github.io/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Design Patterns for Container-based Distributed Systems]]></title>
    <link href="http://tdongsi.github.io/blog/2018/01/14/design-patterns-for-container-based-distributed-systems/"/>
    <updated>2018-01-14T14:56:56-08:00</updated>
    <id>http://tdongsi.github.io/blog/2018/01/14/design-patterns-for-container-based-distributed-systems</id>
    <content type="html"><![CDATA[<p>In earlier dates, object-oriented programming (OOP) revolutionized software development by dividing applications into modular components as objects and classes.
Today, the rise of microservice architectures and containerization technologies enable a similar revolution in developing distributed systems and SaaS/PaaS products.
&ldquo;Containers&rdquo; and &ldquo;container images&rdquo; are analogus to &ldquo;objects&rdquo; and &ldquo;classes&rdquo; in OOP, respectively, as a unit of development and deployment in distributed systems.
In <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45406.pdf">this paper</a>, the authors argued that just like Design Patterns for Object-Oriented Programming, codified in the famous <a href="https://www.amazon.com/Design-Patterns-Elements-Reusable-Object-Oriented/dp/0201633612/ref=sr_1_3?ie=UTF8&amp;qid=1516232505&amp;sr=8-3&amp;keywords=design+patterns">&ldquo;Gang of Four&rdquo; book</a>, some Design Patterns have emerged for Container-based Distributed Systems.
In the same paper, the authors attempted to do something similar, documenting the most common patterns and their usage.</p>

<!--more-->


<h3>General ideas</h3>

<p>The container and the container image should be the abstractions for the development of distributed systems.
Similar to what objects and classes did for OOP, thinking in term of containers abstracts away the low-level details of code and allows us to think in higher-level design patterns.
Based on how containers interact with other containers and get deployed into actual underlying machines, the authors divide the patterns in to three main groups:</p>

<ul>
<li>Single-container patterns: How to expose interface of application in container (similar to effective design of object interface).

<ul>
<li>Upward direction: expose application info/metrics such as <code>/health</code> endpoint.</li>
<li>Downward direction: Formal life cycle agreed between application and management system (similar to Android Activity model).</li>
</ul>
</li>
<li>Single-node multi-container patterns: Basically, how to design a pod in Kubernetes (pod = group of symbiotic containers)

<ul>
<li>Sidecar pattern:

<ul>
<li>Sidecar containers will extend and enhance the main container.</li>
<li>Example: Log forwarding sidecar that collects logs from main container from local disk and stream to a cluster storage system.</li>
</ul>
</li>
<li>Ambassador pattern:

<ul>
<li>Ambassador containers will proxy communication to and from the main container.</li>
<li>Ambassador simplifies and standardizes the outside world to the main container.</li>
<li>Example: Redis proxy ambassador that will discover dependent services for the main container.</li>
</ul>
</li>
<li>Adapter pattern:

<ul>
<li>Adapter containers will standardize and normalize the output of the main container.</li>
<li>In contrast to ambassador, adapter simplifies and normalizes the main app to outside world.</li>
<li>Example: Adapters to ensure all containers have the same monitoring interface to hook to central monitoring system.</li>
</ul>
</li>
</ul>
</li>
<li>Multi-node patterns

<ul>
<li>Leader election pattern

<ul>
<li>When we have many replicas, but only one of them is active at a time.</li>
</ul>
</li>
<li>Work queue pattern

<ul>
<li>One coordinator and many workers distributed to as many nodes for processing.</li>
</ul>
</li>
<li>Scatter/Gather pattern

<ul>
<li>Similar to &ldquo;Work queue&rdquo; pattern, except one coordinator scatter partial works to many slaves, then gather/merge partial outcomes from those slaves.</li>
</ul>
</li>
</ul>
</li>
</ul>


<h3>Why multiple containers on single node</h3>

<p>The more contentious patterns are probably single-node multi-container patterns, especially the sidecar pattern.
The most common anti-pattern is that we try to merge the functionality of the sidecar container into the main container.
Analogously, we also have seen a similar anti-pattern in OOP that ends up with a large class that tries to do many things at once.
There are several benefits to use separate containers:</p>

<ul>
<li>Container is the unit of resource allocation and accounting.

<ul>
<li>In the sidecar example above, the log forwarding container is configured to scavenge spare CPU cycles when the web server is not busy.</li>
</ul>
</li>
<li>Container is the unit of packaging.

<ul>
<li>Two separate teams can work independently on log forwarding and web application.</li>
</ul>
</li>
<li>Container is the unit of reuse.

<ul>
<li>The log forwarding sidecar image can be paired with other &ldquo;main&rdquo; containers.</li>
</ul>
</li>
<li>Container provides failure containment boundary.

<ul>
<li>If the log forwarding container fails, the application container continues serving.</li>
</ul>
</li>
<li>Container is the unit of deployment.

<ul>
<li>Each component can be functionally upgraded or rolled back independently.</li>
</ul>
</li>
</ul>


<h3>References</h3>

<ul>
<li><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45406.pdf">Original Paper</a></li>
<li><a href="http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html">Single-node multi-container patterns</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Minikube in Corporate VPN]]></title>
    <link href="http://tdongsi.github.io/blog/2017/12/31/minikube-in-corporate-vpn/"/>
    <updated>2017-12-31T12:40:08-08:00</updated>
    <id>http://tdongsi.github.io/blog/2017/12/31/minikube-in-corporate-vpn</id>
    <content type="html"><![CDATA[<p>If you are connected to corporate VPN via Cisco&rsquo;s AnyConnect client, you might have problem with starting Minikube.</p>

<pre><code>tdongsi$ minikube start --disk-size=50g --kubernetes-version=v1.8.0
Starting local Kubernetes v1.8.0 cluster...
Starting VM...
Downloading Minikube ISO
 140.01 MB / 140.01 MB [============================================] 100.00% 0s

^C
</code></pre>

<!--more-->


<p>The issue has been extensively discussed in <a href="https://github.com/kubernetes/minikube/issues/1099">this bug report</a>.
<a href="https://github.com/kubernetes/minikube/pull/1329">This pull request</a> supposedly fixes the issue, in v0.19.0 release.
However, I&rsquo;m still occasionally seeing the issue.
I have attempted different approaches but they have different degrees of convenience and success in different networks.</p>

<ol>
<li>Use OpenConnect for VPN access rather than Cisco&rsquo;s AnyConnect client.</li>
<li>Set port forwarding to forward port 8443 on 127.0.0.1 to port 8443 in the minikube VM.</li>
<li>Use <code>--host-only-cidr</code> option in <code>minikube start</code>.</li>
</ol>


<p>In this post, we will look into each approach in more details.</p>

<h3>Using OpenConnect</h3>

<p><a href="http://www.infradead.org/openconnect/">OpenConnect</a> is a CLI client alternative for Cisco&rsquo;s AnyConnect VPN.
Here&rsquo;s how you setup OpenConnect on Mac OSX:</p>

<ol>
<li><p>OpenConnect can be installed via <a href="http://mxcl.github.com/homebrew/">homebrew</a>:</p>

<pre><code class="` plain"> brew update
 brew install openconnect
</code></pre></li>
<li>Install the <a href="http://tuntaposx.sourceforge.net/">Mac OS X TUN/TAP</a> driver</li>
<li><p>Connect. The only thing you should be prompted for is your VPN password.</p>

<pre><code class="` plain"> sudo openconnect --user=&lt;VPN username&gt; &lt;your vpn hostname&gt;
</code></pre></li>
<li>To disconnect, just Ctrl-C in the window where you started the VPN connection.</li>
</ol>


<h3>Port forwarding localhost:xxx -> minikube_IP:xxx</h3>

<p>This approach is the more convenient and more reliable in my experience.
All you need to do is to set up a list of port forwarding rules for minikube&rsquo;s VirtualBox:</p>

<pre><code>VBoxManage controlvm minikube natpf1 k8s-apiserver,tcp,127.0.0.1,8443,,8443
VBoxManage controlvm minikube natpf1 k8s-dashboard,tcp,127.0.0.1,30000,,30000
VBoxManage controlvm minikube natpf1 jenkins,tcp,127.0.0.1,30080,,30080
VBoxManage controlvm minikube natpf1 docker,tcp,127.0.0.1,2376,,2376
</code></pre>

<p>Then, you can set up a new Kubernetes context for working with VPN:</p>

<pre><code>kubectl config set-cluster minikube-vpn --server=https://127.0.0.1:8443 --insecure-skip-tls-verify
kubectl config set-context minikube-vpn --cluster=minikube-vpn --user=minikube
</code></pre>

<p>When working on VPN, you can set <code>kubectl</code> to switch to the new context:</p>

<pre><code>kubectl config use-context minikube-vpn
</code></pre>

<p>All Minikube URLs now must be accessed through <code>localhost</code> in browser.
For example, the standard Kubernetes dashboard URL such as:</p>

<pre><code>tdongsi$ minikube dashboard --url
http://192.168.99.100:30000
</code></pre>

<p>must now be accessed via <code>localhost:30000</code>.
Similar applies to other services that are deployed to minikube, such as <code>jenkins</code> shown above.</p>

<p>In addition, the <code>eval $(minikube docker-env)</code> standard pattern to reuse minikube&rsquo;s Docker deamon would not work anymore.</p>

<pre><code>tdongsi$ minikube docker-env
export DOCKER_TLS_VERIFY="1"
export DOCKER_HOST="tcp://192.168.99.100:2376"
export DOCKER_CERT_PATH="/Users/tdongsi/.minikube/certs"
export DOCKER_API_VERSION="1.23"
# Run this command to configure your shell:
# eval $(minikube docker-env)

tdongsi$ echo $DOCKER_HOST
tcp://192.168.99.100:2376
tdongsi$ docker images
Cannot connect to the Docker daemon at tcp://192.168.99.100:2376. Is the docker daemon running?
</code></pre>

<p>Instead, you have to adjust DOCKER_HOST accordingly and use <code>docker --tlsverify=false ...</code>.</p>

<pre><code>tdongsi$ export DOCKER_HOST="tcp://127.0.0.1:2376"
tdongsi$ alias dockervpn="docker --tlsverify=false"

tdongsi$ dockervpn images
...
</code></pre>

<p>Finally, when not working on VPN, you can set <code>kubectl</code> to switch back to the old context:</p>

<pre><code>kubectl config use-context minikube
</code></pre>

<h3>Use <code>--host-only-cidr</code> option</h3>

<p>This approach is the most simple but it also has less success than I hoped.
The idea of this approach is that AnyConnect VPN client likely routes <code>192.168.96.0/19</code> through its tunnel.
This may conflict with the default Minikube network of <code>192.168.99.0/24</code>.
Therefore, we use <code>minikube start --host-only-cidr 10.254.254.1/24</code> to instruct minikube to use a different, unused arbitrary network.
It is worth a try but it often does not work in my experience.</p>

<h3>Reference</h3>

<ul>
<li><a href="https://github.com/kubernetes/minikube/issues/1099">Bug report &amp; discussion</a></li>
<li><a href="https://gist.github.com/moklett/3170636">OpenConnect instructions</a></li>
<li><a href="https://www.virtualbox.org/manual/ch08.html#vboxmanage-controlvm">VBoxManage</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sending Emails From Docker Containers]]></title>
    <link href="http://tdongsi.github.io/blog/2017/05/25/sending-emails-from-docker-containers/"/>
    <updated>2017-05-25T13:42:45-07:00</updated>
    <id>http://tdongsi.github.io/blog/2017/05/25/sending-emails-from-docker-containers</id>
    <content type="html"><![CDATA[<p>In this post, we looks into how to set up notification emails at the end of CI pipelines in a containerized Jenkins system.
First, we look into conventional Jenkins system (directly hosted) that has direct communication to the SMTP server.
After that, we will look into adjustments required for a containerized Jenkins system to run in the same environment.</p>

<!--more-->


<h3>Sending emails in standard Jenkins setup</h3>

<p>We first look at a typical Jenkins setup, where the Jenkins instance is installed directly on a host machine (VM or bare-metal) and has direct communication to the SMTP server.
For corporate network, you may have to use an SMTP relay server instead.
For those cases, you can configure SMTP communication by <a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-postfix-as-a-send-only-smtp-server-on-ubuntu-14-04">setting up Postfix</a>.
In CentOS, it could be a simple &ldquo;sudo yum install -y mailx&rdquo;.</p>

<p>After installing, update <em>/etc/postfix/main.cf</em> with correct relay information: myhostname, myorigin, mydestination, relayhost, alias_maps, alias_database.
An example <em>/etc/postfix/main.cf</em> is shown below:</p>

<pre><code class="plain /etc/postfix/main.cf example"># See /usr/share/postfix/main.cf.bak for a commented, more complete version

myhostname = dev-worker-1.example.com
smtpd_banner = $myhostname ESMTP $mail_name
biff = no

# appending .domain is the MUA's job.
append_dot_mydomain = no

# Uncomment the next line to generate "delayed mail" warnings
#delay_warning_time = 4h

readme_directory = no

# TLS parameters
smtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem
smtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key
smtpd_use_tls=yes

# See /usr/share/doc/postfix/TLS_README.gz in the postfix-doc package for
# information on enabling SSL in the smtp client.


alias_maps = hash:/etc/aliases
alias_database = hash:/etc/aliases
myorigin = dev-worker-1.example.com
mydestination = dev-worker-1.example.com, localhost.example.com, localhost
relayhost = smtprelay-prd.example.com
mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128
mailbox_size_limit = 0
recipient_delimiter = +
inet_interfaces = localhost
inet_protocols = all
</code></pre>

<p>We can test the setup by sending a test email with the following command:</p>

<pre><code class="plain Send a test email">[tdongsi@dev-worker-1 ~]# echo "Test localhost" | mailx -s Test tdongsi@example.com
send-mail: warning: inet_protocols: disabling IPv6 name/address support: Address family not supported by protocol
postdrop: warning: inet_protocols: disabling IPv6 name/address support: Address family not supported by protocol
</code></pre>

<p>After the <code>postfix</code> service is up, Jenkins can be configured to send email with <a href="https://wiki.jenkins-ci.org/display/JENKINS/Mailer">Mailer plugin</a>.
Mail server can be configured in <strong>Manage Jenkins</strong> page, <strong>E-mail Notification</strong> section.
Please visit <a href="http://www.nailedtothex.org/roller/kyle/entry/articles-jenkins-email">Kohei Nozaki&rsquo;s blog post</a> for more detailed instructions and screenshots.
We can also test the configuration by sending test e-mail in the same <strong>E-mail Notification</strong> section.</p>

<h3>Sending email from container</h3>

<p>Many Jenkins-based CI systems have been containerized and deployed on Kubernetes cluster (in conjunction with <a href="https://wiki.jenkins-ci.org/display/JENKINS/Kubernetes+Plugin">Kubernetes plugin</a>).
For email notifications in such CI systems, one option is to reuse <code>postfix</code> service, which is usually configured and ready on the Kubernetes nodes, and expose it to the Docker containers.</p>

<p>There are two changes need to be made on Postfix to expose it to Docker containers on one host.</p>

<ol>
<li>Exposing Postfix to the docker network, that is, Postfix must be configured to bind to localhost as well as the docker network.</li>
<li>Accepting all incoming connections which come from any Docker containers.</li>
</ol>


<p>Docker bridge (<code>docker0</code>) acts a a bridge between your ethernet port and docker containers so that data can go back and forth.
We achieve the first requirement by adding the IP of <code>docker0</code> to <code>inet_iterfaces</code>.</p>

<pre><code class="plain ifconfig example output">[centos@dev-worker-1 ~]$ ifconfig
docker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1472
        inet 172.22.91.1  netmask 255.255.255.0  broadcast 0.0.0.0
        ether 02:42:88:5f:24:28  txqueuelen 0  (Ethernet)
        RX packets 8624183  bytes 18891507332 (17.5 GiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 15891332  bytes 16911210191 (15.7 GiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

flannel0: flags=4305&lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST&gt;  mtu 1472
        inet 172.22.91.0  netmask 255.255.0.0  destination 172.22.91.0
        unspec 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00  txqueuelen 500  (UNSPEC)
        RX packets 10508237  bytes 7051646109 (6.5 GiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 15511583  bytes 18744591891 (17.4 GiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
</code></pre>

<p>For the second requirement, the whole docker network as well as localhost should be added to <code>mynetworks</code>.
In our kubernetes setup, the docker network should be <code>flannel0</code> and its subnet&rsquo;s CIDR notation is added to the <code>mynetworks</code> line:</p>

<pre><code class="plain Modified "/etc/postfix/main.cf""># See /usr/share/postfix/main.cf.bak for a commented, more complete version

myhostname = dev-worker-1.example.com
smtpd_banner = $myhostname ESMTP $mail_name
biff = no

# appending .domain is the MUA's job.
append_dot_mydomain = no

# Uncomment the next line to generate "delayed mail" warnings
#delay_warning_time = 4h

readme_directory = no

# TLS parameters
smtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem
smtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key
smtpd_use_tls=yes

# See /usr/share/doc/postfix/TLS_README.gz in the postfix-doc package for
# information on enabling SSL in the smtp client.

alias_maps = hash:/etc/aliases
alias_database = hash:/etc/aliases
myorigin = dev-worker-1.example.com
mydestination = dev-worker-1.example.com, localhost.example.com, localhost
relayhost = smtprelay-prd.example.com
mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128 172.22.0.0/16
mailbox_size_limit = 0
recipient_delimiter = +
inet_interfaces = localhost, 172.22.91.1
inet_protocols = all
</code></pre>

<p>Note the differences in <code>inet_interfaces</code> and <code>mynetworks</code> from the last section.
One can simply enter the Docker container/Kubernetes pod to verify such setup.
Note that application <code>mailx</code> maybe not available in a container since we tend to keep the containers light-weight.
Instead, prepare a <code>sendmail.txt</code> file (based on <a href="http://docs.blowb.org/setup-host/postfix.html">this</a>) with the following SMTP commands and use <code>nc</code> to send out the email as shown below.</p>

<pre><code class="plain Send test email from container">mymac:k8s tdongsi$ kubectl --kubeconfig kubeconfig --namespace jenkins exec -it jenkins-8hgsn -- bash -il

jenkins@jenkins-8hgsn:~/test$ cat sendmail.txt
HELO x
MAIL FROM: test@example.com
RCPT TO: tdongsi@example.com
DATA
From: test@example.com
To: $YOUR_EMAIL
Subject: This is a test

The test is successful

.
quit

jenkins@jenkins-8hgsn:~/test$ nc 172.22.91.1 25 &lt;sendmail.txt
220 dev-worker-1.eng.sfdc.net ESMTP Postfix
250 dev-worker-1.eng.sfdc.net
250 2.1.0 Ok
250 2.1.5 Ok
354 End data with &lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt;
250 2.0.0 Ok: queued as 1EF9E60C34
221 2.0.0 Bye
</code></pre>

<p>For containerized Jenkins system, mail server can also be configured in same <strong>Manage Jenkins</strong> page, <strong>E-mail Notification</strong> section.
The only difference is the IP/hostname provided to <strong>SMTP server</strong> option.
Instead of providing the known SMTP server&rsquo;s IP and host, one should use the IP of <code>docker0</code>, as explained above.
In the case of many nodes in Kubernetes cluster with different <code>docker0</code> IP, the Docker container of Jenkins master should reside only on one host and <code>docker0</code>&rsquo;s IP on that host should be used.</p>

<h3>References</h3>

<ul>
<li><a href="http://www.nailedtothex.org/roller/kyle/entry/articles-jenkins-email">Standard email setup in Jenkins</a></li>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-postfix-as-a-send-only-smtp-server-on-ubuntu-14-04">Setup Postfix</a></li>
<li><a href="http://docs.blowb.org/setup-host/postfix.html">Configure Postfix for Docker Containers</a></li>
<li><a href="http://satishgandham.com/2016/12/sending-email-from-docker-through-postfix-installed-on-the-host/">More on Postfix for Docker Containers</a></li>
</ul>


<pre><code class="plain postfix version used in this post">[tdongsi@dev-worker-1 ~]$ postconf -v | grep mail_version
mail_version = 2.10.1
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubernetes: Kube-router]]></title>
    <link href="http://tdongsi.github.io/blog/2017/05/15/kubernetes-kube-router/"/>
    <updated>2017-05-15T10:52:34-07:00</updated>
    <id>http://tdongsi.github.io/blog/2017/05/15/kubernetes-kube-router</id>
    <content type="html"><![CDATA[<p>Kubernetes is one of the <a href="http://www.infoworld.com/article/3118345/cloud-computing/why-kubernetes-is-winning-the-container-war.html">most active open-source project</a> right now.
I&rsquo;m trying to keep up with interesting updates from the Kubernetes community.
This <code>kube-router</code> project is one of them although I&rsquo;ve not get an idea how stable or useful it is.</p>

<p><blockquote><p>Kube-router is a distributed load balancer, firewall and router for Kubernetes. Kube-router can be configured to provide on each cluster node:<br/>* IPVS/LVS based service proxy on each node for ClusterIP and NodePort service types, providing service discovery and load balancing<br/>* an ingress firewall for the pods running on the node as per the defined Kubernetes network policies using iptables and ipset<br/>* a BGP router to advertise and learn the routes to the pod IP&rsquo;s for cross-node pod-to-pod connectivity</p></blockquote></p>

<!--more-->


<p>A few notes on related works in Kubernetes community:</p>

<ul>
<li>The most obvious one is <code>kube-proxy</code> service, which is included in the standard Kubernetes installations. This <code>kube-router</code> can be a replacement for <code>kube-proxy</code> in the future.</li>
<li>Another related work is <a href="https://github.com/kubernetes/kubernetes/issues/44063">IPVS-based in-cluster service load balancing</a>.
Huawei presented this work at Kubecon 2016.
IIRC, it is implemented as a flag to kube-proxy and considerable performance improvement was reported.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Troubleshooting Docker-out-of-Docker]]></title>
    <link href="http://tdongsi.github.io/blog/2017/04/26/troubleshooting-docker-out-of-docker/"/>
    <updated>2017-04-26T16:24:24-07:00</updated>
    <id>http://tdongsi.github.io/blog/2017/04/26/troubleshooting-docker-out-of-docker</id>
    <content type="html"><![CDATA[<p>In this blog post, we are using <a href="/blog/2017/04/23/docker-out-of-docker/">&ldquo;Docker out of Docker&rdquo; approach</a> to build Docker images in our containerized Jenkins slaves.
We look into a problem usually encountered in that approach, especially when reusing a Docker image for another Kubernetes cluster.</p>

<!--more-->


<h3>Problem description</h3>

<p>We got the following error when running Docker inside a Jenkins slave container.</p>

<pre><code class="plain Error message when running Docker">+ docker images
Cannot connect to the Docker daemon. Is the docker daemon running on this host?
</code></pre>

<h3>Discussion</h3>

<p>In summary, for <a href="/blog/2017/04/23/docker-out-of-docker/">&ldquo;Docker out of Docker&rdquo; approach</a>, the basic requirements to enable building Docker images in a containerized Jenkins slave is:</p>

<ol>
<li>You&rsquo;ll need to mount &ldquo;/var/run/docker.sock&rdquo; as a volume at &ldquo;/var/run/docker.sock&rdquo;.</li>
<li>Having <code>docker</code> CLI installed in the containerized Jenkins slave.</li>
<li>Make sure &ldquo;/var/run/docker.sock&rdquo; has the right permission inside the Jenkins slave container: readable for the current user (e.g., user <code>jenkins</code>) or in &ldquo;docker&rdquo; group.</li>
</ol>


<p>The direct cause of the above error message &ldquo;Cannot connect to the Docker daemon&rdquo; is that the socket &ldquo;/var/run/docker.sock&rdquo; to <code>docker</code> daemon on that Jenkins slave does not have the right permission for the current user (<code>jenkins</code> in the example).
By convention, the read permission to that Unix domain socket &ldquo;/var/run/docker.sock&rdquo; is given to <code>root</code> user or users in <code>docker</code> group.
The following commands verify that it is not:</p>

<pre><code class="plain Show GID of docker group">+ ls -l /var/run/docker.sock

srw-rw----. 1 root 992 0 Mar 14 00:57 /var/run/docker.sock
+ cat /etc/group
...
docker:x:999:jenkins
</code></pre>

<p>The expected output of the above <code>ls</code> command is as follows:</p>

<pre><code class="plain Expected output">+ ls -l /var/run/docker.sock
srw-rw----. 1 root docker 0 Mar 14 00:57 /var/run/docker.sock
</code></pre>

<p>The root cause of the problem is that the Docker image of Jenkins slave is built inside another Kubernetes cluster (see example Dockerfile below).
The group <code>docker</code> happens to have the group ID 999 on that Kubernetes cluster.</p>

<pre><code class="plain Dockerfile for installing Docker CLI in Jenkins slave http://stackoverflow.com/questions/31466812/access-docker-sock-from-inside-a-container">FROM jenkins

USER root
ENV DEBIAN_FRONTEND=noninteractive
ENV HOME /home/jenkins
ENV DOCKER_VERSION=1.9.1-0~trusty

RUN apt-get update \
  &amp;&amp; apt-get install -y docker-engine=$DOCKER_VERSION \
  &amp;&amp; rm -rf /var/lib/apt/lists/*

RUN usermod -a -G docker jenkins
</code></pre>

<p>For illustration, the Docker installation steps in Ubuntu are similar:</p>

<pre><code class="plain Installing Docker CLI https://docs.docker.com/engine/installation/linux/linux-postinstall/"># Install from Web
sudo curl -sSL https://get.docker.com/ | sh
sudo usermod -aG docker jenkins

# Install from apt
sudo apt-get update
sudo apt-get install -y docker-engine
sudo usermod -aG docker jenkins
</code></pre>

<p>The last step <code>usermod</code> comes from Docker documentation itself: &ldquo;If you would like to use Docker as a non-root user, you should now consider adding your user to the "docker&rdquo; group".</p>

<h3>Resolving problem</h3>

<p>To resolve the problem, simply entering the Docker image, update its <code>/etc/group</code> file with the correct GID for <code>docker</code> group.
In the example above, the line &ldquo;docker:x:999:jenkins&rdquo; should be updated to &ldquo;docker:x:992:jenkins&rdquo; to make it work.
It&rsquo;s recommended to run <code>docker commit</code> to save the modified container as a new Docker image and push it to Docker registry (similar process in <a href="http://localhost:4000/blog/2017/01/25/docker-root-user-in-a-pod/">this post</a>).</p>

<h3>References</h3>

<ul>
<li><a href="https://docs.docker.com/engine/reference/commandline/dockerd/">dockerd</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
