<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Kubernetes | Personal Programming Notes]]></title>
  <link href="http://tdongsi.github.io/blog/categories/kubernetes/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/"/>
  <updated>2017-10-11T18:02:55-07:00</updated>
  <id>http://tdongsi.github.io/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Sending Emails From Docker Containers]]></title>
    <link href="http://tdongsi.github.io/blog/2017/05/25/sending-emails-from-docker-containers/"/>
    <updated>2017-05-25T13:42:45-07:00</updated>
    <id>http://tdongsi.github.io/blog/2017/05/25/sending-emails-from-docker-containers</id>
    <content type="html"><![CDATA[<p>In this post, we looks into how to set up notification emails at the end of CI pipelines in a containerized Jenkins system.
First, we look into conventional Jenkins system (directly hosted) that has direct communication to the SMTP server.
After that, we will look into adjustments required for a containerized Jenkins system to run in the same environment.</p>

<!--more-->


<h3>Sending emails in standard Jenkins setup</h3>

<p>We first look at a typical Jenkins setup, where the Jenkins instance is installed directly on a host machine (VM or bare-metal) and has direct communication to the SMTP server.
For corporate network, you may have to use an SMTP relay server instead.
For those cases, you can configure SMTP communication by <a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-postfix-as-a-send-only-smtp-server-on-ubuntu-14-04">setting up Postfix</a>.
In CentOS, it could be a simple &ldquo;sudo yum install -y mailx&rdquo;.</p>

<p>After installing, update <em>/etc/postfix/main.cf</em> with correct relay information: myhostname, myorigin, mydestination, relayhost, alias_maps, alias_database.
An example <em>/etc/postfix/main.cf</em> is shown below:</p>

<pre><code class="plain /etc/postfix/main.cf example"># See /usr/share/postfix/main.cf.bak for a commented, more complete version

myhostname = dev-worker-1.example.com
smtpd_banner = $myhostname ESMTP $mail_name
biff = no

# appending .domain is the MUA's job.
append_dot_mydomain = no

# Uncomment the next line to generate "delayed mail" warnings
#delay_warning_time = 4h

readme_directory = no

# TLS parameters
smtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem
smtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key
smtpd_use_tls=yes

# See /usr/share/doc/postfix/TLS_README.gz in the postfix-doc package for
# information on enabling SSL in the smtp client.


alias_maps = hash:/etc/aliases
alias_database = hash:/etc/aliases
myorigin = dev-worker-1.example.com
mydestination = dev-worker-1.example.com, localhost.example.com, localhost
relayhost = smtprelay-prd.example.com
mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128
mailbox_size_limit = 0
recipient_delimiter = +
inet_interfaces = localhost
inet_protocols = all
</code></pre>

<p>We can test the setup by sending a test email with the following command:</p>

<pre><code class="plain Send a test email">[tdongsi@dev-worker-1 ~]# echo "Test localhost" | mailx -s Test tdongsi@example.com
send-mail: warning: inet_protocols: disabling IPv6 name/address support: Address family not supported by protocol
postdrop: warning: inet_protocols: disabling IPv6 name/address support: Address family not supported by protocol
</code></pre>

<p>After the <code>postfix</code> service is up, Jenkins can be configured to send email with <a href="https://wiki.jenkins-ci.org/display/JENKINS/Mailer">Mailer plugin</a>.
Mail server can be configured in <strong>Manage Jenkins</strong> page, <strong>E-mail Notification</strong> section.
Please visit <a href="http://www.nailedtothex.org/roller/kyle/entry/articles-jenkins-email">Kohei Nozaki&rsquo;s blog post</a> for more detailed instructions and screenshots.
We can also test the configuration by sending test e-mail in the same <strong>E-mail Notification</strong> section.</p>

<h3>Sending email from container</h3>

<p>Many Jenkins-based CI systems have been containerized and deployed on Kubernetes cluster (in conjunction with <a href="https://wiki.jenkins-ci.org/display/JENKINS/Kubernetes+Plugin">Kubernetes plugin</a>).
For email notifications in such CI systems, one option is to reuse <code>postfix</code> service, which is usually configured and ready on the Kubernetes nodes, and expose it to the Docker containers.</p>

<p>There are two changes need to be made on Postfix to expose it to Docker containers on one host.</p>

<ol>
<li>Exposing Postfix to the docker network, that is, Postfix must be configured to bind to localhost as well as the docker network.</li>
<li>Accepting all incoming connections which come from any Docker containers.</li>
</ol>


<p>Docker bridge (<code>docker0</code>) acts a a bridge between your ethernet port and docker containers so that data can go back and forth.
We achieve the first requirement by adding the IP of <code>docker0</code> to <code>inet_iterfaces</code>.</p>

<pre><code class="plain ifconfig example output">[centos@dev-worker-1 ~]$ ifconfig
docker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1472
        inet 172.22.91.1  netmask 255.255.255.0  broadcast 0.0.0.0
        ether 02:42:88:5f:24:28  txqueuelen 0  (Ethernet)
        RX packets 8624183  bytes 18891507332 (17.5 GiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 15891332  bytes 16911210191 (15.7 GiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

flannel0: flags=4305&lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST&gt;  mtu 1472
        inet 172.22.91.0  netmask 255.255.0.0  destination 172.22.91.0
        unspec 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00  txqueuelen 500  (UNSPEC)
        RX packets 10508237  bytes 7051646109 (6.5 GiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 15511583  bytes 18744591891 (17.4 GiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
</code></pre>

<p>For the second requirement, the whole docker network as well as localhost should be added to <code>mynetworks</code>.
In our kubernetes setup, the docker network should be <code>flannel0</code> and its subnet&rsquo;s CIDR notation is added to the <code>mynetworks</code> line:</p>

<pre><code class="plain Modified "/etc/postfix/main.cf""># See /usr/share/postfix/main.cf.bak for a commented, more complete version

myhostname = dev-worker-1.example.com
smtpd_banner = $myhostname ESMTP $mail_name
biff = no

# appending .domain is the MUA's job.
append_dot_mydomain = no

# Uncomment the next line to generate "delayed mail" warnings
#delay_warning_time = 4h

readme_directory = no

# TLS parameters
smtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem
smtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key
smtpd_use_tls=yes

# See /usr/share/doc/postfix/TLS_README.gz in the postfix-doc package for
# information on enabling SSL in the smtp client.

alias_maps = hash:/etc/aliases
alias_database = hash:/etc/aliases
myorigin = dev-worker-1.example.com
mydestination = dev-worker-1.example.com, localhost.example.com, localhost
relayhost = smtprelay-prd.example.com
mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128 172.22.0.0/16
mailbox_size_limit = 0
recipient_delimiter = +
inet_interfaces = localhost, 172.22.91.1
inet_protocols = all
</code></pre>

<p>Note the differences in <code>inet_interfaces</code> and <code>mynetworks</code> from the last section.
One can simply enter the Docker container/Kubernetes pod to verify such setup.
Note that application <code>mailx</code> maybe not available in a container since we tend to keep the containers light-weight.
Instead, prepare a <code>sendmail.txt</code> file (based on <a href="http://docs.blowb.org/setup-host/postfix.html">this</a>) with the following SMTP commands and use <code>nc</code> to send out the email as shown below.</p>

<pre><code class="plain Send test email from container">mymac:k8s tdongsi$ kubectl --kubeconfig kubeconfig --namespace jenkins exec -it jenkins-8hgsn -- bash -il

jenkins@jenkins-8hgsn:~/test$ cat sendmail.txt
HELO x
MAIL FROM: test@example.com
RCPT TO: tdongsi@example.com
DATA
From: test@example.com
To: $YOUR_EMAIL
Subject: This is a test

The test is successful

.
quit

jenkins@jenkins-8hgsn:~/test$ nc 172.22.91.1 25 &lt;sendmail.txt
220 dev-worker-1.eng.sfdc.net ESMTP Postfix
250 dev-worker-1.eng.sfdc.net
250 2.1.0 Ok
250 2.1.5 Ok
354 End data with &lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt;
250 2.0.0 Ok: queued as 1EF9E60C34
221 2.0.0 Bye
</code></pre>

<p>For containerized Jenkins system, mail server can also be configured in same <strong>Manage Jenkins</strong> page, <strong>E-mail Notification</strong> section.
The only difference is the IP/hostname provided to <strong>SMTP server</strong> option.
Instead of providing the known SMTP server&rsquo;s IP and host, one should use the IP of <code>docker0</code>, as explained above.
In the case of many nodes in Kubernetes cluster with different <code>docker0</code> IP, the Docker container of Jenkins master should reside only on one host and <code>docker0</code>&rsquo;s IP on that host should be used.</p>

<h3>References</h3>

<ul>
<li><a href="http://www.nailedtothex.org/roller/kyle/entry/articles-jenkins-email">Standard email setup in Jenkins</a></li>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-postfix-as-a-send-only-smtp-server-on-ubuntu-14-04">Setup Postfix</a></li>
<li><a href="http://docs.blowb.org/setup-host/postfix.html">Configure Postfix for Docker Containers</a></li>
<li><a href="http://satishgandham.com/2016/12/sending-email-from-docker-through-postfix-installed-on-the-host/">More on Postfix for Docker Containers</a></li>
</ul>


<pre><code class="plain postfix version used in this post">[tdongsi@dev-worker-1 ~]$ postconf -v | grep mail_version
mail_version = 2.10.1
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubernetes: Kube-router]]></title>
    <link href="http://tdongsi.github.io/blog/2017/05/15/kubernetes-kube-router/"/>
    <updated>2017-05-15T10:52:34-07:00</updated>
    <id>http://tdongsi.github.io/blog/2017/05/15/kubernetes-kube-router</id>
    <content type="html"><![CDATA[<p>Kubernetes is one of the <a href="http://www.infoworld.com/article/3118345/cloud-computing/why-kubernetes-is-winning-the-container-war.html">most active open-source project</a> right now.
I&rsquo;m trying to keep up with interesting updates from the Kubernetes community.
This <code>kube-router</code> project is one of them although I&rsquo;ve not get an idea how stable or useful it is.</p>

<p><blockquote><p>Kube-router is a distributed load balancer, firewall and router for Kubernetes. Kube-router can be configured to provide on each cluster node:<br/>* IPVS/LVS based service proxy on each node for ClusterIP and NodePort service types, providing service discovery and load balancing<br/>* an ingress firewall for the pods running on the node as per the defined Kubernetes network policies using iptables and ipset<br/>* a BGP router to advertise and learn the routes to the pod IP&rsquo;s for cross-node pod-to-pod connectivity</p></blockquote></p>

<!--more-->


<p>A few notes on related works in Kubernetes community:</p>

<ul>
<li>The most obvious one is <code>kube-proxy</code> service, which is included in the standard Kubernetes installations. This <code>kube-router</code> can be a replacement for <code>kube-proxy</code> in the future.</li>
<li>Another related work is <a href="https://github.com/kubernetes/kubernetes/issues/44063">IPVS-based in-cluster service load balancing</a>.
Huawei presented this work at Kubecon 2016.
IIRC, it is implemented as a flag to kube-proxy and considerable performance improvement was reported.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Troubleshooting Docker-out-of-Docker]]></title>
    <link href="http://tdongsi.github.io/blog/2017/04/26/troubleshooting-docker-out-of-docker/"/>
    <updated>2017-04-26T16:24:24-07:00</updated>
    <id>http://tdongsi.github.io/blog/2017/04/26/troubleshooting-docker-out-of-docker</id>
    <content type="html"><![CDATA[<p>In this blog post, we are using <a href="/blog/2017/04/23/docker-out-of-docker/">&ldquo;Docker out of Docker&rdquo; approach</a> to build Docker images in our containerized Jenkins slaves.
We look into a problem usually encountered in that approach, especially when reusing a Docker image for another Kubernetes cluster.</p>

<!--more-->


<h3>Problem description</h3>

<p>We got the following error when running Docker inside a Jenkins slave container.</p>

<pre><code class="plain Error message when running Docker">+ docker images
Cannot connect to the Docker daemon. Is the docker daemon running on this host?
</code></pre>

<h3>Discussion</h3>

<p>In summary, for <a href="/blog/2017/04/23/docker-out-of-docker/">&ldquo;Docker out of Docker&rdquo; approach</a>, the basic requirements to enable building Docker images in a containerized Jenkins slave is:</p>

<ol>
<li>You&rsquo;ll need to mount &ldquo;/var/run/docker.sock&rdquo; as a volume at &ldquo;/var/run/docker.sock&rdquo;.</li>
<li>Having <code>docker</code> CLI installed in the containerized Jenkins slave.</li>
<li>Make sure &ldquo;/var/run/docker.sock&rdquo; has the right permission inside the Jenkins slave container: readable for the current user (e.g., user <code>jenkins</code>) or in &ldquo;docker&rdquo; group.</li>
</ol>


<p>The direct cause of the above error message &ldquo;Cannot connect to the Docker daemon&rdquo; is that the socket &ldquo;/var/run/docker.sock&rdquo; to <code>docker</code> daemon on that Jenkins slave does not have the right permission for the current user (<code>jenkins</code> in the example).
By convention, the read permission to that Unix domain socket &ldquo;/var/run/docker.sock&rdquo; is given to <code>root</code> user or users in <code>docker</code> group.
The following commands verify that it is not:</p>

<pre><code class="plain Show GID of docker group">+ ls -l /var/run/docker.sock

srw-rw----. 1 root 992 0 Mar 14 00:57 /var/run/docker.sock
+ cat /etc/group
...
docker:x:999:jenkins
</code></pre>

<p>The expected output of the above <code>ls</code> command is as follows:</p>

<pre><code class="plain Expected output">+ ls -l /var/run/docker.sock
srw-rw----. 1 root docker 0 Mar 14 00:57 /var/run/docker.sock
</code></pre>

<p>The root cause of the problem is that the Docker image of Jenkins slave is built inside another Kubernetes cluster (see example Dockerfile below).
The group <code>docker</code> happens to have the group ID 999 on that Kubernetes cluster.</p>

<pre><code class="plain Dockerfile for installing Docker CLI in Jenkins slave http://stackoverflow.com/questions/31466812/access-docker-sock-from-inside-a-container">FROM jenkins

USER root
ENV DEBIAN_FRONTEND=noninteractive
ENV HOME /home/jenkins
ENV DOCKER_VERSION=1.9.1-0~trusty

RUN apt-get update \
  &amp;&amp; apt-get install -y docker-engine=$DOCKER_VERSION \
  &amp;&amp; rm -rf /var/lib/apt/lists/*

RUN usermod -a -G docker jenkins
</code></pre>

<p>For illustration, the Docker installation steps in Ubuntu are similar:</p>

<pre><code class="plain Installing Docker CLI https://docs.docker.com/engine/installation/linux/linux-postinstall/"># Install from Web
sudo curl -sSL https://get.docker.com/ | sh
sudo usermod -aG docker jenkins

# Install from apt
sudo apt-get update
sudo apt-get install -y docker-engine
sudo usermod -aG docker jenkins
</code></pre>

<p>The last step <code>usermod</code> comes from Docker documentation itself: &ldquo;If you would like to use Docker as a non-root user, you should now consider adding your user to the "docker&rdquo; group".</p>

<h3>Resolving problem</h3>

<p>To resolve the problem, simply entering the Docker image, update its <code>/etc/group</code> file with the correct GID for <code>docker</code> group.
In the example above, the line &ldquo;docker:x:999:jenkins&rdquo; should be updated to &ldquo;docker:x:992:jenkins&rdquo; to make it work.
It&rsquo;s recommended to run <code>docker commit</code> to save the modified container as a new Docker image and push it to Docker registry (similar process in <a href="http://localhost:4000/blog/2017/01/25/docker-root-user-in-a-pod/">this post</a>).</p>

<h3>References</h3>

<ul>
<li><a href="https://docs.docker.com/engine/reference/commandline/dockerd/">dockerd</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker-in-Docker vs Docker-out-of-Docker]]></title>
    <link href="http://tdongsi.github.io/blog/2017/04/23/docker-out-of-docker/"/>
    <updated>2017-04-23T10:42:04-07:00</updated>
    <id>http://tdongsi.github.io/blog/2017/04/23/docker-out-of-docker</id>
    <content type="html"><![CDATA[<p>In this post, we look into two different approaches to solving the problem of building/pushing Docker images from a containerized Jenkins system.
From that understanding, we will discuss the pros and cons of each approach in production Continuous Integration (CI) systems and how one should be used in practice.</p>

<!--more-->


<h3>Docker-in-Docker &amp; Docker-out-of-Docker</h3>

<p>Jenkins as a CI system has been increasingly containerized and ran as a Docker container in production.
An example setup is to run Jenkins on top of a Kubernetes cluster with Jenkins slaves are created on demand as containers, using <a href="https://wiki.jenkins-ci.org/display/JENKINS/Kubernetes+Plugin">Kubernetes plugin</a>.
The problem in this post arises from how to build/run/push the Docker images insides a Jenkins system that run as a Docker container itself.</p>

<p>&ldquo;Docker-in-Docker&rdquo; refers to the approach of installing and running another Docker engine (daemon) inside Docker containers.
Since Docker 0.6, a &ldquo;privileged&rdquo; option is added to allow running containers in a special mode with almost all capabilities of the host machine, including kernel features and devices acccess.
As a consequence, Docker engine, as a privileged application, can run inside a Docker container itself.</p>

<p>&ldquo;Docker-in-Docker&rdquo; is first discussed by Jerome Petazzoni in <a href="https://blog.docker.com/2013/09/docker-can-now-run-within-docker/">this blog post</a> with example codes.
However, in <a href="https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/">another following blog post</a>, Jerome cautioned against using his &ldquo;dind&rdquo; approach in containerized Jenkins.
He pointed out potential problems with his &ldquo;Docker-in-Docker&rdquo; approach and how to avoid those by simply bind-mounting the Docker socket into the Jenkins container.
The approach of bind-mounting the Docker socket is later referred as &ldquo;Docker-out-of-Docker&rdquo; approach.</p>

<h3>Which one should we use?</h3>

<p>As spelled out clearly by &ldquo;Docker-in-Docker&rdquo; creator Jerome Petazzoni himself, we should not use Docker-in-Docker, especially in containerized Jenkins systems.
Potential problems include 1) security profile of inner Docker will conflict with one of outer Docker 2) Incompatible file systems (e.g. AUFS inside Docker container).</p>

<p>Instead of trying to run Docker engine inside containers, it is advised to just expose the Docker socket to those containers.
This can be done by bind-mounting with the <code>-v</code> flag:</p>

<pre><code class="plain Docker out of Docker">docker run -v /var/run/docker.sock:/var/run/docker.sock ...
</code></pre>

<p>By using the above command, we can access the Docker daemon (running on the host machine) from inside the Docker container, and able to start/build/push containers.
The containers that are started inside the Docker container above are effectively &ldquo;sibling&rdquo; containers instead of &ldquo;child&rdquo; containers since the outer and inner containers are all running on the same host machine.
However, it is important to note that this feels like &ldquo;Docker-in-Docker&rdquo; but without any tricky problems associated with this.
And for the purpose of building/running/pushing Docker images in containerized Jenkins systems, this &ldquo;Docker-out-of-Docker&rdquo; is exactly all we need.</p>

<h3>Further discussion</h3>

<p>The potential issues of &ldquo;Docker-in-Docker&rdquo; is extensively discussed by Jerome Petazzoni in his blog post.
However, what&rsquo;s not mentioned is any potential problem of &ldquo;Docker-out-of-Docker&rdquo; approach.</p>

<p>In my opinion, one potential issue of &ldquo;Docker-out-of-Docker&rdquo; approach is one can access the outer Docker container from the inner container through &ldquo;/var/run/docker.sock&rdquo;.
In the context of containerized Jenkins system, the outer Docker container is usually Jenkins master with sensitive information.
The inside Docker containers are usually Jenkins slaves that are subject to running all kinds of code which might be malicious.
This means that a containerized Jenkins system can be easily compromised if there is no limit on what&rsquo;s running in Jenkins slaves.</p>

<p>It should be noted that, despite of problems listed by Jerome, &ldquo;Docker-in-Docker&rdquo; approach is still a possible choice *IF* you know what you are doing.
Conflict of security profiles can be resolved with the right, careful setup.
There are work-arounds for incompatible file systems between the containers.
With the right setup, &ldquo;Docker-in-Docker&rdquo; can provide essentially free build isolation and security, which is a must for many, especially in corporates.
However, the ever-present disadvantage of this apporach is long build time for large Docker images since Docker image cache has to be re-populated every run.
As noted by Jerome, this cache is designed for exclusive access by one single Docker daemon at once.
Trying to link this cache in each container to some common, pre-populated Docker image cache will lead to corrupt image data.</p>

<h3>References</h3>

<ul>
<li><a href="https://blog.docker.com/2013/09/docker-can-now-run-within-docker/">Docker-in-Docker</a></li>
<li><a href="https://github.com/jpetazzo/dind">dind</a></li>
<li><a href="https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/">Docker-out-of-Docker</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker: Root User in a Pod]]></title>
    <link href="http://tdongsi.github.io/blog/2017/01/25/docker-root-user-in-a-pod/"/>
    <updated>2017-01-25T18:22:51-08:00</updated>
    <id>http://tdongsi.github.io/blog/2017/01/25/docker-root-user-in-a-pod</id>
    <content type="html"><![CDATA[<p>In the following scenario, we have some pod running in Kubernetes cluster.</p>

<pre><code>tdongsi-mac:kubernetes tdongsi$ kubectl --kubeconfig kubeconfig describe pod jenkins
Name:               jenkins
Namespace:          default
Image(s):           docker.registry.company.net/tdongsi/jenkins:2.23
Node:               kube-worker-1/10.252.158.72
Start Time:         Tue, 24 Jan 2017 16:57:47 -0800
Labels:             name=jenkins
Status:             Running
Reason:
Message:
IP:             172.17.27.3
Replication Controllers:    &lt;none&gt;
Containers:
  jenkins:
    Container ID:   docker://943d6e55038804c8
    Image:      docker.registry.company.net/tdongsi/jenkins:2.23
    Image ID:       docker://242c1836544e5ca31616
    State:      Running
      Started:      Tue, 24 Jan 2017 16:57:48 -0800
    Ready:      True
    Restart Count:  0
    Environment Variables:
Conditions:
  Type      Status
  Ready     True
Volumes:
  jenkins-data:
    Type:   HostPath (bare host directory volume)
    Path:   /jdata
No events. 
</code></pre>

<p>For troubleshooting purposes, we sometimes need to enter the container or execute some commands with root privilege.
Sometimes, we simply cannot <code>sudo</code> or have the root password.</p>

<!--more-->


<pre><code>jenkins@jenkins:~$ sudo ls /etc/hosts
[sudo] password for jenkins:
Sorry, try again.
</code></pre>

<p>Modifying the Docker image to set root password (e.g., by editing <code>Dockerfile</code> and rebuild) is sometimes not an option,
such as when the Docker image is downloaded from another source and read-only.
Moreover, if the container is running in production, we don&rsquo;t want to stop the container while troubleshooting some temporary issues.</p>

<h3><code>nsenter</code> approach</h3>

<p>I found one way to enter a &ldquo;live&rdquo; container as root by using <code>nsenter</code>.
In summary, we find the process ID of the target container and provide it to <code>nsenter</code> as an argument.
In the case of a Kuberentes cluster, we need to find which Kubernetes slave the pod is running on and log into it to execute the following <code>docker</code> commands.</p>

<pre><code class="plain Finding running container ID and name">[centos@kube-worker-1 ~]$ sudo docker ps
CONTAINER ID        IMAGE                                              COMMAND                CREATED             STATUS              PORTS               NAMES
943d6e5a3bb8        docker.registry.company.net/tdongsi/jenkins:2.23   "/usr/local/bin/tini   25 hours ago        Up 25 hours                             k8s_jenkins.6e7c865_...
fadfc479f24e        gcr.io/google_containers/pause:0.8.0               "/pause"               25 hours ago        Up 25 hours                             k8s_POD.9243e30_...
</code></pre>

<p>Use <code>docker inspect</code> to find the process ID based on the container ID.
The Go template <code>{ {.State.Pid} }</code> (NOTE: without space) is used to simplify the output to a single numerical Pid.</p>

<pre><code class="plain">[centos@kube-worker-1 ~]$ sudo docker inspect --format { {.State.Pid} } 943d6e5a3bb8
9176

[centos@kube-worker-1 ~]$ sudo nsenter --target 9176 --mount --uts --ipc --net --pid
root@jenkins:/# cd ~
root@jenkins:~# vi /etc/hosts
root@jenkins:~# exit
</code></pre>

<p>For later versions of Docker, the more direct way is to use <code>docker exec</code> with the container name shown in <code>docker ps</code> output (see next section).
However, note that <code>docker exec</code> might not work for earlier versions of Docker (tested with Docker 1.6) and <code>nsenter</code> must be used instead.</p>

<p>After entering the container as <code>root</code>, you might want to add the user into sudo group and save the modified Docker image.</p>

<pre><code>[centos@kube-worker-3 ~]$ sudo nsenter --target 17377 --mount --uts --ipc --net --pid
root@node-v4:~# cd /home/jenkins
root@node-v4:/home/jenkins# usermod -a -G sudo jenkins
root@node-v4:/home/jenkins# passwd jenkins
Enter new UNIX password:
Retype new UNIX password:
passwd: password updated successfully
root@node-v4:/home/jenkins# exit
logout

[centos@kube-worker-3 ~]$ sudo docker commit --author tdongsi --message "Add Jenkins password" \
280e5237cc6a docker.registry.company.net/tdongsi/jenkins-agent:2.80
b1fe6c66195e32fcb8ef4974e3d6228ee2f4cf46ab08dbc074f633d95005941b

[centos@kube-worker-3 ~]$ sudo docker push docker.registry.company.net/tdongsi/jenkins-agent:2.80
The push refers to a repository [docker.registry.company.net/tdongsi/jenkins-agent] (len: 1)
b1fe6c66195e: Image already exists
151c68e860a5: Image successfully pushed
670d6fd894d6: Image successfully pushed
...
</code></pre>

<p>After that, you can verify <code>sudo</code>ing in the new Docker image.</p>

<pre><code>tdongsi-mac:~ tdongsi$ docker pull docker.registry.company.net/tdongsi/jenkins-agent:2.80
2.80: Pulling from tdongsi/jenkins-agent
bf5d46315322: Already exists
9f13e0ac480c: Already exists
ebe26e644840: Pull complete
40af181810e7: Pull complete
...

tdongsi-mac:~ tdongsi$ docker run -d --restart=always --entrypoint="java" \
docker.registry.company.net/tdongsi/jenkins-agent:2.80 -jar /usr/share/jenkins/slave.jar \
-jnlpUrl http://10.252.78.115/computer/slave/slave-agent.jnlp
dd9c207e2ef1c0520439451b1775b976e3c9e09712f8ca1fb42f1bc082f14809

tdongsi-mac:~ tdongsi$ docker ps
CONTAINER ID        IMAGE                                                    COMMAND                  CREATED             STATUS              PORTS               NAMES
dd9c207e2ef1        docker.registry.company.net/tdongsi/jenkins-agent:2.80   "java -jar /usr/sh..."   5 seconds ago       Up 4 seconds                            ecstatic_galileo
tdongsi-mac:~ tdongsi$ docker exec -it dd9c207e2ef1 bash
jenkins@dd9c207e2ef1:~$ sudo ls /etc/hosts
[sudo] password for jenkins:
/etc/hosts
jenkins@dd9c207e2ef1:~$ sudo cat /etc/hosts
127.0.0.1   localhost
::1 localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.17.0.2  dd9c207e2ef1
jenkins@dd9c207e2ef1:~$ exit
exit
</code></pre>

<h3><code>docker exec</code> approach</h3>

<p>Later versions of <code>docker</code> adds <code>--user</code> flag that allows us to specify which user that we should enter the container as.
First, we figure out which Kubernetes node is running a particular pod by using the command <code>kubectl describe pod</code>.
After <code>ssh</code>-ing into that Kubernetes node, we can find the corresponding container running in that pod with the command <code>docker ps -a</code>.
The following examples demonstrate entering a <code>jenkins-slave</code> container as <code>root</code> and <code>jenkins</code> user.</p>

<pre><code class="plain Entering container ">[root@dev-worker-2 ~]# docker ps -a
CONTAINER ID        IMAGE                                                                        COMMAND                  CREATED             STATUS              PORTS               NAMES
10f031d08389        docker.registry.company.net/tdongsi/jenkins:jenkins-agent                    "jenkins-slave 9f22f2"   19 minutes ago      Up 19 minutes                           k8s_slave.beb667bf_...
767915746e2c        docker.registry.company.net/tdongsi/pause:2.0                                "/pause"                 19 minutes ago      Up 19 minutes                           k8s_POD.abb8e705_...

[root@dev-worker-2 ~]# docker exec -it --user root 10f031d08389 /bin/sh
#
# ls
support  workspace
# id
uid=0(root) gid=0(root) groups=0(root)
# exit

[root@dev-worker-2 ~]# docker exec -it --user jenkins 10f031d08389 /bin/sh
$ ls
support  workspace
$ id
uid=25001(jenkins) gid=25001(jenkins) groups=25001(jenkins),992(docker)
$ exit
</code></pre>

<p>As mentioned, older versions of <code>docker</code> does not support <code>--user</code> flag and does not allow entering container as root.
In that case, use <code>nsenter</code> method presented in the previous section.</p>

<pre><code class="plain Unsupported operation on Docker 1.6">[root@kube-worker-1 ~]# docker exec -it --user root af9a884eb3f1 /bin/sh
flag provided but not defined: --user
See 'docker exec --help'.
[root@kube-worker-1 ~]# docker version
Client version: 1.6.2.el7
Client API version: 1.18
Go version (client): go1.4.2
Git commit (client): c3ca5bb/1.6.2
OS/Arch (client): linux/amd64
Server version: 1.6.2.el7
Server API version: 1.18
Go version (server): go1.4.2
Git commit (server): c3ca5bb/1.6.2
OS/Arch (server): linux/amd64
</code></pre>

<h3>References</h3>

<ul>
<li><a href="https://github.com/jpetazzo/nsenter">nsenter tool</a></li>
<li><a href="https://docs.docker.com/engine/reference/commandline/exec/">docker exec</a> manual</li>
<li><a href="http://stackoverflow.com/questions/28721699/root-password-inside-a-docker-container">StackOverflow discussion</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
