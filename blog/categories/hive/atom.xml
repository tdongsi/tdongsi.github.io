<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Hive | Personal Programming Notes]]></title>
  <link href="http://tdongsi.github.io/blog/categories/hive/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/"/>
  <updated>2015-11-24T14:41:17-08:00</updated>
  <id>http://tdongsi.github.io/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Programming Hive (Pt. 3): Runtime Modes]]></title>
    <link href="http://tdongsi.github.io/blog/2015/11/24/programming-hive-getting-started/"/>
    <updated>2015-11-24T18:24:30-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/11/24/programming-hive-getting-started</id>
    <content type="html"><![CDATA[<p><img class="center" src="/images/hive/cat.gif" title="Cover" ></p>

<p>In addition to <a href="/blog/2015/11/23/programming-hive-hive-cli/">Hive CLI</a>, chapter 2 of the <strong>Programming Hive</strong> book also covers some lower-level details of Hive such as Hadoop runtime modes and metastore.</p>

<h3>Runtime Modes</h3>

<p>There are different runtime modes for Hadoop. Because Hive uses Hadoop jobs for most of its work, its behavior is dependent on Hadoop runtime mode that you are using. However, even in distributed mode, Hive can decide on a per-query basis if it can perform the query using just local mode to provide better turnaround.</p>

<table>
<thead>
<tr>
<th> Local Mode </th>
<th> Distributed Mode </th>
<th> Pseudodistributed Mode </th>
</tr>
</thead>
<tbody>
<tr>
<td> Filesystem references use local filesystem. </td>
<td> Filesystem referenes use HDFS. </td>
<td> Similar to distributed mode. </td>
</tr>
<tr>
<td> MapReduce tasks in same process. </td>
<td>  MapReduce tasks in separate <br>processes, managed by JobTracker service. </td>
<td> Similar to distributed mode.</td>
</tr>
<tr>
<td> Default mode. </td>
<td> Usually configured for server clusters. </td>
<td> Like a cluster of one node.</td>
</tr>
</tbody>
</table>


<p><br></p>

<p>Pseudodistributed mode is mainly for developers working on personal machines or VM&rsquo;s when testing their applications since local mode doesn’t fully reflect the behavior of a real cluster. Changes to configuration are done by editing the <code>hive-site.xml</code> file in <code>$HIVE_HOME/conf</code> folder (e.g., <code>/usr/lib/hive/conf</code> on Cloudera VM). Create one if it doesn’t already exist.</p>

<h3>Metastore Using JDBC</h3>

<p>Hive requires only one extra component that Hadoop does not already have; the metastore component. The metastore stores metadata such as table schema and partition information that you specify when you run commands such as create table x&hellip;, or alter table y&hellip;, etc. Any JDBC-compliant database can be used for the metastore. In practice, most installations of Hive use MySQL. In <code>hive-site.xml</code> file, the metastore database configuration looks like this:</p>

<pre><code class="xml">  &lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
    &lt;value&gt;jdbc:mysql://127.0.0.1/metastore?createDatabaseIfNotExist=true&lt;/value&gt;
    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
    &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
    &lt;value&gt;hive&lt;/value&gt;
  &lt;/property&gt;
</code></pre>

<p>The information stored in metatstore is typically much smaller than the data stored in Hive. Therefore, you typically don’t need a powerful dedicated database server for the metastore. However since it represents a Single Point of Failure (SPOF), it is strongly recommended that you replicate and back up this database using the best practices like any other database instances.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Programming Hive (Pt. 2): Hive CLI]]></title>
    <link href="http://tdongsi.github.io/blog/2015/11/23/programming-hive-hive-cli/"/>
    <updated>2015-11-23T19:47:23-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/11/23/programming-hive-hive-cli</id>
    <content type="html"><![CDATA[<p><img class="center" src="/images/hive/cat.gif" title="Cover" ></p>

<p>Chapter 2 of the book covers how to get started with Hive and some basics of Hive, including its command-line interface (CLI).</p>

<h3>Starting Hive with Cloundera Quickstart VM</h3>

<p>On Cloudera Quickstart VM, the cores of its Hive distribution, including files such as <code>hive-exec*.jar</code> and <code>hive-metastore*.jar</code>, can be found in <code>/usr/lib/hive/lib</code>. The Hive executables can be found in <code>/usr/lib/hive/bin</code>. Running <code>hive</code> without any parameter will start Hive&rsquo;s CLI.</p>

<pre><code>[cloudera@quickstart temp]$ hive

Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
hive&gt; CREATE TABLE x (a INT);
OK
Time taken: 3.032 seconds
hive&gt; SELECT * FROM x;
OK
Time taken: 0.465 seconds
hive&gt; SELECT *        
    &gt; FROM x;
OK
Time taken: 0.049 seconds
hive&gt; DROP TABLE x;
OK
Time taken: 0.348 seconds
hive&gt; exit;
</code></pre>

<h3>Hive services</h3>

<p>The <code>hive</code> shell command is actually a wrapper to multiple Hive services, including the CLI.</p>

<pre><code>[cloudera@quickstart temp]$ hive --help
Usage ./hive &lt;parameters&gt; --service serviceName &lt;service parameters&gt;
Service List: beeline cli help hiveserver2 hiveserver hwi jar lineage metastore metatool orcfiledump rcfilecat schemaTool version 
Parameters parsed:
  --auxpath : Auxillary jars 
  --config : Hive configuration directory
  --service : Starts specific service/component. cli is default
Parameters used:
  HADOOP_HOME or HADOOP_PREFIX : Hadoop install directory
  HIVE_OPT : Hive options
For help on a particular service:
  ./hive --service serviceName --help
Debug help:  ./hive --debug --help
</code></pre>

<p>Note the list of services following the line &ldquo;Service List&rdquo;. There are several services available, most notably <strong>cli, hwi, jar, metastore</strong>. You can use <code>--service name</code> option to invoke a service. CLI is the default service, not specifying any service in <code>hive</code> command will run CLI service, as shown above.</p>

<p>For example, to run <a href="https://cwiki.apache.org/confluence/display/Hive/HiveWebInterface">Hive Web Interface</a>, run the service <strong>hwi</strong>. On Cloudera Quickstart VM, you might encounter this error:</p>

<pre><code>[cloudera@quickstart temp]$ hive --service hwi
ls: cannot access /usr/lib/hive/lib/hive-hwi-*.war: No such file or directory
15/11/23 20:22:50 INFO hwi.HWIServer: HWI is starting up
15/11/23 20:22:50 FATAL hwi.HWIServer: HWI WAR file not found at /usr/lib/hive/usr/lib/hive/lib/hive-hwi-0.8.1-cdh4.0.0.jar
</code></pre>

<p>To fix that error, edit the config file <code>hive-site.xml</code> in the <code>config</code> folder (e.g., <code>/usr/lib/hive/conf/hive-site.xml</code> on Cloudera VM) to point to the right location of HWI&rsquo;s war file. On Clouder Quickstart VM, the WAR file property block should look like this:</p>

<pre><code>...
 &lt;property&gt;
    &lt;name&gt;hive.hwi.war.file&lt;/name&gt;
    &lt;value&gt;/lib/hive-hwi.jar&lt;/value&gt;
    &lt;description&gt;This is the WAR file with the jsp content for Hive Web Interface&lt;/description&gt;
  &lt;/property&gt;
...
</code></pre>

<p>Running the <strong>hwi</strong> service again using <code>hive</code> command should work. In order to access the Hive Web Interface, go to <code>[Hive Server Address]</code>:9999/hwi on your web browser.</p>

<pre><code>[cloudera@quickstart temp]$ hive --service hwi
ls: cannot access /usr/lib/hive/lib/hive-hwi-*.war: No such file or directory
15/11/23 20:31:27 INFO hwi.HWIServer: HWI is starting up
15/11/23 20:31:27 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
15/11/23 20:31:27 INFO mortbay.log: jetty-6.1.26.cloudera.4
15/11/23 20:31:27 INFO mortbay.log: Extract /usr/lib/hive/lib/hive-hwi.jar to /tmp/Jetty_0_0_0_0_9999_hive.hwi.0.13.1.cdh5.3.0.jar__hwi__.lcik1p/webapp
15/11/23 20:31:28 INFO mortbay.log: Started SocketConnector@0.0.0.0:9999
</code></pre>

<h3>Hive CLI</h3>

<p>Available options for Hive CLI can be displayed as follows:</p>

<pre><code>[cloudera@quickstart temp]$ hive --help --service cli
usage: hive
 -d,--define &lt;key=value&gt;          Variable subsitution to apply to hive
                                  commands. e.g. -d A=B or --define A=B
    --database &lt;databasename&gt;     Specify the database to use
 -e &lt;quoted-query-string&gt;         SQL from command line
 -f &lt;filename&gt;                    SQL from files
 -H,--help                        Print help information
 -h &lt;hostname&gt;                    connecting to Hive Server on remote host
    --hiveconf &lt;property=value&gt;   Use value for given property
    --hivevar &lt;key=value&gt;         Variable subsitution to apply to hive
                                  commands. e.g. --hivevar A=B
 -i &lt;filename&gt;                    Initialization SQL file
 -p &lt;port&gt;                        connecting to Hive Server on port number
 -S,--silent                      Silent mode in interactive shell
 -v,--verbose                     Verbose mode (echo executed SQL to the
                                  console)
</code></pre>

<h4>Hive variables and properties</h4>

<p>The <code>--define key=value</code> option is equivalent to the <code>--hivevar key=value</code> option. Both let you define custom variables in the <code>hivevar</code> namespace, separate from three other built-in namespaces, <code>hiveconf</code>, <code>system</code>, and <code>env</code>. By convention, the Hive namespaces for variables and properties are as follows:</p>

<ol>
<li>hivevar: user-defined custom variables.</li>
<li>hiveconf: Hive-specific configuration properties.</li>
<li>system: Java configuration properties.</li>
<li>env: (Read-only) environment variables by shell environment (e.g., bash).</li>
</ol>


<p>Inside Hive CLI, the command <code>SET</code> is used to display and change variables. For example:</p>

<pre><code>[cloudera@quickstart temp]$ hive

Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
hive&gt; set env:HOME; &lt;-- display HOME variable in env namespace
env:HOME=/home/cloudera
hive&gt; set; &lt;-- display all variables
...
hive&gt; set -v; &lt;-- display even more variables
...
hive&gt; set hivevar:foo=bar; &lt;-- set foo variable in hivevar namespace to bar
</code></pre>

<h4><code>-e query_string</code> and <code>-S</code> options</h4>

<p><code>-e</code> option allows you to execute a list of semicolon-separated queries as an input string. <code>-S</code> option for silent mode will remove non-essential output. For example:</p>

<pre><code>$ hive -e "SELECT * FROM mytable LIMIT 3";
OK
name1 10
name2 20
name3 30
Time taken: 4.955 seconds
</code></pre>

<pre><code>$ hive -S -e "select * FROM mytable LIMIT 3"
name1 10
name2 20
name3 30
</code></pre>

<p><strong>Tip</strong>: To quickly search for the fullname of a property that you only remember part of its name, pipe the Hive&rsquo;s <code>SET</code> command output to grep. For example:</p>

<pre><code>[cloudera@quickstart temp]$ hive -S -e "set" | grep warehouse
hive.metastore.warehouse.dir=/user/hive/warehouse
hive.warehouse.subdir.inherit.perms=true
</code></pre>

<h4><code>-f script_file</code> option</h4>

<p>This option allows you to execute one or more queries contained in a script file. If you are already within the Hive CLI, you can use the <code>SOURCE</code> command to execute a script file. For example:</p>

<pre><code>$ cat /path/to/file/withqueries.hql
SELECT x.* FROM src x;
$ hive
hive&gt; source /path/to/file/withqueries.hql;
</code></pre>

<h4><code>-i filename</code> option</h4>

<p>This option lets you specify an initialization file with a list of commands for the CLI to run when it starts. The default initialization file is the file <code>$HOME/.hiverc</code> if it exists.</p>

<h4>Tips</h4>

<ul>
<li>To print column headers (disabled by default), set the hiveconf property <code>hive.cli.print.header</code> to true: <code>set hive.cli.print.header=true;</code>.</li>
<li>Hive does have command history, saved into a file <code>$HOME/.hivehistory</code>. Use the up and down arrow keys to scroll through previous commands.</li>
<li>To run HDFS commands from within Hive CLI, drop the hdfs. For example:</li>
</ul>


<pre><code>hive&gt; dfs -ls input; 
Found 1 items
-rw-r--r--   1 cloudera cloudera         31 2015-01-15 18:04 input/wordcount.txt
</code></pre>

<ul>
<li>To run the bash shell commands from within Hive CLI, prefix <code>!</code> before the bash commands and terminate the line with a semicolon (;). Note that interactive commands, shell pipes <code>|</code>, and file globs <code>*</code> will not work. Example:</li>
</ul>


<pre><code>hive&gt; !pwd;
hive&gt; /home/cloudera/temp
</code></pre>

<ul>
<li>Set the property <code>set hive.exec.mode.local.auto=true;</code> to use local mode more aggressively and gain performance in Hive queries, especially when working with small data sets.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Programming Hive (Pt. 1): Introduction]]></title>
    <link href="http://tdongsi.github.io/blog/2015/11/22/programming-hive-chapter-1/"/>
    <updated>2015-11-22T17:22:51-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/11/22/programming-hive-chapter-1</id>
    <content type="html"><![CDATA[<!---
"Chapter 1: Introduction" of the "Programming Hive" book.
-->


<p>This post is the first of many posts summarizing the <strong>Programming Hive</strong> book, with some observations from my own experience.</p>

<p><img class="center" src="/images/hive/cat.gif" title="Cover" ></p>

<h3>Introduction</h3>

<p>Hive provides a SQL dialect, called Hive Query Language (HiveQL or HQL) for querying data stored in a Hadoop cluster. SQL knowledge is widespread for a reason; it&rsquo;s an effective, reasonably intuitive model for organizing and using data. Therefore, Hive helps lower the barrier, making transition to Hadoop from traditional relational databases easier for expert database designers and administrators.</p>

<p>Note that Hive is more suited for data warehouse applications, where data is relatively static and fast response time is not required. For example, a simple query such as <code>select count(*) from my_table</code> can take several seconds for a very small table (mostly due to startup overhead for MapReduce jobs). Hive is a heavily batch-oriented system: in addition to large startup overheads, it neither provides record-level update, insert, or delete nor transactions. In short, Hive is not a full database (hint: check HBase).</p>

<p>HiveQL does not conform to the ANSI SQL standard (not many do), but it is quite close to MySQL dialect.</p>

<h3>Hive within the Hadoop Ecosystem</h3>

<p>A basic understanding of Hadoop and MapReduce can help you to understand and appreciate how Hive works. Simple examples such as WordCount in my <a href="/blog/2015/11/21/explaining-wordcount-example/">last post</a> can be very involving when using the Hadoop Java API. The API requires Java developers to manage many low-level details, repetitive wiring to/from Mappers and Reducers. The WordCount example&rsquo;s Java implementation can be found <a href="https://wiki.apache.org/hadoop/WordCount">here</a>.</p>

<p>Hive not only eliminates advanced, sometimes repetitive Java coding but also provides a familiar interface to those who know SQL. Hive lets you complete a lot of work with relatively little effort. For example, the same WordCount example in HiveQL can be as simple as:</p>

<pre><code class="sql WordCount example in HiveQL">CREATE TABLE docs (line STRING);

/* Load text files into TABLE docs: each line as a row */
LOAD DATA INPATH 'wordcount.txt' OVERWRITE INTO TABLE docs;

CREATE TABLE word_counts AS
SELECT word, count(1) AS count
FROM
   -- explode will return rows of tokens
  (SELECT explode(split(line, '\s')) AS word
   FROM docs) w
GROUP BY word
ORDER BY word;
</code></pre>

<p>In the remaining sections of Chapter 1, the authors also discuss various related Hadoop projects such as Pig, Hue, HBase, Spark, Storm, Kafka, etc.</p>
]]></content>
  </entry>
  
</feed>
