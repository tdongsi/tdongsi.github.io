<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | Personal Programming Notes]]></title>
  <link href="http://tdongsi.github.io/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/"/>
  <updated>2017-05-30T00:49:12-07:00</updated>
  <id>http://tdongsi.github.io/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Sending Emails From Docker Containers]]></title>
    <link href="http://tdongsi.github.io/blog/2017/05/25/sending-emails-from-docker-containers/"/>
    <updated>2017-05-25T13:42:45-07:00</updated>
    <id>http://tdongsi.github.io/blog/2017/05/25/sending-emails-from-docker-containers</id>
    <content type="html"><![CDATA[<p>In this post, we looks into sending notification emails at the end of CI pipelines in a containerized Jenkins system.</p>

<h3>Sending emails in standard Jenkins setup</h3>

<p>We first look at a typical Jenkins setup, where the Jenkins instance is installed directly on a host machine (VM or bare-metal) and has direct communication to the SMTP server.
For corporate network, you may have to use an SMTP relay server instead.
For those cases, you can configure SMTP communication by <a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-postfix-as-a-send-only-smtp-server-on-ubuntu-14-04">setting up Postfix</a>.
Its typical settings is defined in <em>/etc/postfix/main.cf</em> file like this:</p>

<pre><code class="plain /etc/postfix/main.cf example"># See /usr/share/postfix/main.cf.bak for a commented, more complete version

myhostname = dev-worker-1.example.com
smtpd_banner = $myhostname ESMTP $mail_name
biff = no

# appending .domain is the MUA's job.
append_dot_mydomain = no

# Uncomment the next line to generate "delayed mail" warnings
#delay_warning_time = 4h

readme_directory = no

# TLS parameters
smtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem
smtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key
smtpd_use_tls=yes

# See /usr/share/doc/postfix/TLS_README.gz in the postfix-doc package for
# information on enabling SSL in the smtp client.


alias_maps = hash:/etc/aliases
alias_database = hash:/etc/aliases
myorigin = dev-worker-1.example.com
mydestination = dev-worker-1.example.com, localhost.example.com, localhost
relayhost = smtprelay-prd.example.com
mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128
mailbox_size_limit = 0
recipient_delimiter = +
inet_interfaces = localhost
inet_protocols = all
</code></pre>

<p>We can test the setup by sending a test email with the following command:</p>

<pre><code class="plain Send a test email">[tdongsi@dev-worker-1 ~]# echo "Test localhost" | mailx -s Test tdongsi@example.com
send-mail: warning: inet_protocols: disabling IPv6 name/address support: Address family not supported by protocol
postdrop: warning: inet_protocols: disabling IPv6 name/address support: Address family not supported by protocol
</code></pre>

<p>After the <code>postfix</code> service is up, Jenkins can be configured to send email with <a href="https://wiki.jenkins-ci.org/display/JENKINS/Mailer">Mailer plugin</a>.
Mail server can be configured in <strong>Manage Jenkins</strong> page, <strong>E-mail Notification</strong> section.
Please visit <a href="http://www.nailedtothex.org/roller/kyle/entry/articles-jenkins-email">this post</a> for more detailed instructions and screenshots.
We can also test the configuration by sending test e-mail in the same <strong>E-mail Notification</strong> section.</p>

<h3>Sending email from container</h3>

<p>Many Jenkins-based CI systems have been containerized and deployed on Kubernetes cluster (in conjunction with <a href="https://wiki.jenkins-ci.org/display/JENKINS/Kubernetes+Plugin">Kubernetes plugin</a>).
For email notifications in such CI systems, one option is to reuse <code>postfix</code> service, which is usually configured and ready on the Kubernetes nodes, and expose it to the Docker containers.</p>

<p>There are two changes need to be made on Postfix to expose it to Docker containers on one host.</p>

<ol>
<li>Exposing Postfix to the docker network, that is, Postfix must be configured to bind to localhost as well as the docker network.</li>
<li>Accepting all incoming connections which come from any Docker containers.</li>
</ol>


<p>Docker bridge (<code>docker0</code>) acts a a bridge between your ethernet port and docker containers so that data can go back and forth.
We achieve the first requirement by adding the IP of <code>docker0</code> to <code>inet_iterfaces</code>.</p>

<pre><code class="plain ifconfig example output">[centos@dev-worker-1 ~]$ ifconfig
docker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1472
        inet 172.22.91.1  netmask 255.255.255.0  broadcast 0.0.0.0
        ether 02:42:88:5f:24:28  txqueuelen 0  (Ethernet)
        RX packets 8624183  bytes 18891507332 (17.5 GiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 15891332  bytes 16911210191 (15.7 GiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

flannel0: flags=4305&lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST&gt;  mtu 1472
        inet 172.22.91.0  netmask 255.255.0.0  destination 172.22.91.0
        unspec 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00  txqueuelen 500  (UNSPEC)
        RX packets 10508237  bytes 7051646109 (6.5 GiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 15511583  bytes 18744591891 (17.4 GiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
</code></pre>

<p>For the second requirement, the whole docker network as well as localhost should be added to <code>mynetworks</code>.
In our kubernetes setup, the docker network should be <code>flannel0</code> and its subnet&rsquo;s CIDR notation is added to the <code>mynetworks</code> line:</p>

<pre><code class="plain Modified "/etc/postfix/main.cf""># See /usr/share/postfix/main.cf.bak for a commented, more complete version

myhostname = dev-worker-1.example.com
smtpd_banner = $myhostname ESMTP $mail_name
biff = no

# appending .domain is the MUA's job.
append_dot_mydomain = no

# Uncomment the next line to generate "delayed mail" warnings
#delay_warning_time = 4h

readme_directory = no

# TLS parameters
smtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem
smtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key
smtpd_use_tls=yes

# See /usr/share/doc/postfix/TLS_README.gz in the postfix-doc package for
# information on enabling SSL in the smtp client.

alias_maps = hash:/etc/aliases
alias_database = hash:/etc/aliases
myorigin = dev-worker-1.example.com
mydestination = dev-worker-1.example.com, localhost.example.com, localhost
relayhost = smtprelay-prd.example.com
mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128 172.22.0.0/16
mailbox_size_limit = 0
recipient_delimiter = +
inet_interfaces = localhost, 172.22.91.1
inet_protocols = all
</code></pre>

<p>Note the differences in <code>inet_interfaces</code> and <code>mynetworks</code> from the last section.
One can simply enter the Docker container/Kubernetes pod to verify such setup.
Note that application <code>mailx</code> maybe not available in a container since we tend to keep the containers light-weight.
Instead, prepare a <code>sendmail.txt</code> file (based on <a href="http://docs.blowb.org/setup-host/postfix.html">this</a>) with the following SMTP commands and use <code>nc</code> to send out the email as shown below.</p>

<pre><code class="plain Send test email from container">mymac:k8s tdongsi$ kubectl --kubeconfig kubeconfig --namespace jenkins exec -it jenkins-8hgsn -- bash -il

jenkins@jenkins-8hgsn:~/test$ cat sendmail.txt
HELO x
MAIL FROM: test@example.com
RCPT TO: tdongsi@example.com
DATA
From: test@example.com
To: $YOUR_EMAIL
Subject: This is a test

The test is successful

.
quit

jenkins@jenkins-8hgsn:~/test$ nc 172.22.91.1 25 &lt;sendmail.txt
220 dev-worker-1.eng.sfdc.net ESMTP Postfix
250 dev-worker-1.eng.sfdc.net
250 2.1.0 Ok
250 2.1.5 Ok
354 End data with &lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt;
250 2.0.0 Ok: queued as 1EF9E60C34
221 2.0.0 Bye
</code></pre>

<p>For containerized Jenkins system, mail server can also be configured in same <strong>Manage Jenkins</strong> page, <strong>E-mail Notification</strong> section.
The only difference is the IP/hostname provided to <strong>SMTP server</strong> option.
Instead of providing the known SMTP server&rsquo;s IP and host, one should use the IP of <code>docker0</code>, as explained above.
In the case of many nodes in Kubernetes cluster with different <code>docker0</code> IP, the Docker container of Jenkins master should reside only on one host and <code>docker0</code>&rsquo;s IP on that host should be used.</p>

<h3>References</h3>

<ul>
<li><a href="http://www.nailedtothex.org/roller/kyle/entry/articles-jenkins-email">Standard email setup in Jenkins</a></li>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-postfix-as-a-send-only-smtp-server-on-ubuntu-14-04">Setup Postfix</a></li>
<li><a href="http://docs.blowb.org/setup-host/postfix.html">Configure Postfix for Docker Containers</a></li>
<li><a href="http://satishgandham.com/2016/12/sending-email-from-docker-through-postfix-installed-on-the-host/">More on Postfix for Docker Containers</a></li>
</ul>


<pre><code class="plain postfix version used in this post">[tdongsi@dev-worker-1 ~]$ postconf -v | grep mail_version
mail_version = 2.10.1
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker: Copy File Into a Container]]></title>
    <link href="http://tdongsi.github.io/blog/2017/02/09/docker-copy-file-into-a-container/"/>
    <updated>2017-02-09T15:17:19-08:00</updated>
    <id>http://tdongsi.github.io/blog/2017/02/09/docker-copy-file-into-a-container</id>
    <content type="html"><![CDATA[<p>In the following scenario, we have a running Docker container or a running pod in Kubernetes cluster.
We want to add some files into the running containers to fix some issue, verify, and commit the changes.</p>

<h3>Best-case scenario: <code>docker cp</code></h3>

<p>The most obvious way is to create a Dockerfile and rebuild the Docker image.
The Dockerfile will look like this:</p>

<pre><code class="plain Dockerfile">FROM olderImage
ADD myfile /path/myfile
...
</code></pre>

<p>However, in this approach, we need to stop the Docker containers, update, and re-run with the new Docker images.
It does not work if we want to work with <strong><em>running</em></strong> containers.
For running containers, the better way to add files into containers is to copy files into containers.
For the more updated versions of Docker (1.8+), the recommended way for copying is to use <a href="https://docs.docker.com/engine/reference/commandline/cp/"><code>docker cp</code> command</a>.</p>

<h3>Copy file directly</h3>

<p><code>docker cp</code> does not always work, especially in older versions of Docker.
In older versions of Docker, the <code>docker cp</code> command only allowed copying files from a <strong>container</strong> to the <strong>host</strong>.
Only since Docker 1.8, copying files from the host to a container is added.
You will get some error with unhelpful messages like this in older versions of Docker:</p>

<pre><code class="plain Unsupported "docker cp"">[centos@comp ~]$ ls maven_3.3.9-3_all.deb
maven_3.3.9-3_all.deb

[centos@comp ~]$ sudo docker cp maven_3.3.9-3_all.deb 9a8d782156ca:/home/jenkins
FATA[0000] Error: Path not specified
[centos@comp ~]$ sudo docker cp ./maven_3.3.9-3_all.deb 9a8d782156ca:/home/jenkins
FATA[0000] Error: Path not specified
[centos@comp ~]$ sudo docker cp ./maven_3.3.9-3_all.deb 9a8d782156ca:/home/jenkins/
FATA[0000] Error: Path not specified
[centos@comp ~]$ sudo docker cp maven_3.3.9-3_all.deb 9a8d782156ca:/home/jenkins/maven_3.3.9-3_all.deb
FATA[0000] Error: Path not specified
</code></pre>

<p>If you find yourself stuck with older versions of Docker, the alternative is to manually copy the files from hosts filesystem to containers filesystem location.
First, you need to determine where the containers filesystem (volume) is mounted on the host:</p>

<pre><code class="plain Using inspect to find Volume location">[centos@comp ~]$ sudo docker ps
CONTAINER ID      IMAGE    COMMAND ...
9a8d782156ca

[centos@comp ~]$ sudo docker inspect -f { {.Id} } 9a8d782156ca
9a8d782156ca9a3bd59545a18943de408ca58f42c4389c12e9bb43f4ad239d52

[centos@comp ~]$ sudo docker inspect -f { {.Volumes} } 9a8d782156ca
map[/home/jenkins:/var/lib/docker/vfs/dir/b051cc2b086c53ce436ad82b9332ba79687f3ddcf8ee77e3f8264e7cafe32438]
[centos@comp ~]$ sudo ls /var/lib/docker/vfs/dir/b051cc2b086c53ce436ad82b9332ba79687f3ddcf8ee77e3f8264e7cafe32438
test.txt
</code></pre>

<p>NOTE: In the shell commands above, there is no space between <code>{</code> (space is added for Jekyll blog engine).
After the mounting path is determined, you can manipulate the container'ss filesystem directly, including copying files into it.</p>

<pre><code class="plain Directly copy file into containers filesystem">[centos@comp ~]$ sudo cp maven_3.3.9-3_all.deb /var/lib/docker/vfs/dir/b051cc2b086c53ce436ad82b9332ba79687f3ddcf8ee77e3f8264e7cafe32438
</code></pre>

<p>You can verify such manipulation by <code>docker exec</code>-ing into the container and verify the files:</p>

<pre><code class="plain Before and After">jenkins@9a8d782156ca:~$ ls
test.txt

jenkins@9a8d782156ca:~$ ls
maven_3.3.9-3_all.deb  test.txt
</code></pre>

<h3>Reference</h3>

<ul>
<li><a href="https://docs.docker.com/engine/reference/commandline/cp/">docker cp</a></li>
<li><a href="http://stackoverflow.com/questions/22907231/copying-files-from-host-to-docker-container">Stackoverflow discussion</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker: Override ENTRYPOINT]]></title>
    <link href="http://tdongsi.github.io/blog/2017/02/08/docker-override-entrypoint/"/>
    <updated>2017-02-08T16:08:02-08:00</updated>
    <id>http://tdongsi.github.io/blog/2017/02/08/docker-override-entrypoint</id>
    <content type="html"><![CDATA[<p>The docker image for JNLP-based Jenkins agent requires us to pass a few arguments.
Simply running such docker image will give the following error:</p>

<pre><code>mymac:jenkins tdongsi$ docker run --restart=always gcr.io/jenkins-agent:2.60
two arguments required, but got []
java -jar slave.jar [options...] &lt;secret key&gt; &lt;slave name&gt;
 -cert VAL                       : Specify additional X.509 encoded PEM
                                   certificates to trust when connecting to
                                   Jenkins root URLs. If starting with @ then
                                   the remainder is assumed to be the name of
                                   the certificate file to read.
 -credentials USER:PASSWORD      : HTTP BASIC AUTH header to pass in for making
                                   HTTP requests.
 -headless                       : Run in headless mode, without GUI
 -jar-cache DIR                  : Cache directory that stores jar files sent
                                   from the master
 -noreconnect                    : If the connection ends, don't retry and just
                                   exit.
 -proxyCredentials USER:PASSWORD : HTTP BASIC AUTH header to pass in for making
                                   HTTP authenticated proxy requests.
 -tunnel HOST:PORT               : Connect to the specified host and port,
                                   instead of connecting directly to Jenkins.
                                   Useful when connection to Hudson needs to be
                                   tunneled. Can be also HOST: or :PORT, in
                                   which case the missing portion will be
                                   auto-configured like the default behavior
 -url URL                        : Specify the Jenkins root URLs to connect to.
</code></pre>

<p>Most of the error messages above is from Jenkins binary <code>slave.jar</code> and has nothing to do with Docker.
To make the container run on Docker, we must override its <code>ENTRYPOINT</code> at runtime to provide the arguments required.
However, one common mistake while trying to override is as follows:</p>

<pre><code class="plain Standard mistake">mymac:jenkins tdongsi$ docker run --restart=always gcr.io/jenkins-agent:2.60 \
--entrypoint java -jar /usr/share/jenkins/slave.jar
"--entrypoint" is not a valid option
</code></pre>

<p>Except for passing argument to the <code>ENTRYPOINT</code>, the Docker image is usually the last parameter.
Another attempt to make it &ldquo;right&rdquo; is as follows:</p>

<pre><code class="plain Another attempt, still not working">mymac:jenkins tdongsi$ docker run --restart=always \
--entrypoint="java -jar /usr/share/jenkins/slave.jar" gcr.io/jenkins-agent:2.60

container_linux.go:247: starting container process caused "exec: \"java -jar /usr/share/jenkins/slave.jar\": 
stat java -jar /usr/share/jenkins/slave.jar: no such file or directory"
docker: Error response from daemon: oci runtime error: container_linux.go:247: starting container process 
caused "exec: \"java -jar /usr/share/jenkins/slave.jar\": stat java -jar /usr/share/jenkins/slave.jar: no such
 file or directory".
ERRO[0001] error getting events from daemon: net/http: request canceled
</code></pre>

<p>This attempt try to put the entire overridden command as the parameter for &ldquo;&ndash;entrypoint&rdquo; flag.
However, this does NOT work because, as stated in documentation, the entrypoint should specify the <strong>executable</strong>, not the command.
The correct way to do it is as follows:</p>

<pre><code>mymac:jenkins tdongsi$ docker run --restart=always --entrypoint="java" \
gcr.io/jenkins-agent:2.60 -jar /usr/share/jenkins/slave.jar \
-jnlpUrl http://10.252.78.115/computer/slave/slave-agent.jnlp

Failing to obtain http://10.252.78.115/computer/slave/slave-agent.jnlp
java.net.ConnectException: Connection refused
    at java.net.PlainSocketImpl.socketConnect(Native Method)
</code></pre>

<p>As seeen above, the executable is passed into &ldquo;&ndash;entrypoint&rdquo; flag, while its arguments are being passed <strong>after</strong> the image name.</p>

<h3>Reference</h3>

<ul>
<li><a href="https://docs.docker.com/engine/reference/run/">Guide of Docker Run</a></li>
<li><a href="https://docs.docker.com/engine/reference/commandline/run/">Docker Run CLI options</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker: Root User in a Pod]]></title>
    <link href="http://tdongsi.github.io/blog/2017/01/25/docker-root-user-in-a-pod/"/>
    <updated>2017-01-25T18:22:51-08:00</updated>
    <id>http://tdongsi.github.io/blog/2017/01/25/docker-root-user-in-a-pod</id>
    <content type="html"><![CDATA[<p>In the following scenario, we have some pod running in Kubernetes cluster.</p>

<pre><code>tdongsi-mac:kubernetes tdongsi$ kubectl --kubeconfig kubeconfig describe pod jenkins
Name:               jenkins
Namespace:          default
Image(s):           docker.registry.company.net/tdongsi/jenkins:2.23
Node:               kube-worker-1/10.252.158.72
Start Time:         Tue, 24 Jan 2017 16:57:47 -0800
Labels:             name=jenkins
Status:             Running
Reason:
Message:
IP:             172.17.27.3
Replication Controllers:    &lt;none&gt;
Containers:
  jenkins:
    Container ID:   docker://943d6e55038804c8
    Image:      docker.registry.company.net/tdongsi/jenkins:2.23
    Image ID:       docker://242c1836544e5ca31616
    State:      Running
      Started:      Tue, 24 Jan 2017 16:57:48 -0800
    Ready:      True
    Restart Count:  0
    Environment Variables:
Conditions:
  Type      Status
  Ready     True
Volumes:
  jenkins-data:
    Type:   HostPath (bare host directory volume)
    Path:   /jdata
No events. 
</code></pre>

<p>For troubleshooting purposes, we sometimes need to enter the container or execute some commands with root privilege.
Sometimes, we simply cannot <code>sudo</code> or have the root password.</p>

<pre><code>jenkins@jenkins:~$ sudo ls /etc/hosts
[sudo] password for jenkins:
Sorry, try again.
</code></pre>

<p>Modifying the Docker image to set root password (e.g., by editing <code>Dockerfile</code> and rebuild) is sometimes not an option,
such as when the Docker image is downloaded from another source and read-only.
Moreover, if the container is running in production, we don&rsquo;t want to stop the container while troubleshooting some temporary issues.</p>

<h3><code>nsenter</code> approach</h3>

<p>I found one way to enter a &ldquo;live&rdquo; container as root by using <code>nsenter</code>.
In summary, we find the process ID of the target container and provide it to <code>nsenter</code> as an argument.
In the case of a Kuberentes cluster, we need to find which Kubernetes slave the pod is running on and log into it to execute the following <code>docker</code> commands.</p>

<pre><code class="plain Finding running container ID and name">[centos@kube-worker-1 ~]$ sudo docker ps
CONTAINER ID        IMAGE                                              COMMAND                CREATED             STATUS              PORTS               NAMES
943d6e5a3bb8        docker.registry.company.net/tdongsi/jenkins:2.23   "/usr/local/bin/tini   25 hours ago        Up 25 hours                             k8s_jenkins.6e7c865_...
fadfc479f24e        gcr.io/google_containers/pause:0.8.0               "/pause"               25 hours ago        Up 25 hours                             k8s_POD.9243e30_...
</code></pre>

<p>Use <code>docker inspect</code> to find the process ID based on the container ID.
The Go template <code>{ {.State.Pid} }</code> (NOTE: without space) is used to simplify the output to a single numerical Pid.</p>

<pre><code class="plain">[centos@kube-worker-1 ~]$ sudo docker inspect --format { {.State.Pid} } 943d6e5a3bb8
9176

[centos@kube-worker-1 ~]$ sudo nsenter --target 9176 --mount --uts --ipc --net --pid
root@jenkins:/# cd ~
root@jenkins:~# vi /etc/hosts
root@jenkins:~# exit
</code></pre>

<p>For later versions of Docker, the more direct way is to use <code>docker exec</code> with the container name shown in <code>docker ps</code> output (see next section).
However, note that <code>docker exec</code> might not work for earlier versions of Docker (tested with Docker 1.6) and <code>nsenter</code> must be used instead.</p>

<p>After entering the container as <code>root</code>, you might want to add the user into sudo group and save the modified Docker image.</p>

<pre><code>[centos@kube-worker-3 ~]$ sudo nsenter --target 17377 --mount --uts --ipc --net --pid
root@node-v4:~# cd /home/jenkins
root@node-v4:/home/jenkins# usermod -a -G sudo jenkins
root@node-v4:/home/jenkins# passwd jenkins
Enter new UNIX password:
Retype new UNIX password:
passwd: password updated successfully
root@node-v4:/home/jenkins# exit
logout

[centos@kube-worker-3 ~]$ sudo docker commit --author tdongsi --message "Add Jenkins password" \
280e5237cc6a docker.registry.company.net/tdongsi/jenkins-agent:2.80
b1fe6c66195e32fcb8ef4974e3d6228ee2f4cf46ab08dbc074f633d95005941b

[centos@kube-worker-3 ~]$ sudo docker push docker.registry.company.net/tdongsi/jenkins-agent:2.80
The push refers to a repository [docker.registry.company.net/tdongsi/jenkins-agent] (len: 1)
b1fe6c66195e: Image already exists
151c68e860a5: Image successfully pushed
670d6fd894d6: Image successfully pushed
...
</code></pre>

<p>After that, you can verify <code>sudo</code>ing in the new Docker image.</p>

<pre><code>tdongsi-mac:~ tdongsi$ docker pull docker.registry.company.net/tdongsi/jenkins-agent:2.80
2.80: Pulling from tdongsi/jenkins-agent
bf5d46315322: Already exists
9f13e0ac480c: Already exists
ebe26e644840: Pull complete
40af181810e7: Pull complete
...

tdongsi-mac:~ tdongsi$ docker run -d --restart=always --entrypoint="java" \
docker.registry.company.net/tdongsi/jenkins-agent:2.80 -jar /usr/share/jenkins/slave.jar \
-jnlpUrl http://10.252.78.115/computer/slave/slave-agent.jnlp
dd9c207e2ef1c0520439451b1775b976e3c9e09712f8ca1fb42f1bc082f14809

tdongsi-mac:~ tdongsi$ docker ps
CONTAINER ID        IMAGE                                                    COMMAND                  CREATED             STATUS              PORTS               NAMES
dd9c207e2ef1        docker.registry.company.net/tdongsi/jenkins-agent:2.80   "java -jar /usr/sh..."   5 seconds ago       Up 4 seconds                            ecstatic_galileo
tdongsi-mac:~ tdongsi$ docker exec -it dd9c207e2ef1 bash
jenkins@dd9c207e2ef1:~$ sudo ls /etc/hosts
[sudo] password for jenkins:
/etc/hosts
jenkins@dd9c207e2ef1:~$ sudo cat /etc/hosts
127.0.0.1   localhost
::1 localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.17.0.2  dd9c207e2ef1
jenkins@dd9c207e2ef1:~$ exit
exit
</code></pre>

<h3><code>docker exec</code> approach</h3>

<p>Later versions of <code>docker</code> adds <code>--user</code> flag that allows us to specify which user that we should enter the container as.
First, we figure out which Kubernetes node is running a particular pod by using the command <code>kubectl describe pod</code>.
After <code>ssh</code>-ing into that Kubernetes node, we can find the corresponding container running in that pod with the command <code>docker ps -a</code>.
The following examples demonstrate entering a <code>jenkins-slave</code> container as <code>root</code> and <code>jenkins</code> user.</p>

<pre><code class="plain Entering container ">[root@dev-worker-2 ~]# docker ps -a
CONTAINER ID        IMAGE                                                                        COMMAND                  CREATED             STATUS              PORTS               NAMES
10f031d08389        docker.registry.company.net/tdongsi/jenkins:jenkins-agent                    "jenkins-slave 9f22f2"   19 minutes ago      Up 19 minutes                           k8s_slave.beb667bf_...
767915746e2c        docker.registry.company.net/tdongsi/pause:2.0                                "/pause"                 19 minutes ago      Up 19 minutes                           k8s_POD.abb8e705_...

[root@dev-worker-2 ~]# docker exec -it --user root 10f031d08389 /bin/sh
#
# ls
support  workspace
# id
uid=0(root) gid=0(root) groups=0(root)
# exit

[root@dev-worker-2 ~]# docker exec -it --user jenkins 10f031d08389 /bin/sh
$ ls
support  workspace
$ id
uid=25001(jenkins) gid=25001(jenkins) groups=25001(jenkins),992(docker)
$ exit
</code></pre>

<p>As mentioned, older versions of <code>docker</code> does not support <code>--user</code> flag and does not allow entering container as root.
In that case, use <code>nsenter</code> method presented in the previous section.</p>

<pre><code class="plain Unsupported operation on Docker 1.6">[root@kube-worker-1 ~]# docker exec -it --user root af9a884eb3f1 /bin/sh
flag provided but not defined: --user
See 'docker exec --help'.
[root@kube-worker-1 ~]# docker version
Client version: 1.6.2.el7
Client API version: 1.18
Go version (client): go1.4.2
Git commit (client): c3ca5bb/1.6.2
OS/Arch (client): linux/amd64
Server version: 1.6.2.el7
Server API version: 1.18
Go version (server): go1.4.2
Git commit (server): c3ca5bb/1.6.2
OS/Arch (server): linux/amd64
</code></pre>

<h3>References</h3>

<ul>
<li><a href="https://github.com/jpetazzo/nsenter">nsenter tool</a></li>
<li><a href="https://docs.docker.com/engine/reference/commandline/exec/">docker exec</a> manual</li>
<li><a href="http://stackoverflow.com/questions/28721699/root-password-inside-a-docker-container">StackOverflow discussion</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubernetes: Pod-to-Node Communication Loss]]></title>
    <link href="http://tdongsi.github.io/blog/2017/01/24/kubernetes-pod-to-node-communication-loss/"/>
    <updated>2017-01-24T15:05:15-08:00</updated>
    <id>http://tdongsi.github.io/blog/2017/01/24/kubernetes-pod-to-node-communication-loss</id>
    <content type="html"><![CDATA[<p>This post goes over what happens if we misconfigure <code>etcd</code> and <code>flannel</code> to use the same network (e.g., &ldquo;10.252.61.0/16&rdquo;) as the infrastructure (e.g., &ldquo;10.252.158.72&rdquo; node).
This newbie mistake is rare but very perplexing and this post shows how to troubleshoot it with <code>busybox</code> container.</p>

<h3>Problem symptoms</h3>

<p>From a pod (e.g., <code>jenkins</code>) on one node (e.g., <code>10.252.158.71</code>), we cannot communicate with another node (e.g., <code>10.252.158.72</code>) even though two nodes can communicate with each other normally.</p>

<pre><code class="plain">mymac:kubernetes tdongsi$ kubectl --kubeconfig kubeconfig exec -it jenkins -- bash -il
jenkins@jenkins:~$ ping 10.252.158.72
PING 10.252.158.72 (10.252.158.72) 56(84) bytes of data.
^C
--- 10.252.158.72 ping statistics ---
16 packets transmitted, 0 received, 100% packet loss, time 14999ms

jenkins@jenkins:~$ exit
</code></pre>

<p>Even more perplexing, the pod-to-pod communication is fine (as described right below), even though the second pod is on the same node (e.g., <code>10.252.158.72</code>) that the first pod cannot communciate to.</p>

<h3>Troubleshooting with <code>busybox</code></h3>

<p>Try to run a test pod <code>busybox</code>.
<code>jenkins</code> pod can ping the <code>busybox</code> pod, but not the node that <code>busybox</code> pod is running on.</p>

<pre><code>mymac:kubernetes tdongsi$ kubectl --kubeconfig kubeconfig run busybox \
--image=docker.registry.company.net/tdongsi/busybox --restart=Never --tty -i --generator=run-pod/v1
Waiting for pod default/busybox to be running, status is Pending, pod ready: false
Waiting for pod default/busybox to be running, status is Running, pod ready: false
Waiting for pod default/busybox to be running, status is Running, pod ready: false

mymac:kubernetes tdongsi$ kubectl --kubeconfig kubeconfig exec -it jenkins -- bash -il
jenkins@jenkins:~$ ping 10.252.61.7
PING 10.252.61.7 (10.252.61.7) 56(84) bytes of data.
64 bytes from 10.252.61.7: icmp_seq=1 ttl=62 time=0.540 ms
64 bytes from 10.252.61.7: icmp_seq=2 ttl=62 time=0.186 ms
64 bytes from 10.252.61.7: icmp_seq=3 ttl=62 time=0.177 ms
64 bytes from 10.252.61.7: icmp_seq=4 ttl=62 time=0.161 ms
64 bytes from 10.252.61.7: icmp_seq=5 ttl=62 time=0.187 ms
^C
--- 10.252.61.7 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 4000ms
rtt min/avg/max/mdev = 0.161/0.250/0.540/0.145 ms

jenkins@jenkins:~$ ping 10.252.158.72
PING 10.252.158.72 (10.252.158.72) 56(84) bytes of data.
^C
--- 10.252.158.72 ping statistics ---
14 packets transmitted, 0 received, 100% packet loss, time 13000ms
</code></pre>

<p>In this case, we would use <code>traceroute</code> from the <code>busybox</code> container to determine when the packets are dropped.
<code>10.252.158.72</code> is IP of the VM. <code>10.252.100.5</code> is the IP of the <code>jenkins</code> pod.</p>

<pre><code>mymac:kubernetes tdongsi$ kubectl --kubeconfig kubeconfig run busybox \
--image=docker.registry.company.net/tdongsi/busybox --restart=Never --tty -i --generator=run-pod/v1

Waiting for pod default/busybox to be running, status is Pending, pod ready: false
Waiting for pod default/busybox to be running, status is Running, pod ready: false
Waiting for pod default/busybox to be running, status is Running, pod ready: false

/ # traceroute 10.252.158.72
traceroute to 10.252.158.72 (10.252.158.72), 30 hops max, 46 byte packets
 1  10.252.61.1 (10.252.61.1)  0.005 ms  0.012 ms  0.001 ms
 2  *  *  *
 3  *  *  *
 4  *  *  *
 5  *  *  *
/ #
/ # traceroute 10.252.100.5
traceroute to 10.252.100.5 (10.252.100.5), 30 hops max, 46 byte packets
 1  10.252.61.1 (10.252.61.1)  0.005 ms  0.004 ms  0.002 ms
 2  *  10.252.100.0 (10.252.100.0)  0.487 ms  0.241 ms
 3  10.252.100.5 (10.252.100.5)  0.141 ms  0.563 ms  0.132 ms
/ # exit
</code></pre>

<p>For the context, <code>10.252.100.5</code> is the IP of the service, as shown in the command below.</p>

<pre><code>mymac:private_cloud tdongsi$ kubectl --kubeconfig kubeconfig describe services
Name:           jenkins
Namespace:      default
Labels:         &lt;none&gt;
Selector:       name=jenkins
Type:           NodePort
IP:         10.252.77.85
Port:           http    80/TCP
NodePort:       http    30080/TCP
Endpoints:      10.252.100.5:8080
Session Affinity:   None
No events.
</code></pre>

<h3>What went wrong?</h3>

<p>It&rsquo;s a newbie mistake when configuring Kubernetes.
When setting up <code>etcd</code> and configuring it to hold <code>flannel</code> configuration, it is important to pick an unused network.
I made a mistake for using <code>10.252.61.0/16</code> for flannel when some of my kubernetes nodes has IPs as &ldquo;10.252.xxx.xxx&rdquo;.
As a result, kube-proxy services intercept the traffic from the container and thinks its a virtual traffic since my node IP happens to be in the same subnet with <code>flanneld</code>.
This leads to pod-to-VM communication loss as described above.
The solution is simply reset flanneld with another subnet after resetting configruation value in <code>etcd</code> to &ldquo;172.17.0.0/16&rdquo;.</p>

<pre><code class="plain Update etcd">[centos@kube-master ~]$ etcdctl update /kube-centos/network/config \
"{ \"Network\": \"172.17.0.0/16\", \"SubnetLen\": 24, \"Backend\": { \"Type\": \"vxlan\" } }"

[centos@kube-master ~]$ etcdctl rm --recursive /kube-centos/network/subnets
[centos@kube-master ~]$ etcdctl ls /kube-centos/network
/kube-centos/network/config
</code></pre>

<p>After this, we can reset and restart <code>flannel</code> services on all nodes to use the new network overlay configuration.</p>
]]></content>
  </entry>
  
</feed>
