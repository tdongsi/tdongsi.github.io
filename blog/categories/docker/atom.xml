<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | Personal Programming Notes]]></title>
  <link href="http://tdongsi.github.io/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/"/>
  <updated>2017-05-16T16:56:32-07:00</updated>
  <id>http://tdongsi.github.io/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Docker: Override ENTRYPOINT]]></title>
    <link href="http://tdongsi.github.io/blog/2017/02/08/docker-override-entrypoint/"/>
    <updated>2017-02-08T16:08:02-08:00</updated>
    <id>http://tdongsi.github.io/blog/2017/02/08/docker-override-entrypoint</id>
    <content type="html"><![CDATA[<p>The docker image for JNLP-based Jenkins agent requires us to pass a few arguments.
Simply running such docker image will give the following error:</p>

<pre><code>mymac:jenkins tdongsi$ docker run --restart=always gcr.io/jenkins-agent:2.60
two arguments required, but got []
java -jar slave.jar [options...] &lt;secret key&gt; &lt;slave name&gt;
 -cert VAL                       : Specify additional X.509 encoded PEM
                                   certificates to trust when connecting to
                                   Jenkins root URLs. If starting with @ then
                                   the remainder is assumed to be the name of
                                   the certificate file to read.
 -credentials USER:PASSWORD      : HTTP BASIC AUTH header to pass in for making
                                   HTTP requests.
 -headless                       : Run in headless mode, without GUI
 -jar-cache DIR                  : Cache directory that stores jar files sent
                                   from the master
 -noreconnect                    : If the connection ends, don't retry and just
                                   exit.
 -proxyCredentials USER:PASSWORD : HTTP BASIC AUTH header to pass in for making
                                   HTTP authenticated proxy requests.
 -tunnel HOST:PORT               : Connect to the specified host and port,
                                   instead of connecting directly to Jenkins.
                                   Useful when connection to Hudson needs to be
                                   tunneled. Can be also HOST: or :PORT, in
                                   which case the missing portion will be
                                   auto-configured like the default behavior
 -url URL                        : Specify the Jenkins root URLs to connect to.
</code></pre>

<p>Most of the error messages above is from Jenkins binary <code>slave.jar</code> and has nothing to do with Docker.
To make the container run on Docker, we must override its <code>ENTRYPOINT</code> at runtime to provide the arguments required.
However, one common mistake while trying to override is as follows:</p>

<pre><code class="plain Standard mistake">mymac:jenkins tdongsi$ docker run --restart=always gcr.io/jenkins-agent:2.60 \
--entrypoint java -jar /usr/share/jenkins/slave.jar
"--entrypoint" is not a valid option
</code></pre>

<p>Except for passing argument to the <code>ENTRYPOINT</code>, the Docker image is usually the last parameter.
Another attempt to make it &ldquo;right&rdquo; is as follows:</p>

<pre><code class="plain Another attempt, still not working">mymac:jenkins tdongsi$ docker run --restart=always \
--entrypoint="java -jar /usr/share/jenkins/slave.jar" gcr.io/jenkins-agent:2.60

container_linux.go:247: starting container process caused "exec: \"java -jar /usr/share/jenkins/slave.jar\": 
stat java -jar /usr/share/jenkins/slave.jar: no such file or directory"
docker: Error response from daemon: oci runtime error: container_linux.go:247: starting container process 
caused "exec: \"java -jar /usr/share/jenkins/slave.jar\": stat java -jar /usr/share/jenkins/slave.jar: no such
 file or directory".
ERRO[0001] error getting events from daemon: net/http: request canceled
</code></pre>

<p>This attempt try to put the entire overridden command as the parameter for &ldquo;&ndash;entrypoint&rdquo; flag.
However, this does NOT work because, as stated in documentation, the entrypoint should specify the <strong>executable</strong>, not the command.
The correct way to do it is as follows:</p>

<pre><code>mymac:jenkins tdongsi$ docker run --restart=always --entrypoint="java" \
gcr.io/jenkins-agent:2.60 -jar /usr/share/jenkins/slave.jar \
-jnlpUrl http://10.252.78.115/computer/slave/slave-agent.jnlp

Failing to obtain http://10.252.78.115/computer/slave/slave-agent.jnlp
java.net.ConnectException: Connection refused
    at java.net.PlainSocketImpl.socketConnect(Native Method)
</code></pre>

<p>As seeen above, the executable is passed into &ldquo;&ndash;entrypoint&rdquo; flag, while its arguments are being passed <strong>after</strong> the image name.</p>

<h3>Reference</h3>

<ul>
<li><a href="https://docs.docker.com/engine/reference/run/">Guide of Docker Run</a></li>
<li><a href="https://docs.docker.com/engine/reference/commandline/run/">Docker Run CLI options</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker: Root User in a Pod]]></title>
    <link href="http://tdongsi.github.io/blog/2017/01/25/docker-root-user-in-a-pod/"/>
    <updated>2017-01-25T18:22:51-08:00</updated>
    <id>http://tdongsi.github.io/blog/2017/01/25/docker-root-user-in-a-pod</id>
    <content type="html"><![CDATA[<p>In the following scenario, we have some pod running in Kubernetes cluster.</p>

<pre><code>tdongsi-mac:kubernetes tdongsi$ kubectl --kubeconfig kubeconfig describe pod jenkins
Name:               jenkins
Namespace:          default
Image(s):           docker.registry.company.net/tdongsi/jenkins:2.23
Node:               kube-worker-1/10.252.158.72
Start Time:         Tue, 24 Jan 2017 16:57:47 -0800
Labels:             name=jenkins
Status:             Running
Reason:
Message:
IP:             172.17.27.3
Replication Controllers:    &lt;none&gt;
Containers:
  jenkins:
    Container ID:   docker://943d6e55038804c8
    Image:      docker.registry.company.net/tdongsi/jenkins:2.23
    Image ID:       docker://242c1836544e5ca31616
    State:      Running
      Started:      Tue, 24 Jan 2017 16:57:48 -0800
    Ready:      True
    Restart Count:  0
    Environment Variables:
Conditions:
  Type      Status
  Ready     True
Volumes:
  jenkins-data:
    Type:   HostPath (bare host directory volume)
    Path:   /jdata
No events. 
</code></pre>

<p>For troubleshooting purposes, we sometimes need to enter the container or execute some commands with root privilege.
Sometimes, we simply cannot <code>sudo</code> or have the root password.</p>

<pre><code>jenkins@jenkins:~$ sudo ls /etc/hosts
[sudo] password for jenkins:
Sorry, try again.
</code></pre>

<p>Modifying the Docker image to set root password (e.g., by editing <code>Dockerfile</code> and rebuild) is sometimes not an option,
such as when the Docker image is downloaded from another source and read-only.
Moreover, if the container is running in production, we don&rsquo;t want to stop the container while troubleshooting some temporary issues.</p>

<h3><code>nsenter</code> approach</h3>

<p>I found one way to enter a &ldquo;live&rdquo; container as root by using <code>nsenter</code>.
In summary, we find the process ID of the target container and provide it to <code>nsenter</code> as an argument.
In the case of a Kuberentes cluster, we need to find which Kubernetes slave the pod is running on and log into it to execute the following <code>docker</code> commands.</p>

<pre><code class="plain Finding running container ID and name">[centos@kube-worker-1 ~]$ sudo docker ps
CONTAINER ID        IMAGE                                              COMMAND                CREATED             STATUS              PORTS               NAMES
943d6e5a3bb8        docker.registry.company.net/tdongsi/jenkins:2.23   "/usr/local/bin/tini   25 hours ago        Up 25 hours                             k8s_jenkins.6e7c865_...
fadfc479f24e        gcr.io/google_containers/pause:0.8.0               "/pause"               25 hours ago        Up 25 hours                             k8s_POD.9243e30_...
</code></pre>

<p>Use <code>docker inspect</code> to find the process ID based on the container ID.
The Go template <code>{ {.State.Pid} }</code> (NOTE: without space) is used to simplify the output to a single numerical Pid.</p>

<pre><code class="plain">[centos@kube-worker-1 ~]$ sudo docker inspect --format { {.State.Pid} } 943d6e5a3bb8
9176

[centos@kube-worker-1 ~]$ sudo nsenter --target 9176 --mount --uts --ipc --net --pid
root@jenkins:/# cd ~
root@jenkins:~# vi /etc/hosts
root@jenkins:~# exit
</code></pre>

<p>For later versions of Docker, the more direct way is to use <code>docker exec</code> with the container name shown in <code>docker ps</code> output (see next section).
However, note that <code>docker exec</code> might not work for earlier versions of Docker (tested with Docker 1.6) and <code>nsenter</code> must be used instead.</p>

<p>After entering the container as <code>root</code>, you might want to add the user into sudo group and save the modified Docker image.</p>

<pre><code>[centos@kube-worker-3 ~]$ sudo nsenter --target 17377 --mount --uts --ipc --net --pid
root@node-v4:~# cd /home/jenkins
root@node-v4:/home/jenkins# usermod -a -G sudo jenkins
root@node-v4:/home/jenkins# passwd jenkins
Enter new UNIX password:
Retype new UNIX password:
passwd: password updated successfully
root@node-v4:/home/jenkins# exit
logout

[centos@kube-worker-3 ~]$ sudo docker commit --author tdongsi --message "Add Jenkins password" \
280e5237cc6a docker.registry.company.net/tdongsi/jenkins-agent:2.80
b1fe6c66195e32fcb8ef4974e3d6228ee2f4cf46ab08dbc074f633d95005941b

[centos@kube-worker-3 ~]$ sudo docker push docker.registry.company.net/tdongsi/jenkins-agent:2.80
The push refers to a repository [docker.registry.company.net/tdongsi/jenkins-agent] (len: 1)
b1fe6c66195e: Image already exists
151c68e860a5: Image successfully pushed
670d6fd894d6: Image successfully pushed
...
</code></pre>

<p>After that, you can verify <code>sudo</code>ing in the new Docker image.</p>

<pre><code>tdongsi-mac:~ tdongsi$ docker pull docker.registry.company.net/tdongsi/jenkins-agent:2.80
2.80: Pulling from tdongsi/jenkins-agent
bf5d46315322: Already exists
9f13e0ac480c: Already exists
ebe26e644840: Pull complete
40af181810e7: Pull complete
...

tdongsi-mac:~ tdongsi$ docker run -d --restart=always --entrypoint="java" \
docker.registry.company.net/tdongsi/jenkins-agent:2.80 -jar /usr/share/jenkins/slave.jar \
-jnlpUrl http://10.252.78.115/computer/slave/slave-agent.jnlp
dd9c207e2ef1c0520439451b1775b976e3c9e09712f8ca1fb42f1bc082f14809

tdongsi-mac:~ tdongsi$ docker ps
CONTAINER ID        IMAGE                                                    COMMAND                  CREATED             STATUS              PORTS               NAMES
dd9c207e2ef1        docker.registry.company.net/tdongsi/jenkins-agent:2.80   "java -jar /usr/sh..."   5 seconds ago       Up 4 seconds                            ecstatic_galileo
tdongsi-mac:~ tdongsi$ docker exec -it dd9c207e2ef1 bash
jenkins@dd9c207e2ef1:~$ sudo ls /etc/hosts
[sudo] password for jenkins:
/etc/hosts
jenkins@dd9c207e2ef1:~$ sudo cat /etc/hosts
127.0.0.1   localhost
::1 localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.17.0.2  dd9c207e2ef1
jenkins@dd9c207e2ef1:~$ exit
exit
</code></pre>

<h3><code>docker exec</code> approach</h3>

<p>Later versions of <code>docker</code> adds <code>--user</code> flag that allows us to specify which user that we should enter the container as.
First, we figure out which Kubernetes node is running a particular pod by using the command <code>kubectl describe pod</code>.
After <code>ssh</code>-ing into that Kubernetes node, we can find the corresponding container running in that pod with the command <code>docker ps -a</code>.
The following examples demonstrate entering a <code>jenkins-slave</code> container as <code>root</code> and <code>jenkins</code> user.</p>

<pre><code class="plain Entering container ">[root@dev-worker-2 ~]# docker ps -a
CONTAINER ID        IMAGE                                                                        COMMAND                  CREATED             STATUS              PORTS               NAMES
10f031d08389        docker.registry.company.net/tdongsi/jenkins:jenkins-agent                    "jenkins-slave 9f22f2"   19 minutes ago      Up 19 minutes                           k8s_slave.beb667bf_...
767915746e2c        docker.registry.company.net/tdongsi/pause:2.0                                "/pause"                 19 minutes ago      Up 19 minutes                           k8s_POD.abb8e705_...

[root@dev-worker-2 ~]# docker exec -it --user root 10f031d08389 /bin/sh
#
# ls
support  workspace
# id
uid=0(root) gid=0(root) groups=0(root)
# exit

[root@dev-worker-2 ~]# docker exec -it --user jenkins 10f031d08389 /bin/sh
$ ls
support  workspace
$ id
uid=25001(jenkins) gid=25001(jenkins) groups=25001(jenkins),992(docker)
$ exit
</code></pre>

<p>As mentioned, older versions of <code>docker</code> does not support <code>--user</code> flag and does not allow entering container as root.
In that case, use <code>nsenter</code> method presented in the previous section.</p>

<pre><code class="plain Unsupported operation on Docker 1.6">[root@kube-worker-1 ~]# docker exec -it --user root af9a884eb3f1 /bin/sh
flag provided but not defined: --user
See 'docker exec --help'.
[root@kube-worker-1 ~]# docker version
Client version: 1.6.2.el7
Client API version: 1.18
Go version (client): go1.4.2
Git commit (client): c3ca5bb/1.6.2
OS/Arch (client): linux/amd64
Server version: 1.6.2.el7
Server API version: 1.18
Go version (server): go1.4.2
Git commit (server): c3ca5bb/1.6.2
OS/Arch (server): linux/amd64
</code></pre>

<h3>References</h3>

<ul>
<li><a href="https://github.com/jpetazzo/nsenter">nsenter tool</a></li>
<li><a href="https://docs.docker.com/engine/reference/commandline/exec/">docker exec</a> manual</li>
<li><a href="http://stackoverflow.com/questions/28721699/root-password-inside-a-docker-container">StackOverflow discussion</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubernetes: Pod-to-Node Communication Loss]]></title>
    <link href="http://tdongsi.github.io/blog/2017/01/24/kubernetes-pod-to-node-communication-loss/"/>
    <updated>2017-01-24T15:05:15-08:00</updated>
    <id>http://tdongsi.github.io/blog/2017/01/24/kubernetes-pod-to-node-communication-loss</id>
    <content type="html"><![CDATA[<p>This post goes over what happens if we misconfigure <code>etcd</code> and <code>flannel</code> to use the same network (e.g., &ldquo;10.252.61.0/16&rdquo;) as the infrastructure (e.g., &ldquo;10.252.158.72&rdquo; node).
This newbie mistake is rare but very perplexing and this post shows how to troubleshoot it with <code>busybox</code> container.</p>

<h3>Problem symptoms</h3>

<p>From a pod (e.g., <code>jenkins</code>) on one node (e.g., <code>10.252.158.71</code>), we cannot communicate with another node (e.g., <code>10.252.158.72</code>) even though two nodes can communicate with each other normally.</p>

<pre><code class="plain">mymac:kubernetes tdongsi$ kubectl --kubeconfig kubeconfig exec -it jenkins -- bash -il
jenkins@jenkins:~$ ping 10.252.158.72
PING 10.252.158.72 (10.252.158.72) 56(84) bytes of data.
^C
--- 10.252.158.72 ping statistics ---
16 packets transmitted, 0 received, 100% packet loss, time 14999ms

jenkins@jenkins:~$ exit
</code></pre>

<p>Even more perplexing, the pod-to-pod communication is fine (as described right below), even though the second pod is on the same node (e.g., <code>10.252.158.72</code>) that the first pod cannot communciate to.</p>

<h3>Troubleshooting with <code>busybox</code></h3>

<p>Try to run a test pod <code>busybox</code>.
<code>jenkins</code> pod can ping the <code>busybox</code> pod, but not the node that <code>busybox</code> pod is running on.</p>

<pre><code>mymac:kubernetes tdongsi$ kubectl --kubeconfig kubeconfig run busybox \
--image=docker.registry.company.net/tdongsi/busybox --restart=Never --tty -i --generator=run-pod/v1
Waiting for pod default/busybox to be running, status is Pending, pod ready: false
Waiting for pod default/busybox to be running, status is Running, pod ready: false
Waiting for pod default/busybox to be running, status is Running, pod ready: false

mymac:kubernetes tdongsi$ kubectl --kubeconfig kubeconfig exec -it jenkins -- bash -il
jenkins@jenkins:~$ ping 10.252.61.7
PING 10.252.61.7 (10.252.61.7) 56(84) bytes of data.
64 bytes from 10.252.61.7: icmp_seq=1 ttl=62 time=0.540 ms
64 bytes from 10.252.61.7: icmp_seq=2 ttl=62 time=0.186 ms
64 bytes from 10.252.61.7: icmp_seq=3 ttl=62 time=0.177 ms
64 bytes from 10.252.61.7: icmp_seq=4 ttl=62 time=0.161 ms
64 bytes from 10.252.61.7: icmp_seq=5 ttl=62 time=0.187 ms
^C
--- 10.252.61.7 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 4000ms
rtt min/avg/max/mdev = 0.161/0.250/0.540/0.145 ms

jenkins@jenkins:~$ ping 10.252.158.72
PING 10.252.158.72 (10.252.158.72) 56(84) bytes of data.
^C
--- 10.252.158.72 ping statistics ---
14 packets transmitted, 0 received, 100% packet loss, time 13000ms
</code></pre>

<p>In this case, we would use <code>traceroute</code> from the <code>busybox</code> container to determine when the packets are dropped.
<code>10.252.158.72</code> is IP of the VM. <code>10.252.100.5</code> is the IP of the <code>jenkins</code> pod.</p>

<pre><code>mymac:kubernetes tdongsi$ kubectl --kubeconfig kubeconfig run busybox \
--image=docker.registry.company.net/tdongsi/busybox --restart=Never --tty -i --generator=run-pod/v1

Waiting for pod default/busybox to be running, status is Pending, pod ready: false
Waiting for pod default/busybox to be running, status is Running, pod ready: false
Waiting for pod default/busybox to be running, status is Running, pod ready: false

/ # traceroute 10.252.158.72
traceroute to 10.252.158.72 (10.252.158.72), 30 hops max, 46 byte packets
 1  10.252.61.1 (10.252.61.1)  0.005 ms  0.012 ms  0.001 ms
 2  *  *  *
 3  *  *  *
 4  *  *  *
 5  *  *  *
/ #
/ # traceroute 10.252.100.5
traceroute to 10.252.100.5 (10.252.100.5), 30 hops max, 46 byte packets
 1  10.252.61.1 (10.252.61.1)  0.005 ms  0.004 ms  0.002 ms
 2  *  10.252.100.0 (10.252.100.0)  0.487 ms  0.241 ms
 3  10.252.100.5 (10.252.100.5)  0.141 ms  0.563 ms  0.132 ms
/ # exit
</code></pre>

<p>For the context, <code>10.252.100.5</code> is the IP of the service, as shown in the command below.</p>

<pre><code>mymac:private_cloud tdongsi$ kubectl --kubeconfig kubeconfig describe services
Name:           jenkins
Namespace:      default
Labels:         &lt;none&gt;
Selector:       name=jenkins
Type:           NodePort
IP:         10.252.77.85
Port:           http    80/TCP
NodePort:       http    30080/TCP
Endpoints:      10.252.100.5:8080
Session Affinity:   None
No events.
</code></pre>

<h3>What went wrong?</h3>

<p>It&rsquo;s a newbie mistake when configuring Kubernetes.
When setting up <code>etcd</code> and configuring it to hold <code>flannel</code> configuration, it is important to pick an unused network.
I made a mistake for using <code>10.252.61.0/16</code> for flannel when some of my kubernetes nodes has IPs as &ldquo;10.252.xxx.xxx&rdquo;.
As a result, kube-proxy services intercept the traffic from the container and thinks its a virtual traffic since my node IP happens to be in the same subnet with <code>flanneld</code>.
This leads to pod-to-VM communication loss as described above.
The solution is simply reset flanneld with another subnet after resetting configruation value in <code>etcd</code> to &ldquo;172.17.0.0/16&rdquo;.</p>

<pre><code class="plain Update etcd">[centos@kube-master ~]$ etcdctl update /kube-centos/network/config \
"{ \"Network\": \"172.17.0.0/16\", \"SubnetLen\": 24, \"Backend\": { \"Type\": \"vxlan\" } }"

[centos@kube-master ~]$ etcdctl rm --recursive /kube-centos/network/subnets
[centos@kube-master ~]$ etcdctl ls /kube-centos/network
/kube-centos/network/config
</code></pre>

<p>After this, we can reset and restart <code>flannel</code> services on all nodes to use the new network overlay configuration.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubernetes: Pause Container and Private Docker Registry]]></title>
    <link href="http://tdongsi.github.io/blog/2017/01/16/kubernetes-pulling-from-private-image-repository/"/>
    <updated>2017-01-16T11:48:05-08:00</updated>
    <id>http://tdongsi.github.io/blog/2017/01/16/kubernetes-pulling-from-private-image-repository</id>
    <content type="html"><![CDATA[<p>This post documents dealing with implicit container <code>pause</code> in a corporate context, where Internet access is restricted and private Docker registry must be used.</p>

<h3>Problem description</h3>

<p>In this problem, Kubernetes cluster are all installed and configured.
We are trying to create some &ldquo;Hello World&rdquo; pod, using the example described <a href="https://kubernetes.io/docs/user-guide/walkthrough/#pod-definition">here</a>.</p>

<pre><code>tdongsi-mac:private_cloud tdongsi$ kubectl --kubeconfig kubeconfig get nodes
NAME            LABELS                                 STATUS    AGE
kube-worker-1   kubernetes.io/hostname=kube-worker-1   Ready     1d
kube-worker-3   kubernetes.io/hostname=kube-worker-3   Ready     1d
kube-worker-4   kubernetes.io/hostname=kube-worker-4   Ready     1d
tdongsi-mac:private_cloud tdongsi$ kubectl --kubeconfig kubeconfig create -f pod-nginx.yaml
pod "nginx" created
</code></pre>

<p>However, one can see the following error messages:</p>

<pre><code>tdongsi-mac:private_cloud tdongsi$ kubectl --kubeconfig kubeconfig get pods
NAME      READY     STATUS                                                                                       RESTARTS   AGE
nginx     0/1       Image: artifactrepo1.corp.net/tdongsi/nginx:1.7.9 is not ready on the node                   0          4m

tdongsi-mac:private_cloud tdongsi$ kubectl --kubeconfig kubeconfig get events
FIRSTSEEN   LASTSEEN   COUNT     NAME      KIND      SUBOBJECT                           REASON      SOURCE                    MESSAGE
40m         40m        1         nginx     Pod                                           scheduled   {scheduler }              Successfully assigned nginx to kube-worker-3
40m         40m        3         nginx     Pod       implicitly required container POD   pulling     {kubelet kube-worker-3}   Pulling image "gcr.io/google_containers/pause:0.8.0"
40m         39m        3         nginx     Pod       implicitly required container POD   failed      {kubelet kube-worker-3}   Failed to pull image "gcr.io/google_containers/pause:0.8.0":...
)
</code></pre>

<p>The full error message for the third event above is quoted below:</p>

<p><blockquote><p>Failed to pull image &ldquo;gcr.io/google_containers/pause:0.8.0&rdquo;: image pull failed for gcr.io/google_containers/pause:0.8.0, this may be because there are no credentials on this request.  details: (API error (500):  v1 ping attempt failed with error: Get <a href="https://gcr.io/v1/_ping:">https://gcr.io/v1/_ping:</a> dial tcp 173.194.175.82:443: i/o timeout. If this private registry supports only HTTP or HTTPS with an unknown CA certificate, please add <code>--insecure-registry gcr.io</code> to the daemon&rsquo;s arguments. In the case of HTTPS, if you have access to the registry&rsquo;s CA certificate, no need for the flag; simply place the CA certificate at /etc/docker/certs.d/gcr.io/ca.crt</p></blockquote></p>

<h3>What is <code>pause</code> container?</h3>

<p>Whenever we create a pod, a <code>pause</code> container image such as <em>gcr.io/google_containers/pause:0.8.0</em> is implicitly required.
What is that <code>pause</code> container&rsquo;s purpose?
The <code>pause</code> container essentially holds the network namespace for the pod.
It does nothing useful and its container image (see <a href="https://github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile">its Dockerfile</a>) basically contains a simple binary that goes to sleep and never wakes up (see <a href="https://github.com/kubernetes/kubernetes/blob/master/build/pause/pause.c">its code</a>).
However, when the top container such as <code>nginx</code> container dies and gets restarted by kubernetes, all the network setup will still be there.
Normally, if the last process in a network namespace dies, the namespace will be destroyed.
Restarting <code>nginx</code> container without <code>pause</code> would require creating all new network setup.
With <code>pause</code>, you will always have that one last thing in the namespace.</p>

<h3><code>pause</code> container and private Docker registry</h3>

<p>What trouble does such <code>pause</code> container can give us?
As the full container image path indicates, the <code>pause</code> container image is downloaded from Google Container Registry (&ldquo;gcr.io&rdquo;) by default.
If a kubernetes node is inside a corporate network with restricted access to Internet, one cannot simply pull that Docker image from Google Container Registry or Docker Hub.
And that is what error message quoted above indicates.
However, each corporate may have its own internal Docker registry with vetted Docker images that you can push to and pull from.
One work-around is to push that <code>pause</code> image to the internal Docker registry, downloaded to each Kubernetes slave, and retagged it (from internal tag <code>artifactrepo1.corp.net</code> to <code>gcr.io</code> tag).
Essentially, I pre-loaded each Kubenetes slave with a <code>pause:0.8.0</code> Docker image.</p>

<pre><code>tdongsi-mac:private_cloud tdongsi$ docker pull gcr.io/google_containers/pause:0.8.0
0.8.0: Pulling from google_containers/pause
a3ed95caeb02: Pull complete
bccc832946aa: Pull complete
Digest: sha256:bbeaef1d40778579b7b86543fe03e1ec041428a50d21f7a7b25630e357ec9247
Status: Downloaded newer image for gcr.io/google_containers/pause:0.8.0

tdongsi-mac:private_cloud tdongsi$ docker tag gcr.io/google_containers/pause:0.8.0 artifactrepo1.corp.net/tdongsi/pause:0.8.0

tdongsi-mac:private_cloud tdongsi$ docker push artifactrepo1.corp.net/tdongsi/pause:0.8.0
The push refers to a repository [artifactrepo1.corp.net/tdongsi/pause]
5f70bf18a086: Mounted from tdongsi/nginx
152b0ca1d7a4: Pushed
0.8.0: digest: sha256:a252a0fc9c760e531dbc9d41730e398fc690938ccb10739ef2eda61565762ae5 size: 2505
</code></pre>

<p>The more scalable way, such as for Puppet automation, is to use <code>kubelet</code> option &ldquo;&ndash;pod-infra-container-image&rdquo;.
In the config file &ldquo;/etc/kubernetes/kubelet&rdquo; of <code>kubelet</code> service, modify the following lines:</p>

<pre><code class="plain Custom kubelet option"># Add your own! 
KUBELET_ARGS="--pod-infra-container-image=artifactrepo1.corp.net/tdongsi/pause:0.8.0"
</code></pre>

<p>Note that if the private Docker registry &ldquo;artifactrepo1.corp.net&rdquo; requires authentication, specifying the container image in the above <code>kubelet</code> option might NOT work.
In some older versions of Docker/Kubernetes, image pull secrets, even though created for authenticating to such Docker registry, are not properly used to load <code>pause</code> container image.
Therefore, loading <code>pause</code> container image happens first and fails to authenticate with such private Docker registry, before the actual required container image can be loaded.</p>

<p>In that case, the alternative way for scalable automation is to prepare a binary <code>tar</code> file for <code>pause</code> container image (with <code>docker save</code>) and pre-load the image on each kubernetes node with <code>docker load</code> command.
We can upload the binary <code>tar</code> file onto new kubernetes nodes whenever each of those is created and added to the kubernetes cluster.</p>

<pre><code class="plain docker load">docker load -i /path/to/pause-amd64.tar
</code></pre>

<!--
### Pulling fails even with pull image secret

**WARNING**: 
This section is for older versions of Kubernetes (< 1.2) with internal corporate constraints. 
Using such old Kubernetes version is not recommended to begin with because of various stability and performance issues.
However, some companies may dive into Kubernetes early, contribute lots of code to make it work and the problem described below may persist, especially for new hires.

Validate

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>tdongsi-mac:private_cloud tdongsi$ kubectl --kubeconfig kubeconfig get secret corpregistry -o yaml | grep dockerconfigjson: | cut -f 2 -d : | base64 -D
</span><span class='line'>{ "artifactrepo1.corp.net": { "auth": "XXXXX", "email": "tdongsi@salesforce.com" } }</span></code></pre></td></tr></table></div></figure>
-->


<h3>References</h3>

<ul>
<li><a href="https://github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile">pause Dockerfile</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/master/build/pause/pause.c">pause source code</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker Image for ETL Development in Vertica]]></title>
    <link href="http://tdongsi.github.io/blog/2016/09/01/docker-image-for-vertica/"/>
    <updated>2016-09-01T11:38:27-07:00</updated>
    <id>http://tdongsi.github.io/blog/2016/09/01/docker-image-for-vertica</id>
    <content type="html"><![CDATA[<p>Docker is Awesome!!!</p>

<p>I wish I knew Docker earlier, before going through the hassle of creating VMs (<a href="/blog/2016/01/10/find-and-replace-a-string-in-multiple-files/">one-node</a>
 or <a href="/blog/2016/03/12/set-up-three-node-vertica-sandbox-vms-on-mac/">three-node</a> cluster) for local ETL development and testing.
Docker can make the whole setup even easier.
It can be done in just a few commands, using <a href="https://github.com/tdongsi/vertica/tree/master/docker">a Vertica Dockerfile</a>, created based on <a href="https://github.com/wmarinho/docker-hp-vertica">this</a>.
In addition to easy virtualization, Docker also enables the entire setup can be automated in a script, allowing it to be version-controlled (i.e., <a href="https://en.wikipedia.org/wiki/Infrastructure_as_Code">Infrastructure as Code</a>).</p>

<p>Some notes about this Dockerfile, compared to <code>wmarinho</code>&rsquo;s:</p>

<ul>
<li>Added new schema, new user and new role as examples. Avoid using <code>dbadmin</code> user for development purpose.</li>
<li>Added Java and Maven for Java-based ETL and automated test execution.</li>
<li>Demonstrated running Bash and SQL scripts to initialize the container/database.</li>
</ul>


<h3>How to run</h3>

<p>Before running <code>docker build</code>, download Vertica Community Edition from <a href="https://my.vertica.com/">https://my.vertica.com/</a> and place in the same folder as the <code>Dockerfile</code>.
This <code>Dockerfile</code> takes &ldquo;vertica-7.2.3-0.x86_64.RHEL6.rpm&rdquo; as the install file.</p>

<pre><code class="plain Windows output">epigineer@epigineerpc MINGW64 /c/Work/Github/vertica/docker (develop)
$ docker build -t vertica .
...

epigineer@epigineerpc MINGW64 /c/Work/Github/vertica/docker (develop)
$ docker images
REPOSITORY          TAG                 IMAGE ID            CREATED
SIZE
vertica             latest              d2607fa1f457        13 seconds ago
1.638 GB
&lt;none&gt;              &lt;none&gt;              486163abe73f        11 minutes ago
1.638 GB
centos              centos6.6           2c886f766286        8 weeks ago
202.6 MB

epigineer@epigineerpc MINGW64 /c/Work/Github/vertica/docker (develop)
$ docker run -p 5433:5433 --hostname=verthost --privileged=true --memory 4G -t
-i d2607fa1f457 /bin/bash
Info: no password specified, using none
        Starting nodes:
                v_docker_node0001 (127.0.0.1)
        Starting Vertica on all nodes. Please wait, databases with large catalog
 may take a while to initialize.
        Node Status: v_docker_node0001: (DOWN)
        Node Status: v_docker_node0001: (DOWN)
        Node Status: v_docker_node0001: (DOWN)
        Node Status: v_docker_node0001: (DOWN)
        Node Status: v_docker_node0001: (UP)
Database docker started successfully
creating schema
CREATE SCHEMA
creating user
CREATE USER
creating role
CREATE ROLE
grant usage, create on schema
GRANT PRIVILEGE
</code></pre>

<h3>Troubleshooting Notes</h3>

<p>In Mac OSX, remember that the <code>entrypoint.sh</code> file should have executable permission.
Otherwise, you might get the error &ldquo;oci runtime error: exec: &rdquo;/entrypoint.sh": permission denied".
After changing the file permission, you have to rebuild the image with <code>docker build</code> before <code>docker run</code> again.</p>

<h4>&ldquo;Insufficient resources&rdquo; error when running ETL</h4>

<p>You might get &ldquo;Insufficient resources to execute plan on pool general &hellip; Memory exceeded&rdquo; error when running a large ETL script against the Vertica container.
For complex ETL, Vertica might need additional memory to execute the query plan.
Simply setting higher memory allocation using <code>--memory</code> option of <code>docker run</code> might NOT work if using <strong>Docker Toolbox</strong>.
To set higher memory allowance, stop the <code>docker-machine</code> and set memory as follows:</p>

<pre><code class="plain">tdongsi$ docker-machine stop
Stopping "default"...
Machine "default" was stopped.

tdongsi$ VBoxManage modifyvm default --memory 8192

tdongsi$ docker-machine start
Starting "default"...
(default) Check network to re-create if needed...
(default) Waiting for an IP...
Machine "default" was started.
Waiting for SSH to be available...
Detecting the provisioner...
Started machines may have new IP addresses. You may need to re-run the `docker-machine env` command.
</code></pre>

<p>Note that after running the above commands, <code>docker-machine inspect</code> still shows <code>"Memory":"2048"</code>.
To verify if memory is properly allocated as desired, run <code>free</code> command, for example, inside the container to verify.</p>

<h3>Links</h3>

<ul>
<li><a href="https://github.com/tdongsi/vertica/tree/master/docker">My Dockerfile for ETL development and testing on Vertica</a></li>
<li><a href="https://github.com/wmarinho/docker-hp-vertica">Original Dockerfile</a></li>
<li><a href="https://www.docker.com/">Docker</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
