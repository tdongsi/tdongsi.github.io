<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Cloudera | Personal Programming Notes]]></title>
  <link href="http://tdongsi.github.io/blog/categories/cloudera/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/"/>
  <updated>2015-12-02T20:42:21-08:00</updated>
  <id>http://tdongsi.github.io/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Programming Hive (Pt. 2): Hive CLI]]></title>
    <link href="http://tdongsi.github.io/blog/2015/11/23/programming-hive-hive-cli/"/>
    <updated>2015-11-23T19:47:23-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/11/23/programming-hive-hive-cli</id>
    <content type="html"><![CDATA[<p><img class="center" src="/images/hive/cat.gif" title="Cover" ></p>

<p>Chapter 2 of the book covers how to get started with Hive and some basics of Hive, including its command-line interface (CLI).</p>

<h3>Starting Hive with Cloudera Quickstart VM</h3>

<p>On Cloudera Quickstart VM, the cores of its Hive distribution, including files such as <code>hive-exec*.jar</code> and <code>hive-metastore*.jar</code>, can be found in <code>/usr/lib/hive/lib</code>. The Hive executables can be found in <code>/usr/lib/hive/bin</code>. Running <code>hive</code> without any parameter will start Hive&rsquo;s CLI.</p>

<pre><code>[cloudera@quickstart temp]$ hive

Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
hive&gt; CREATE TABLE x (a INT);
OK
Time taken: 3.032 seconds
hive&gt; SELECT * FROM x;
OK
Time taken: 0.465 seconds
hive&gt; SELECT *        
    &gt; FROM x;
OK
Time taken: 0.049 seconds
hive&gt; DROP TABLE x;
OK
Time taken: 0.348 seconds
hive&gt; exit;
</code></pre>

<h3>Hive services</h3>

<p>The <code>hive</code> shell command is actually a wrapper to multiple Hive services, including the CLI.</p>

<pre><code>[cloudera@quickstart temp]$ hive --help
Usage ./hive &lt;parameters&gt; --service serviceName &lt;service parameters&gt;
Service List: beeline cli help hiveserver2 hiveserver hwi jar lineage metastore metatool orcfiledump rcfilecat schemaTool version 
Parameters parsed:
  --auxpath : Auxillary jars 
  --config : Hive configuration directory
  --service : Starts specific service/component. cli is default
Parameters used:
  HADOOP_HOME or HADOOP_PREFIX : Hadoop install directory
  HIVE_OPT : Hive options
For help on a particular service:
  ./hive --service serviceName --help
Debug help:  ./hive --debug --help
</code></pre>

<p>Note the list of services following the line &ldquo;Service List&rdquo;. There are several services available, most notably <strong>cli, hwi, jar, metastore</strong>. You can use <code>--service name</code> option to invoke a service. CLI is the default service, not specifying any service in <code>hive</code> command will run CLI service, as shown above.</p>

<p>For example, to run <a href="https://cwiki.apache.org/confluence/display/Hive/HiveWebInterface">Hive Web Interface</a>, run the service <strong>hwi</strong>. On Cloudera Quickstart VM, you might encounter this error:</p>

<pre><code>[cloudera@quickstart temp]$ hive --service hwi
ls: cannot access /usr/lib/hive/lib/hive-hwi-*.war: No such file or directory
15/11/23 20:22:50 INFO hwi.HWIServer: HWI is starting up
15/11/23 20:22:50 FATAL hwi.HWIServer: HWI WAR file not found at /usr/lib/hive/usr/lib/hive/lib/hive-hwi-0.8.1-cdh4.0.0.jar
</code></pre>

<p>To fix that error, edit the config file <code>hive-site.xml</code> in the <code>config</code> folder (e.g., <code>/usr/lib/hive/conf/hive-site.xml</code> on Cloudera VM) to point to the right location of HWI&rsquo;s war file. On Cloudera Quickstart VM, the WAR file property block should look like this:</p>

<pre><code>...
 &lt;property&gt;
    &lt;name&gt;hive.hwi.war.file&lt;/name&gt;
    &lt;value&gt;/lib/hive-hwi.jar&lt;/value&gt;
    &lt;description&gt;This is the WAR file with the jsp content for Hive Web Interface&lt;/description&gt;
  &lt;/property&gt;
...
</code></pre>

<p>Running the <strong>hwi</strong> service again using <code>hive</code> command should work. In order to access the Hive Web Interface, go to <code>[Hive Server Address]</code>:9999/hwi on your web browser.</p>

<pre><code>[cloudera@quickstart temp]$ hive --service hwi
ls: cannot access /usr/lib/hive/lib/hive-hwi-*.war: No such file or directory
15/11/23 20:31:27 INFO hwi.HWIServer: HWI is starting up
15/11/23 20:31:27 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
15/11/23 20:31:27 INFO mortbay.log: jetty-6.1.26.cloudera.4
15/11/23 20:31:27 INFO mortbay.log: Extract /usr/lib/hive/lib/hive-hwi.jar to /tmp/Jetty_0_0_0_0_9999_hive.hwi.0.13.1.cdh5.3.0.jar__hwi__.lcik1p/webapp
15/11/23 20:31:28 INFO mortbay.log: Started SocketConnector@0.0.0.0:9999
</code></pre>

<h3>Hive CLI</h3>

<p>Available options for Hive CLI can be displayed as follows:</p>

<pre><code>[cloudera@quickstart temp]$ hive --help --service cli
usage: hive
 -d,--define &lt;key=value&gt;          Variable subsitution to apply to hive
                                  commands. e.g. -d A=B or --define A=B
    --database &lt;databasename&gt;     Specify the database to use
 -e &lt;quoted-query-string&gt;         SQL from command line
 -f &lt;filename&gt;                    SQL from files
 -H,--help                        Print help information
 -h &lt;hostname&gt;                    connecting to Hive Server on remote host
    --hiveconf &lt;property=value&gt;   Use value for given property
    --hivevar &lt;key=value&gt;         Variable subsitution to apply to hive
                                  commands. e.g. --hivevar A=B
 -i &lt;filename&gt;                    Initialization SQL file
 -p &lt;port&gt;                        connecting to Hive Server on port number
 -S,--silent                      Silent mode in interactive shell
 -v,--verbose                     Verbose mode (echo executed SQL to the
                                  console)
</code></pre>

<h4>Hive variables and properties</h4>

<p>The <code>--define key=value</code> option is equivalent to the <code>--hivevar key=value</code> option. Both let you define custom variables in the <code>hivevar</code> namespace, separate from three other built-in namespaces, <code>hiveconf</code>, <code>system</code>, and <code>env</code>. By convention, the Hive namespaces for variables and properties are as follows:</p>

<ol>
<li>hivevar: user-defined custom variables.</li>
<li>hiveconf: Hive-specific configuration properties.</li>
<li>system: Java configuration properties.</li>
<li>env: (Read-only) environment variables by shell environment (e.g., bash).</li>
</ol>


<p>Inside Hive CLI, the command <code>SET</code> is used to display and change variables. For example:</p>

<pre><code>[cloudera@quickstart temp]$ hive

Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
hive&gt; set env:HOME; &lt;-- display HOME variable in env namespace
env:HOME=/home/cloudera
hive&gt; set; &lt;-- display all variables
...
hive&gt; set -v; &lt;-- display even more variables
...
hive&gt; set hivevar:foo=bar; &lt;-- set foo variable in hivevar namespace to bar
</code></pre>

<h4><code>-e query_string</code> and <code>-S</code> options</h4>

<p><code>-e</code> option allows you to execute a list of semicolon-separated queries as an input string. <code>-S</code> option for silent mode will remove non-essential output. For example:</p>

<pre><code>$ hive -e "SELECT * FROM mytable LIMIT 3";
OK
name1 10
name2 20
name3 30
Time taken: 4.955 seconds
</code></pre>

<pre><code>$ hive -S -e "select * FROM mytable LIMIT 3"
name1 10
name2 20
name3 30
</code></pre>

<p><strong>Tip</strong>: To quickly search for the full name of a property that you only remember part of its name, pipe the Hive&rsquo;s <code>SET</code> command output to grep. For example:</p>

<pre><code>[cloudera@quickstart temp]$ hive -S -e "set" | grep warehouse
hive.metastore.warehouse.dir=/user/hive/warehouse
hive.warehouse.subdir.inherit.perms=true
</code></pre>

<h4><code>-f script_file</code> option</h4>

<p>This option allows you to execute one or more queries contained in a script file. If you are already within the Hive CLI, you can use the <code>SOURCE</code> command to execute a script file. For example:</p>

<pre><code>$ cat /path/to/file/withqueries.hql
SELECT x.* FROM src x;
$ hive
hive&gt; source /path/to/file/withqueries.hql;
</code></pre>

<h4><code>-i filename</code> option</h4>

<p>This option lets you specify an initialization file with a list of commands for the CLI to run when it starts. The default initialization file is the file <code>$HOME/.hiverc</code> if it exists.</p>

<h4>Tips</h4>

<ul>
<li>To print column headers (disabled by default), set the hiveconf property <code>hive.cli.print.header</code> to true: <code>set hive.cli.print.header=true;</code>.</li>
<li>Hive does have command history, saved into a file <code>$HOME/.hivehistory</code>. Use the up and down arrow keys to scroll through previous commands.</li>
<li>To run HDFS commands from within Hive CLI, drop the hdfs. For example:</li>
</ul>


<pre><code>hive&gt; dfs -ls input; 
Found 1 items
-rw-r--r--   1 cloudera cloudera         31 2015-01-15 18:04 input/wordcount.txt
</code></pre>

<ul>
<li>To run the bash shell commands from within Hive CLI, prefix <code>!</code> before the bash commands and terminate the line with a semicolon (;). Note that interactive commands, shell pipes <code>|</code>, and file globs <code>*</code> will not work. Example:</li>
</ul>


<pre><code>hive&gt; !pwd;
hive&gt; /home/cloudera/temp
</code></pre>

<ul>
<li>Set the property <code>set hive.exec.mode.local.auto=true;</code> to use local mode more aggressively and gain performance in Hive queries, especially when working with small data sets.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[WordCount Example in Cloudera Quickstart VM]]></title>
    <link href="http://tdongsi.github.io/blog/2015/11/20/wordcount-sample-in-cloudera-quickstart-vm/"/>
    <updated>2015-11-20T11:47:51-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/11/20/wordcount-sample-in-cloudera-quickstart-vm</id>
    <content type="html"><![CDATA[<p><a href="https://wiki.apache.org/hadoop/WordCount">WordCount</a> is the Hadoop equivalent of “Hello World” example program. When you first start learning a new language or framework, you would want to run and look into some &ldquo;Hello World&rdquo; example to get a feel of the new development environment. Your first few programs in those new languages or frameworks are probably extended from those basic &ldquo;Hello World&rdquo; examples.</p>

<p>Most Hadoop tutorials are quite overwhelming in text, but provide little guide on practical hands-on experiments (such as <a href="https://developer.yahoo.com/hadoop/tutorial/">this</a>). Although they are good and thorough tutorials, many new Hadoop users may be lost midway after walls of texts.</p>

<p>The purpose of this post is to help new users dive into Hadoop more easily. After reading this, you should be able to:</p>

<ol>
<li>Get started with a simple, local Hadoop sandbox for hands-on experiments.</li>
<li>Perform some simple tasks in HDFS.</li>
<li>Run the most basic example program WordCount, using your own input data.</li>
</ol>


<h3>Get your Hadoop sandbox</h3>

<p>Nowadays, many companies provide Hadoop sandboxes for learning purpose, such as Cloudera, <a href="http://hortonworks.com/products/hortonworks-sandbox/">Hortonworks</a>. In this post, I used <a href="http://www.cloudera.com/content/www/en-us/documentation/enterprise/5-2-x/topics/cloudera_quickstart_vm.html">Cloudera Quickstart VM</a>. Download the VM and start it up in VirtualBox or VMWare Fusion.</p>

<h3>Working with HDFS</h3>

<p>Before running WordCount example, we need to create some input text file, then move it to HDFS. First, create an input test file in your local file system.</p>

<pre><code class="">[cloudera@quickstart temp]$ echo “This is a hadoop tutorial test" &gt; wordcount.txt
</code></pre>

<p>Next, we need to move this file into HDFS. The following commands are the most basic HDFS commands to manage files in HDFS. In order of appearance below, we create a folder, copy the input file from local filesystem to HDFS, and list the content on HDFS.</p>

<pre><code class="">[cloudera@quickstart temp]$ hdfs dfs -mkdir /user/cloudera/input
[cloudera@quickstart temp]$ hdfs dfs -put /home/cloudera/temp/wordcount.txt /user/cloudera/input
[cloudera@quickstart temp]$ hdfs dfs -ls /user/cloudera/input
Found 1 items
-rw-r--r--   1 cloudera cloudera         31 2015-01-15 18:04 /user/cloudera/input/wordcount.txt
</code></pre>

<p>It should be noted that for a fresh Cloudera VM, there is a &ldquo;/user&rdquo; folder in HDFS but not in the local filesystem. This example illustrates that local file system and HDFS are separate, and the Linux&rsquo;s &ldquo;ls&rdquo; and HDFS&rsquo;s &ldquo;ls&rdquo; interact with those independently.</p>

<pre><code class="">[cloudera@quickstart temp]$ ls /user

ls: cannot access /user: No such file or directory
[cloudera@quickstart temp]$ hdfs dfs -ls /user
Found 5 items
drwxr-xr-x   - cloudera cloudera          0 2014-12-18 07:08 /user/cloudera
drwxr-xr-x   - mapred   hadoop            0 2014-12-18 07:08 /user/history
drwxrwxrwx   - hive     hive              0 2014-12-18 07:08 /user/hive
drwxrwxrwx   - oozie    oozie             0 2014-12-18 07:09 /user/oozie
drwxr-xr-x   - spark    spark             0 2014-12-18 07:09 /user/spark
</code></pre>

<p>To see the content of a file on HDFS, use cat subcommand:</p>

<pre><code>[cloudera@quickstart temp]$ hdfs dfs -cat /user/cloudera/input/wordcount.txt
this is a hadoop tutorial test
</code></pre>

<p>For large files, if you want to view just the first or last parts, there is no -more or -tail subcommand. Instead, pipe the output of the -cat subcommand through your local shell’s more, or tail. For example: <code>hdfs dfs -cat wc-out/* | more</code>.</p>

<p>For more HDFS commands, check out links in References section below.</p>

<h3>Running the WordCount example</h3>

<p>Next, we want to run some MapReduce example, such as WordCount. The WordCount example is commonly used to illustrate how MapReduce works. The example returns a list of all the words that appear in a text file and the count of how many times each word appears. The output should show each word found and its count, line by line.</p>

<p>We need to locate the example programs on the sandbox VM. On Cloudera Quickstart VM, they are packaged in this jar file &ldquo;hadoop-mapreduce-examples.jar&rdquo;. Running that jar file without any argument will give you a list of available examples.</p>

<pre><code class="">[cloudera@quickstart temp]$ ls -ltr /usr/lib/hadoop-mapreduce/
lrwxrwxrwx 1 root root      44 Dec 18 07:01 hadoop-mapreduce-examples.jar -&gt; hadoop-mapreduce-examples-2.5.0-cdh5.3.0.jar

[cloudera@quickstart temp]$ hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar
Valid program names are:
  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.
  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.
  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.
  dbcount: An example job that count the pageview counts from a database.
  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.
  grep: A map/reduce program that counts the matches of a regex in the input.
  join: A job that effects a join over sorted, equally partitioned datasets
  multifilewc: A job that counts words from several files.
  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.
  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.
  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.
  randomwriter: A map/reduce program that writes 10GB of random data per node.
  secondarysort: An example defining a secondary sort to the reduce.
  sort: A map/reduce program that sorts the data written by the random writer.
  sudoku: A sudoku solver.
  teragen: Generate data for the terasort
  terasort: Run the terasort
  teravalidate: Checking results of terasort
  wordcount: A map/reduce program that counts the words in the input files.
  wordmean: A map/reduce program that counts the average length of the words in the input files.
  wordmedian: A map/reduce program that counts the median length of the words in the input files.
  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.
</code></pre>

<p>To run the WordCount example using the input file that we just moved to HDFS, use the following command:</p>

<pre><code class="">[cloudera@quickstart temp]$ hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount 
/user/cloudera/input/wordcount.txt /user/cloudera/output

15/11/15 18:14:45 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
15/11/15 18:14:46 INFO input.FileInputFormat: Total input paths to process : 1
15/11/15 18:14:46 INFO mapreduce.JobSubmitter: number of splits:1
15/11/15 18:14:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1421372394109_0001
15/11/15 18:14:46 INFO impl.YarnClientImpl: Submitted application application_1421372394109_0001
15/11/15 18:14:46 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1421372394109_0001/
15/11/15 18:14:46 INFO mapreduce.Job: Running job: job_1421372394109_0001
15/11/15 18:14:55 INFO mapreduce.Job: Job job_1421372394109_0001 running in uber mode : false
15/11/15 18:14:55 INFO mapreduce.Job:  map 0% reduce 0%
15/11/15 18:15:01 INFO mapreduce.Job:  map 100% reduce 0%
15/11/15 18:15:07 INFO mapreduce.Job:  map 100% reduce 100%
15/11/15 18:15:08 INFO mapreduce.Job: Job job_1421372394109_0001 completed successfully
</code></pre>

<p>The output folder is specified as &ldquo;/user/cloudera/output&rdquo; in the above command. Finally, check the output of WordCount example in the output folder.</p>

<pre><code class="">[cloudera@quickstart temp]$ hdfs dfs -ls /user/cloudera/output

Found 2 items
-rw-r--r--   1 cloudera cloudera          0 2015-11-15 18:15 /user/cloudera/output/_SUCCESS
-rw-r--r--   1 cloudera cloudera         43 2015-11-15 18:15 /user/cloudera/output/part-r-00000
[cloudera@quickstart temp]$ hdfs dfs -cat /user/cloudera/output/part-r-00000
a     1
hadoop     1
is     1
test     1
this     1
tutorial     1
</code></pre>

<p>Congratulations!! You just finished the first step of the journey into Hadoop.</p>

<h3>Additional links</h3>

<ol>
<li><a href="http://hortonworks.com/hadoop-tutorial/using-commandline-manage-files-hdfs/">http://hortonworks.com/hadoop-tutorial/using-commandline-manage-files-hdfs/</a></li>
<li><a href="http://wiki.apache.org/hadoop/WordCount">http://wiki.apache.org/hadoop/WordCount</a></li>
<li><a href="https://developer.yahoo.com/hadoop/tutorial/">https://developer.yahoo.com/hadoop/tutorial/</a></li>
</ol>

]]></content>
  </entry>
  
</feed>
