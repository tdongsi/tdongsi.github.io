<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Vertica | Personal Programming Notes]]></title>
  <link href="http://tdongsi.github.io/blog/categories/vertica/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/"/>
  <updated>2016-02-14T00:48:29-08:00</updated>
  <id>http://tdongsi.github.io/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Vertica: Performance Optimization Notes]]></title>
    <link href="http://tdongsi.github.io/blog/2016/02/13/vertica-post-6/"/>
    <updated>2016-02-13T23:52:44-08:00</updated>
    <id>http://tdongsi.github.io/blog/2016/02/13/vertica-post-6</id>
    <content type="html"><![CDATA[<p>Most of these optimization notes in this post are learnt through interaction with <a href="http://www.nexius.com/software-and-business-intelligence/">Nexius</a> consultants.
Also see <a href="/blog/2015/12/16/vertica-tip-best-practices/">Veritca Best Practices</a>.</p>

<h3><code>NOT IN</code> is better than <code>NOT EXISTS</code></h3>

<p>When we want to insert a row into a dimension table AND check for duplicates at the same time, we usually do this in DML scripts:</p>

<pre><code class="sql BAD">SELECT 'United States', 'English' 
WHERE NOT EXISTS (SELECT 'x' FROM dim_country WHERE country_name = 'United States')
</code></pre>

<p>However, for all such inserts, we were recently informed that it is better <strong>in Vertica</strong> to do <code>NOT IN</code> instead of <code>NOT EXISTS</code>.
So, for example above:</p>

<pre><code class="sql GOOD">SELECT 'United States', 'English' 
WHERE 'United States' NOT IN (select country_name from dim_country)
</code></pre>

<h3>Avoid using <code>LEFT JOIN</code> to check existence</h3>

<p>Let&rsquo;s say you have an ETL that regularly inserts new data into an existing dimension table.</p>

<pre><code class="sql BAD">INSERT INTO dim_country                    
(
    country_id,
    country_name,
    country_language,
) 
SELECT ssp.country_id,
    ssp.country_name,
    ssp.country_language,
FROM staging_table ssp
LEFT JOIN dim_country dc on dc.country_id=ssp.country_id
WHERE dc.country_id is NULL;
</code></pre>

<p>We are sometimes doing <code>LEFT JOIN</code> like this only to determine whether or not an entry already exists in the table.
It would be faster to instead use a <code>WHERE</code> clause to check if an entry exists.
Although it might sound counter-intuitive, but reducing <code>JOIN</code> operations like this has been regularly recommended.</p>

<pre><code class="sql GOOD">INSERT INTO dim_country                    
(
    country_id,
    country_name,
    country_language,
) 
SELECT ssp.country_id,
    ssp.country_name,
    ssp.country_language,
FROM staging_table ssp
WHERE ssp.country_id NOT IN (SELECT country_id FROM dim_country);
</code></pre>

<h3>Avoid function calls in <code>WHERE</code> and <code>JOIN</code> clauses</h3>

<p>For this performance tip, we make a slight change the ETL example in the last section above where <code>country_id</code> column is removed. In this case, we can use a normalized <code>country_name</code> as the ID to check for existing entries in the table:</p>

<pre><code class="sql BAD">INSERT INTO dim_country                    
(
    country_name,
    country_language,
) SELECT ssp.country_name,
    ssp.country_language,
FROM staging_table ssp
LEFT JOIN dim_country dc on lower(dc.country_name)=lower(ssp.country_name)
WHERE dc.country_name is NULL;
</code></pre>

<p>In this example, we normalize <code>country_name</code> to lower case. Note that <code>WHERE</code> clause should be used instead of <code>LEFT JOIN</code> as discussed above.</p>

<pre><code class="sql BETTER, but still BAD">INSERT INTO dim_country                    
(
    country_name,
    country_language,
) SELECT ssp.country_name,
    ssp.country_language,
FROM staging_table ssp
WHERE lower(ssp.country_name) NOT IN (SELECT lower(country_name) FROM dim_country);;
</code></pre>

<p>However, such change still has bad performance because, in general, function calls in <code>WHERE</code> and <code>JOIN</code> clauses should be avoided in Vertica.
In both examples above, calling functions like <code>LOWER</code> in <code>WHERE</code> and <code>JOIN</code> clauses will affect the performance of the ETLs.</p>

<p>The solution for the above example is that since we control what goes into dimension tables, we can ensure that columns like <code>country_name</code> are always stored in lower-case.
Then, we can do the same when creating the temporary table such as <code>staging_table</code> that we are comparing to check for existence.</p>

<h3>Use  <a href="https://my.vertica.com/docs/7.1.x/HTML/Content/Authoring/SQLReferenceManual/Functions/VerticaFunctions/ANALYZE_STATISTICS.htm">ANALYZE_STATISTICS</a></h3>

<p>Make sure to run <code>ANALYZE_STATISTICS</code> after all data loads.
Using this function, tables are analyzed for best performance in subsequent queries ran against it.
Without information from <code>ANALYZE_STATISTICS</code>, the query optimizer assumes uniform distribution of data values and equal storage usage for all projections.</p>

<p>Note that <code>ANALYZE_STATISTICS</code> is only supported on <em>local</em> temporary tables, but not on <em>global</em> temporary tables.
In addition, when you add ANALYZE_STATISTICS function calls into your ETL scripts, errors might be thrown when a second <code>ANALYZE_STATISTICS</code> call is made while the first is still running.
Those errors can be ignored but they must be caught accordingly to separate with other Vertica error messages.</p>

<h3>Avoid creating temporary tables using <code>SELECT</code></h3>

<p>Instead of creating temporary tables using <code>SELECT</code>, it is recommended:</p>

<ol>
<li>Create the temporary table first without a projection.</li>
<li>Create a super projection with the correct column encodings and <code>ORDER BY</code> clause</li>
<li>Populate it using <code>INSERT /*+ direct */ INTO</code>. Note the <code>/*+ direct */</code> hint to write data directly to disk, bypassing memory.</li>
</ol>


<p>For example, in a Vertica ETL script that runs daily, we usually create a temporary table to retrieve the latest records from the source table like this:</p>

<pre><code class="sql BAD">CREATE TEMPORARY TABLE customer_last_temp 
ON COMMIT PRESERVE ROWS
AS(
  select * from (
    select *,
    row_number() OVER (PARTITION BY customer_id ORDER BY last_modify_date DESC) AS rank 
    from  stg_customer rpt 
  ) t1 where t1.rank =1
);
</code></pre>

<p>In this example, <code>last_modify_date</code> is the <a href="https://en.wikipedia.org/wiki/Change_data_capture">CDC</a> column and <code>customer_id</code> is the primary key column.
Although this SQL statement is simple and easy to understand, it is really slow for a large and growing <code>stg_customer</code> table that contains updates to all customers on multiple dates, with millions of <em>new</em> customer entries each day.
Instead, the recommended coding pattern is to create a temporary table first without a projection:</p>

<pre><code class="sql Create a temporary table without projection">CREATE LOCAL TEMPORARY TABLE customer_last_temp  ( 
        customer_id                     int,
        subscribe_date                  timestamp,
        cancel_date                     timestamp,
        last_modify_date                timestamp,
)
ON COMMIT PRESERVE ROWS NO PROJECTION;
</code></pre>

<p>It is also recommended that the column names are explicitly specified, so that only required columns are created in the temporary table.
A <code>LOCAL</code> temporary table is created, instead of <code>GLOBAL</code>, so that we can use <code>ANALYZE_STATISTICS</code> functions as discussed above.
Next, create a super projection with the correct column encodings and <code>ORDER BY</code> clause:</p>

<pre><code class="sql Create a super projection">CREATE PROJECTION customer_last_temp_super (
      customer_id ENCODING DELTARANGE_COMP 
    , subscribe_date ENCODING GCDDELTA
    , cancel_date ENCODING BLOCKDICT_COMP     
    , last_modify_date ENCODING BLOCKDICT_COMP 
)
AS 
SELECT customer_id 
     , subscribe_date
     , cancel_date
     , last_modify_date
  FROM customer_last_temp 
 ORDER BY customer_id
SEGMENTED BY HASH (customer_id) ALL NODES;
</code></pre>

<p>Finally, insert &ldquo;directly&rdquo; into the temporary table:</p>

<pre><code class="sql Populate the table">INSERT /*+ direct */ INTO customer_last_temp (
      customer_id 
    , subscribe_date 
    , cancel_date 
    , last_modify_date 
)
WITH t1 AS (
    SELECT company_id 
         , subscribe_date 
         , cancel_date 
         , last_modify_date 
         , ROW_NUMBER() OVER (PARTITION BY customer_id 
                                  ORDER BY last_modify_date DESC) AS rank 
      FROM stg_customer AS rpt 
)
SELECT company_id 
     , subscribe_date 
     , cancel_date 
     , last_modify_date 
FROM t1
WHERE t1.rank = 1;  
</code></pre>

<p>The <code>WITH</code> clause is just a more readable way to write the sub-query in the original SQL statement.
In addition, the wildcard <code>*</code> in the original SQL query is also avoided, in case the table <code>stg_customer</code> is a very wide table.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Virtual Machine for ETL Testing]]></title>
    <link href="http://tdongsi.github.io/blog/2016/01/10/find-and-replace-a-string-in-multiple-files/"/>
    <updated>2016-01-10T23:49:15-08:00</updated>
    <id>http://tdongsi.github.io/blog/2016/01/10/find-and-replace-a-string-in-multiple-files</id>
    <content type="html"><![CDATA[<h3>Vertica Virtual Machine as sandbox test environment</h3>

<p>When developing data-warehouse solutions in Vertica, you want to set up some test environment.
Ideally, you should have separate schema for each developer.
However, it is usually NOT possible in my experience: developers and test engineers have to share very few schemas in development environment.
The explanation that I usually get is that having a schema for each developer will not scale in database maintenance and administration, and there are likely some limits in Vertica&rsquo;s commercial license.
If that is the case, I recommend that we look into using Vertica Community Edition on <strong>Virtual Machines (VMs)</strong> for sandbox test environment, as a cheap alternative.</p>

<p>Are VMs really necessary in data-warehouse testing? When testing Extract-Transform-Load (ETL) processes, I find that many of test cases require regular set-up and tear-down, adding mock records to represent corner cases, and/or running ETLs multiple times to simulate daily runs of those processes.
Regular tear-down requires dropping multiple tables regularly, which requires much greater care and drains much mental energy when working with others' data and tables.
Similarly, adding mock records into some commonly shared tables might affect others when they assume the data is production-like.
Running ETL scripts regularly, which could be computationally intensive, on a shared Vertica cluster might affect the performance or get affected by others' processes.</p>

<p>In short, for these tests, I cannot use the common schema that is shared with others since it might interfere others and/or destroy valuable common data.
Using a Vertica VM as the sandbox test environment helps us minimize interference to and from others' data and activities.</p>

<h3>Single-node VM and KSAFE clause</h3>

<p>I have been using a <strong>single-node</strong> Vertica VM to run tests for sometime. And it works wonderfully for testing purpose, especially when you want to isolate issues, for example, a corner case. The Vertica VM can be downloaded from HP Vertica&rsquo;s support website (NOTE: As of 2016 Jan 1st, the Vertica 7.1 VM is taken down while the Vertica 7.2 VM is not available).</p>

<p>The only minor problem is when we add <code>KSAFE 1</code> in our DDL scripts (i.e., <code>CREATE TABLE</code> statements) for production purposes which gives error on single-node VM when running DDL scripts to set up schema.
The reason is that Vertica database with 1 or 2 hosts cannot be <em>k-safe</em> (i.e., it may lose data if it crashes) and three-node cluster is the minimum requirement to have <code>KSAFE 1</code> in <code>CREATE TABLE</code> statements to work.</p>

<p>Even then, the workaround for running those DDL scripts in tests is easy enough if all DDL scripts are all located in a single folder. The idea is that since <code>KSAFE 1</code> does not affect ETL processes' transform logics, we can remove those KSAFE clauses to set up the test schema and go ahead with our ETL testing. Specifically, in my project, my workflow for ETL testing with <strong>Git</strong> is as follows:</p>

<ul>
<li>Branch the latest code (<code>develop</code> branch) into a temporary branch (e.g., <code>local/develop</code> branch).</li>
<li>Find and remove <code>KSAFE 1</code> in all DDL files (see subsection below).</li>
<li>While still in <code>local/develop</code> branch, commit all these changes in a <strong>single</strong> commit with some unique description (e.g., &ldquo;KSAFE REMOVAL&rdquo;).</li>
<li>Add unit and functional tests to ETL scripts in this branch.</li>
<li>After tests are properly developed and checked-in, reverse the &ldquo;KSAFE REMOVAL&rdquo; commit above.

<ul>
<li>In SourceTree, it could be done by a simple right-click on that commit and selecting &ldquo;Reverse Commit&rdquo;.</li>
</ul>
</li>
<li>Merge <code>local/develop</code> branch into <code>develop</code> branch. You will now have your tests with the latest codes in <code>develop</code> branch.</li>
</ul>


<h4>Find and replace a string in multiple files</h4>

<p>There are times and times again that you find that you have to replace every single occurrences of some string in multiple files with another string. Finding and removing <code>KSAFE 1</code> like the above workflow is an example where &ldquo;removing string&rdquo; is a special case of &ldquo;replacing string&rdquo; with nothing. This operation can be quickly done by the following bash command:</p>

<pre><code>grep -rl match_string your_dir/ | xargs sed -i 's/old_string/new_string/g'
</code></pre>

<p>If you are familiar with bash scripting, the above command is straight forward. This quick explanation is for anyone who does not understand the command:</p>

<ul>
<li><code>grep</code> command finds all files in <code>your_dir</code> directory that contain <code>match_string</code>. <code>-l</code> option makes sure it will return a list of files</li>
<li><code>sed</code> command then execute the replacement regex on all those files. A regex tip: the forward slash <code>/</code> delimiter could be another delimiter (e.g., <code>#</code>). This might be useful if you need to search HTML files.</li>
</ul>


<p>Example: In my case, all the DDL scripts are in multiple sub-directories under <code>tables</code> directory. To find and remove all <code>KSAFE 1</code> occurrences, the command is:</p>

<pre><code>grep -rl 'KSAFE 1' tables | xargs sed -i 's/KSAFE 1//g'
</code></pre>

<p>This will search for the string <code>KSAFE 1</code> in all files in the <code>tables</code> directory and replace <code>KSAFE 1</code> with nothing <code>''</code> for each occurrence of the string in each file.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vertica Tip: Find Empty Tables]]></title>
    <link href="http://tdongsi.github.io/blog/2015/12/18/vertica-tip-find-empty-tables-in-a-schema/"/>
    <updated>2015-12-18T21:39:56-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/12/18/vertica-tip-find-empty-tables-in-a-schema</id>
    <content type="html"><![CDATA[<p>This post is a reminder of using Vertica&rsquo;s system tables for administrating and monitoring our own tables. One common house-cleaning operation when developing/testing in Vertica is to find and drop tables that are empty (truncated) and never used again.</p>

<p>You might ask why the tables are not dropped directly when I truncated the table in the first place. The answer is that all those tables have some specific designs on projection segmentation and partition, and those information will be lost if I drop the tables. These tables are frequently populated with data and cleared for testing purposes, and truncating and inserting with <code>direct</code> <a href="https://my.vertica.com/docs/7.1.x/HTML/Content/Authoring/SQLReferenceManual/Statements/INSERT.htm">hint</a> will give a significant performance boost (see <a href="/blog/2015/12/16/vertica-tip-best-practices/">Best practices</a>).</p>

<h3>v_monitor schema and COLUMN_STORAGE system table</h3>

<p>The <a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/SystemTables/MONITOR/COLUMN_STORAGE.htm">COLUMN_STORAGE system table</a> in <code>v_monitor</code> schema returns the &ldquo;amount of disk storage used by each column of each projection on each node&rdquo;. Therefore, to get the size of each table, you only need to aggregate the <code>used_byte</code> data, grouped by schema name and table name.</p>

<pre><code class="sql Query to list tables' sizes in a schema">select anchor_table_schema, anchor_table_name, sum(used_bytes)
FROM v_monitor.column_storage
where anchor_table_schema = 'some_schema'
group by anchor_table_schema, anchor_table_name
</code></pre>

<p>According to <a href="http://vertica.tips/2014/01/25/table-size/">here</a>, the number from the above query is the <em>compressed</em> size of the Vertica tables. To get the <em>raw</em> size of the tables, which probably only matters for license limit, perform a <em>license audit</em>, and query the system table <code>license_audits</code> in <code>v_catalog</code> schema. However, the most important takeaway is that empty tables will not appear in this <code>COLUMN_STORAGE</code> system table.</p>

<h3>v_catalog schema and TABLES system table</h3>

<p>The <a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/SystemTables/CATALOG/TABLES.htm">TABLES system table</a> is probably more well-known. It contains all the information about all the tables in all the schemas. For example, to list all the tables in some schema:</p>

<pre><code class="sql Query to list all tables in a schema">select table_schema, table_name from tables
where table_schema = 'some_schema'
</code></pre>

<p>Another useful system table in <code>v_catalog</code> schema is <code>USER_FUNCTIONS</code> which lists all user-defined functions and their function signatures in the database.</p>

<h3>Find all the empty (truncated) tables</h3>

<p>Having all the tables in <code>v_catalog.tables</code> table and only non-empty tables in <code>v_monitor.column_storage</code> table, finding empty tables is pretty straight-forward in SQL:</p>

<pre><code class="sql Query to find empty tables in a schema">select table_name
from v_catalog.tables
where table_schema = 'some_schema'
EXCEPT
select anchor_table_name
from v_monitor.column_storage
where anchor_table_schema = 'some_schema' 
</code></pre>

<h3>External Links</h3>

<ol>
<li><a href="http://vertica.tips/2014/01/25/table-size/">Finding table&rsquo;s compressed size</a></li>
<li><a href="http://vertica.tips/2014/01/24/license-audit-utilization-raw-size/">Vertica License audit</a></li>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/SystemTables/MONITOR/COLUMN_STORAGE.htm">COLUMN_STORAGE system table</a></li>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/SystemTables/CATALOG/TABLES.htm">TABLES system table</a></li>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/SystemTables/CATALOG/USER_FUNCTIONS.htm">USER_FUNCTIONS system table</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vertica Tip: Using Vsql CLI]]></title>
    <link href="http://tdongsi.github.io/blog/2015/12/17/vertica-tip-using-vsql/"/>
    <updated>2015-12-17T22:54:07-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/12/17/vertica-tip-using-vsql</id>
    <content type="html"><![CDATA[<h3>Using vsql</h3>

<p>You can connect to Vertica database with username and password. When doing this, note that the password might be seen in the command history.</p>

<pre><code>vsql -h internal.vertica.net -p 5433 -d VMart -U vertica_user -w password 
</code></pre>

<p>Or you can connect to Vertica with Kerberos authentication.</p>

<pre><code>vsql -h internal.vertica.net -p 5433 -d VMart -k KerberosServiceName -K KerberosHostName
</code></pre>

<p>Note that from time to time, you could run into Kerberos GSI failure because the ticket expired. This is how you can renew and extend the ticket: run the following command to refresh Kerberos cache for the headless account <code>vertica_user</code>.</p>

<pre><code class="">kinit -kt /home/path/to/vertica_user.keytab vertica_user@CORP.INTERNAL.NET
</code></pre>

<p>You can also run a single SQL command from command line with <code>-c</code> option or, alternatively, a SQL script file with multiple commands with <code>-f</code> option.
These options can be very useful to automate in shell/python scripts.
Note that you can also parameterize your scripts by using <code>-v</code> option to assign variables inside your SQL scripts.</p>

<h3>Vsql meta commands</h3>

<p>Here is list of commonly used vsql <a href="http://my.vertica.com/docs/7.0.x/HTML/index.htm#Authoring/ProgrammersGuide/vsql/Meta-Commands.htm">meta commands</a>:</p>

<pre><code>dbadmin=&gt; \dt — (list of all tables)
dbadmin=&gt; \dt user* — (list of tables starting with user)
dbadmin=&gt; \d tablename — (describe table)
dbadmin=&gt; \dv — (list of all views)
</code></pre>

<p>Here are the vsql commands to export a file:</p>

<pre><code>dbadmin=&gt; \o sample_users_lists.csv
dbadmin=&gt; \f|
dbadmin=&gt; select * from my_dwh.users limit 20;
dbadmin=&gt; \o
dbadmin=&gt; \q
</code></pre>

<h3>External links</h3>

<ol>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/ConnectingToHPVertica/vsql/CommandLineOptions.htm">Command line options</a></li>
<li><a href="http://my.vertica.com/docs/7.0.x/HTML/index.htm#Authoring/ProgrammersGuide/vsql/Meta-Commands.htm">Meta Commands</a></li>
<li><a href="http://my.vertica.com/docs/7.0.x/HTML/index.htm#Authoring/ProgrammersGuide/vsql/Meta-Commands/TheDPATTERNMeta-commands.htm">Meta Commands: \d[Pattern]</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vertica Tip: Best Practices]]></title>
    <link href="http://tdongsi.github.io/blog/2015/12/16/vertica-tip-best-practices/"/>
    <updated>2015-12-16T23:12:06-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/12/16/vertica-tip-best-practices</id>
    <content type="html"><![CDATA[<p>This post lists some tips and tricks that I learnt when working with Vertica database.</p>

<h3>General Tips and Tricks</h3>

<h4>CREATE (INSERT)</h4>

<ul>
<li><p>If you want to write data directly to disk and bypass memory, then you should include <code>/*+ direct */</code> as a &ldquo;hint&rdquo; in your <code>INSERT</code> statement. This is especially helpful when you are loading data from big files into Vertica. If you don&rsquo;t use <code>/*+ direct */</code>, then <code>INSERT</code> statement first uses memory, which may be more useful when you want to optimally do inserts and run queries.</p></li>
<li><p>ALWAYS include <code>COMMIT</code> in your SQL statements when you are creating or updating Vertica schemas, because there is NO auto commit in Vertica.</p></li>
<li><p>If you are copying a table, <strong>DO NOT</strong> use <code>CREATE TABLE copy AS SELECT * FROM source</code>. This will give you a copy table with default projections and storage policy. Instead, you should use <code>CREATE TABLE</code> statement with the <a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/AdministratorsGuide/Tables/CreatingATableLikeAnother.htm"><code>LIKE existing_table</code> clause</a> and use <code>INSERT /*+ direct */</code> statement. Creating a table with <code>LIKE</code> option replicates the table definition and storage policy associated with the source table, which can make a significant difference in data loading performance. Note that the <code>LIKE</code> clause does not work if the existing source table is a temporary table.</p></li>
</ul>


<pre><code class="sql DO NOT do this">create table to_schema.to_table_name
as select * from from_schema.from_table_name;
</code></pre>

<pre><code class="sql DO this">CREATE TABLE to_schema.to_table_name LIKE from_schema.from_table_name INCLUDING PROJECTIONS;
INSERT /*+ direct */ INTO to_schema.to_table_name SELECT * from from_schema.from_table_name;
</code></pre>

<ul>
<li>Before making a copy of a table, be sure to consider alternatives in order to execute optimal queries: create views, rewrite queries, use sub-queries, limit queries to only a subset of data for analysis.</li>
</ul>


<h4>READ</h4>

<ul>
<li><p>Avoid joining large tables (e.g., > 50M records). Run a <code>count(*)</code> on tables before joining and use <code>MERGE JOIN</code> to optimally join tables. When you use smaller subsets of data, the Vertica Optimizer will pick the <code>MERGE JOIN</code> algorithm instead of the <code>HASH JOIN</code> one, which is less optimal.</p></li>
<li><p>When an approximate value will be enough, Vertica offers an alternative to <code>COUNT(DISTINCT)</code>: <code>APPROXIMATE_COUNT_DISTINCT</code>. This function is recommended when you have a large data set and you do not require an exact count of distinct values: e.g., sanity checks that verify the tables are populated. According to <a href="http://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/AnalyzingData/Optimizations/OptimizingCOUNTDISTINCTByCalculatingApproximateCounts.htm">this documentation</a>, you can get much better performance than <code>COUNT(DISTINCT)</code>. <a href="http://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/Functions/Aggregate/APPROXIMATE_COUNT_DISTINCT.htm">Here</a> is an example of the <code>APPROXIMATE_COUNT_DISTINCT</code> syntax that you should use.</p></li>
</ul>


<h4>UPDATE &amp; DELETE</h4>

<ul>
<li><p>Deletes and updates take exclusive locks on the table. Hence, only one <code>DELETE</code> or <code>UPDATE</code> transaction on that table can be in progress at a time and only when no <code>INSERTs</code> are in progress. Deletes and updates on different tables can be run concurrently.</p></li>
<li><p>Try to avoid <code>DELETE</code> or <code>UPDATE</code> as much as you can, especially on shared Vertica databases. Instead, it may work better to move the data you want to update to a new temporary table, work on that copy, drop the original table, and rename the temporary table with the original table name. For example:</p></li>
</ul>


<pre><code class="sql">CREATE temp_table LIKE src_table INCLUDING PROJECTIONS;
INSERT INTO temp_table (SELECT statement based on the updated data or the needed rows);
DROP TABLE src_table;
ALTER TABLE temp_table RENAME TO src_table;
</code></pre>

<ul>
<li>Delete from tables marks rows with delete vectors and stores them so data can be rolled back to a previous epoch. The data must be eventually purged before the database can reclaim disk space.</li>
</ul>


<h3>Query plan</h3>

<p>A query plan is a sequence of step-like paths that the HP Vertica cost-based query optimizer selects to access or alter information in your HP Vertica database. You can get information about query plans by prefixing the SQL query with the <code>EXPLAIN</code> command.</p>

<pre><code class="sql EXPLAIN statement">EXPLAIN SELECT customer_name, customer_state FROM customer_dimension
WHERE customer_state in ('MA','NH') AND customer_gender = 'Male'     
ORDER BY customer_name LIMIT 10;
</code></pre>

<p>The output from a query plan is presented in a tree-like structure, where each step path represents a single operation in the database that the optimizer uses for its execution strategy. The following example output is based on the previous query:</p>

<pre><code class="bash Query Plan description">EXPLAIN SELECT
customer_name,
customer_state
FROM customer_dimension
WHERE customer_state in ('MA','NH')
AND customer_gender = 'Male'
ORDER BY customer_name
LIMIT 10;
Access Path:
+-SELECT  LIMIT 10 [Cost: 370, Rows: 10] (PATH ID: 0)
|  Output Only: 10 tuples
|  Execute on: Query Initiator
| +---&gt; SORT [Cost: 370, Rows: 544] (PATH ID: 1)
| |      Order: customer_dimension.customer_name ASC
| |      Output Only: 10 tuples
| |      Execute on: Query Initiator
| | +---&gt; STORAGE ACCESS for customer_dimension [Cost: 331, Rows: 544] (PATH ID: 2) 
| | |      Projection: public.customer_dimension_DBD_1_rep_vmartdb_design_vmartdb_design_node0001
| | |      Materialize: customer_dimension.customer_state, customer_dimension.customer_name
| | |      Filter: (customer_dimension.customer_gender = 'Male')
| | |      Filter: (customer_dimension.customer_state = ANY (ARRAY['MA', 'NH']))
| | |      Execute on: Query Initiator
</code></pre>

<p>If you want to understand the details of the query plan, observe the real-time flow of data through the plan to identify possible query bottlenecks, you can:</p>

<ol>
<li>query the <a href="https://my.vertica.com/docs/7.1.x/HTML/Content/Authoring/SQLReferenceManual/SystemTables/MONITOR/QUERY_PLAN_PROFILES.htm">V_MONITOR.QUERY_PLAN_PROFILES</a> system table.</li>
<li>review <a href="https://my.vertica.com/docs/7.1.x/HTML/Content/Authoring/AdministratorsGuide/Profiling/ProfilingQueryPlanProfiles.htm">Profiling Query Plans</a>.</li>
<li>use <a href="https://my.vertica.com/docs/7.1.x/HTML/Content/Authoring/SQLReferenceManual/Statements/PROFILE.htm">PROFILE</a> statement to view further detailed analysis of your query.</li>
</ol>


<h3>External Links</h3>

<ol>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm">Vertica documentation</a></li>
<li><a href="http://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/SQLReferenceManual/Functions/Aggregate/APPROXIMATE_COUNT_DISTINCT.htm">APPROXIMATE_COUNT_DISTINCT</a></li>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/AdministratorsGuide/Tables/CreatingATableLikeAnother.htm">Create a Table Like Another</a></li>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/Content/Authoring/SQLReferenceManual/SystemTables/MONITOR/QUERY_PLAN_PROFILES.htm">V_MONITOR.QUERY_PLAN_PROFILES</a> system table.</li>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/Content/Authoring/AdministratorsGuide/Profiling/ProfilingQueryPlanProfiles.htm">Profiling Query Plans</a>.</li>
<li><a href="https://my.vertica.com/docs/7.1.x/HTML/Content/Authoring/SQLReferenceManual/Statements/PROFILE.htm">PROFILE</a> statement.</li>
</ol>

]]></content>
  </entry>
  
</feed>
