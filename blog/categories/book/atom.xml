<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Book | Personal Programming Notes]]></title>
  <link href="http://tdongsi.github.io/blog/categories/book/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/"/>
  <updated>2021-03-11T07:26:03-08:00</updated>
  <id>http://tdongsi.github.io/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hive: Introduction]]></title>
    <link href="http://tdongsi.github.io/blog/2015/11/22/programming-hive-chapter-1/"/>
    <updated>2015-11-22T17:22:51-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/11/22/programming-hive-chapter-1</id>
    <content type="html"><![CDATA[<!---
"Chapter 1: Introduction" of the "Programming Hive" book.
-->


<p>Experimenting Hive with <a href="/blog/2015/11/20/wordcount-sample-in-cloudera-quickstart-vm/">Cloudera Quickstart VM</a>.</p>

<!--more-->


<h3>Introduction</h3>

<p>Hive provides a SQL dialect, called Hive Query Language (HiveQL or HQL) for querying data stored in a Hadoop cluster. SQL knowledge is widespread for a reason; it&rsquo;s an effective, reasonably intuitive model for organizing and using data. Therefore, Hive helps lower the barrier, making transition to Hadoop from traditional relational databases easier for database users such as business analysts.</p>

<p>Note that Hive is more suited for data warehouse applications, where data is relatively static and fast response time is not required. For example, a simple query such as <code>select count(*) from my_table</code> can take several seconds for a very small table (mostly due to startup overhead for MapReduce jobs). Hive is a heavily batch-oriented system: in addition to large startup overheads, it neither provides record-level update, insert, or delete nor transactions. In short, Hive is not a full database (hint: check HBase).</p>

<p>HiveQL does not conform to the ANSI SQL standard (not many do), but it is quite close to MySQL dialect.</p>

<h3>Hive within the Hadoop Ecosystem</h3>

<p>A basic understanding of Hadoop and MapReduce can help you to understand and appreciate how Hive works. Simple examples such as WordCount in my <a href="/blog/2015/11/21/explaining-wordcount-example/">last post</a> can be very involving when using the Hadoop Java API. The API requires Java developers to manage many low-level details, repetitive wiring to/from Mappers and Reducers. The WordCount example&rsquo;s Java implementation can be found <a href="https://wiki.apache.org/hadoop/WordCount">here</a>.</p>

<p>Hive not only eliminates advanced, sometimes repetitive Java coding but also provides a familiar interface to those who know SQL. Hive lets you complete a lot of work with relatively little effort. For example, the same WordCount example in HiveQL can be as simple as:</p>

<pre><code class="sql WordCount example in HiveQL">CREATE TABLE docs (line STRING);

/* Load text files into TABLE docs: each line as a row */
LOAD DATA INPATH 'wordcount.txt' OVERWRITE INTO TABLE docs;

CREATE TABLE word_counts AS
SELECT word, count(1) AS count
FROM
   -- explode will return rows of tokens
  (SELECT explode(split(line, '\s')) AS word
   FROM docs) w
GROUP BY word
ORDER BY word;
</code></pre>

<!--
In the remaining sections of Chapter 1, the authors also discuss various related Hadoop projects such as Pig, Hue, HBase, Spark, Storm, Kafka, etc.
-->

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Overview of MapReduce: Explaining WordCount Example]]></title>
    <link href="http://tdongsi.github.io/blog/2015/11/21/explaining-wordcount-example/"/>
    <updated>2015-11-21T02:37:20-08:00</updated>
    <id>http://tdongsi.github.io/blog/2015/11/21/explaining-wordcount-example</id>
    <content type="html"><![CDATA[<p>MapReduce is a programming framework that decomposes large data processing jobs into individual tasks that can be executed in parallel across a cluster of servers.
The name MapReduce comes from the fact that there are two fundamental data transformation operations: <em>map</em> and <em>reduce</em>.
These MapReduce operations would be more clear if we walk through a simple example, such as WordCount in my last <a href="/blog/2015/11/20/wordcount-sample-in-cloudera-quickstart-vm/">post</a>.
The process flow of the WordCount example is shown below:</p>

<!---
(from [here](https://www.safaribooksonline.com/library/view/programming-hive/9781449326944/ch01.html)):

![Process Flow of WordCount Example](https://www.safaribooksonline.com/library/view/programming-hive/9781449326944/httpatomoreillycomsourceoreillyimages1321235.png)
-->


<p><img class="center" src="/images/hive/wordcount.png" title="Process Flow of WordCount Example" ></p>

<!--more-->


<p>The fundamental data structure for input and output in MapReduce is the key-value pair. When starting the WordCount example, the Mapper processes the input documents line by line, with the key being the character offset into the document and the value being the line of text.</p>

<p>A <strong>map</strong> operation converts input key-values pairs from one form to another. In WordCount, the key (character offset) is discarded but it may not be always the case. The value (the line of text) is normalized (e.g., converted to lower case) and tokenized into words, using some technique such as splitting on whitespace. In this way, “HADOOP” and “Hadoop” will be counted as the same word. For each word in the line, the Mapper outputs a key-value pair, with the word as the key and the number 1 as the value.</p>

<p>Next is the <strong>shuffling</strong> phase. Hadoop sorts the key-value pairs by key and it “shuffles” all pairs with the same key to the same Reducer. In the WordCount example, each Reducer may get some range of keys, i.e. a group of words/tokens.</p>

<p>A <strong>reduce</strong> operation converts the collection for each key in input key-value pairs to another smaller collection (or a value when the collection has a single element). In WordCount, the input key is one of the words found and the value will be a collection of all the counts for that word. The Reducers add all the counts in the value collection and the final output are key-value pairs consisting of each word and the count for that word.</p>

<p>The three phases of processing in WordCount example with their input and output key-value pairs are summarized in the table below. Note that the input and output key-value pairs can be very different for each phase, not only in value but also in type.</p>

<table>
<thead>
<tr>
<th> </th>
<th> Mapper </th>
<th> Shuffling </th>
<th> Reducer </th>
</tr>
</thead>
<tbody>
<tr>
<td> <strong>Input</strong> </td>
<td> <code>(offset, text_line)</code> </td>
<td> Multiple <code>(token,1)</code> </td>
<td> <code>(token,[1,1,1,...])</code> </td>
</tr>
<tr>
<td> <strong>Processing</strong> </td>
<td> Discard the key <code>offset</code>. <br> Normalize and tokenize <code>text_line</code>.</td>
<td> Move <code>(token,1)</code>with same <code>token</code> to same Reducer </td>
<td> Sum all elements in collection </td>
</tr>
<tr>
<td> <strong>Output</strong> </td>
<td> Multiple <code>(token,1)</code> </td>
<td> Sorted <code>(token,[1,1,1,...])</code> </td>
<td> <code>(token, count)</code> </td>
</tr>
</tbody>
</table>


<p><br></p>
]]></content>
  </entry>
  
</feed>
