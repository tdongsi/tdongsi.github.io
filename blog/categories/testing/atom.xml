<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Testing | Personal Programming Notes]]></title>
  <link href="http://tdongsi.github.io/blog/categories/testing/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/"/>
  <updated>2017-08-21T16:05:10-07:00</updated>
  <id>http://tdongsi.github.io/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Unit Tests Pass on Local but Fail on CI]]></title>
    <link href="http://tdongsi.github.io/blog/2016/06/30/java-intermittent-test-failures/"/>
    <updated>2016-06-30T17:51:13-07:00</updated>
    <id>http://tdongsi.github.io/blog/2016/06/30/java-intermittent-test-failures</id>
    <content type="html"><![CDATA[<p>We have all seen it before: intermittent unit test failures.
It could be agonizing that unit tests pass locally, but then fail in the Jenkins unit test build.</p>

<!--more-->


<p>In our experience, one of the most common causes is:
<strong>static initialization code that dynamically sets a static member variable from a config file value.</strong></p>

<p>What happens locally?
If you’re running from the command line, you probably have some environment variables set.
These allow some ConfigHelper class to find the resource properties files and load them.
In the end, code that looks like the following often ends up succeeding:</p>

<pre><code class="java DbQueue class">private static final String MY_CONFIG = ConfigHelper.getBoolean("config_key", false);
</code></pre>

<p>But the unit tests on the CI server run without being set up for a Tomcat application server run.
Instead, they run using some mock framework such as JMockit.
Mocking in this scenario is a good, desirable thing.
However, it also means that code like that ends up failing to find those resources.
In the example above, the class <code>DbQueue</code>&rsquo;s static code was invoked <strong>even though the class itself has been mocked out</strong>.
And very often, classes like that throw some misleading exceptions, especially when trying to load and convert to a numeric value from a resource.</p>

<p>So, how do we fix it?
How do we prevent that class static member initialization code from being invoked in Jenkins test build?
The answer is when we mock the class in JMockit using the <code>@Mocked</code> annotation, we can provide the <code>stubOutClassInitialization=true</code> parameter, like this:</p>

<pre><code class="java Mock with JMockit">public class MyTest {
    @Mocked( stubOutClassInitialization = true )
    DbQueue queue;

    ...
}
</code></pre>

<p>That will prevent the static code in the class <code>DbQueue</code> from running in Jenkins unit test builds.
The additional benefit of doing this <em>correctly</em> and <em>completely</em> is that we’ll be able to run our unit tests from inside Eclipse WITHOUT setting the <code>–DSBNHOME=</code> environment variable and the test will still complete as desired.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java: Unit Test Performance]]></title>
    <link href="http://tdongsi.github.io/blog/2016/06/06/java-unit-test-performance/"/>
    <updated>2016-06-06T22:47:42-07:00</updated>
    <id>http://tdongsi.github.io/blog/2016/06/06/java-unit-test-performance</id>
    <content type="html"><![CDATA[<p>According to <a href="https://www.youtube.com/watch?v=wEhu57pih5w">this</a>, the right way of automated testing is that we have lots of unit tests as majority of our overall automated tests, supplemented by a smaller set of functional tests and even fewer sets of integration tests (a.k.a., Test Automation Pyramid).
However, for that strategy to work, we should pay attention to unit test performance.
It is not productive for us developers to wait 30+ minutes to run unit tests locally, especially when we have multiple check-ins per day.
In addition, the runtime will get compounded as we add more unit tests.
Here, I list out few commonly observed mistakes to avoid and suggestions that frequently improve Java unit test performance.</p>

<!--more-->


<p>1) Do NOT add loggings/printing to your tests.
Use TestNG assertions instead of checking screen output.
Remove from the test classes all the <code>System.out.println</code> statements (that we might add when we start writing unit tests).
The logs don&rsquo;t matter when we&rsquo;re running in parallel.
Moreover, it could add 5-10 minutes to the build time, regardless of running in sequential or parallel.</p>

<p>2) Another common mistake is to override the default <code>System.out</code> by calling <code>System.setOut(PrintStream)</code> and verify by asserting against log statements.
This tactic is often used to verify expected method invocations, which will subsequently generate some specific log entries.
For such behavior testing, consider using <a href="https://jmockit.googlecode.com/svn-history/r2056/trunk/www/tutorial/BehaviorBasedTesting.html">Jmockit Verifications</a> instead of depending on output of logs generated.</p>

<p>3) Mock logging and config classes if applicable.
Otherwise, we might encountered errors like &ldquo;Exception encountered, logging will be disabled&rdquo;, probably thrown by JMockit.
If there is any static initialization block in the mocked class for logging and configuration purposes, consider using <code>(stubOutClassInitialization = true)</code> (see <a href="/blog/2016/06/30/java-intermittent-test-failures/">this</a>).</p>

<p>4) Choosing the right parallel execution settings can substantially improve the execution time.
However, for parallel test runs, consider splitting big test classes (> 100 tests) that are taking much longer than others.
As we are running test classes in parallel across multiple JVMs, it is often the case that all JVMs are shut down except for one or two which are running some big test classes.
Splitting those classes into multiple smaller classes will distribute the load equally across multiple JVMs.</p>

<p>5) Out of all the <code>maven-surefire</code> options for running tests in parallel, the one that worked considering JMockit limitations with parallel execution (and our test structure) are as below:</p>

<pre><code class="xml Maven-surefire options">&lt;parallel&gt;classes&lt;/parallel&gt;
&lt;forkCount&gt;${forkCount}&lt;/forkCount&gt;
&lt;reuseForks&gt;false&lt;/resuseForks&gt;
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Use JMockit ONLY]]></title>
    <link href="http://tdongsi.github.io/blog/2016/02/21/java-1-single-mocking-framework/"/>
    <updated>2016-02-21T12:20:46-08:00</updated>
    <id>http://tdongsi.github.io/blog/2016/02/21/java-1-single-mocking-framework</id>
    <content type="html"><![CDATA[<p>A more general title would be &ldquo;Use a single mocking framework ONLY&rdquo;.
Personally, it just means that I should defer learning Wiremock&rsquo;s advanced features and learn JMockit (specifically JMockit 1.2x) which is recently adopted at work.</p>

<p>We know that mocking is a critical enabler for unit tests and automated functional tests that don’t require networks and databases and can complete in reasonable time.
Mocking tools work by integrating with and replacing critical parts of the Java Class Loader.
It means that having multiple mocking tools in use will lead to those tools contend to replace the class loader in JVM.
This will lead to complex and unexpected consequences and, as a result, random test failures and unreliable tests.
For example, we might have tests that work fine locally but start failing when running in combination with others (using other mocking tools) because different mocking frameworks take over the class loader in different order or in different ways.</p>

<p>To fix that, we need to standardize and settle on a single mocking framework for an organization or a project.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mocking Current Date and Time in Python]]></title>
    <link href="http://tdongsi.github.io/blog/2016/01/27/python-1-mock-datetime-freezegun/"/>
    <updated>2016-01-27T17:36:53-08:00</updated>
    <id>http://tdongsi.github.io/blog/2016/01/27/python-1-mock-datetime-freezegun</id>
    <content type="html"><![CDATA[<p>In this post, we looks into unit testing some calendar utilities in Python.
A requirement for unit testing those is to &ldquo;mock&rdquo; current date and time, i.e., overriding those returned by <code>today</code> and <code>now</code> methods with some specific date values (e.g., a date in future).</p>

<!--more-->


<h3>Calendar types</h3>

<p>What is a big deal about calendar utitlities?
There are surprisingly many types of calendar.
Some of them are: regular calendar, lunar calendar, fiscal calendar, retail calendar.
See <a href="https://github.com/tdongsi/calendars">here</a> for more information of each calendar type.
Out of the above calendar types, retail calendar seems to have more complex rules.
However, this calendar type is frequently used in industries like retail and manufacturing for ease of planning around it.</p>

<h3>Mocking current time in Python</h3>

<p>Due to retail calendar&rsquo;s desirable characteristics, we may have code that work with retail calendars in commercial applications eventually.
I ended up working with a utility Python module for retail calendar with functions which return values based on current time/date.
For example, a utility function to check if a given date is in the current 544 year works like this:</p>

<pre><code class="python Original version">def is_current_year_544(given_date):
    my_today = datetime.date.today()
    if year_start_544(my_today) &lt;= given_date &lt;= year_end_544(my_today):
        return "Y"
    else:
        return "N"
</code></pre>

<p>Some utility functions in that module are even more complicated than this example function.
For those, I think calling <code>today</code> or <code>now</code> inside those functions is a bad design.
They are essentially another <em>variable</em> in those functions (i.e., when do you run?), and it is better to expose that variable as an input parameter.
In addition, being able to specify what &ldquo;today&rdquo; or &ldquo;now&rdquo; value is will make automated unit testing easier.
For example, I want to know how my Python programs work if it runs on a particular date, such as end of retail year July 29, 2006.
A probably better, more testable function would be something like this.</p>

<pre><code class="python More desirable">def is_current_year_544(given_date, run_date = datetime.date.today()):
    if year_start_544(run_date) &lt;= given_date &lt;= year_end_544(run_date):
        return "Y"
    else:
        return "N"
</code></pre>

<p>However, in reality, you sometimes have to live with the original utility Python module.
Then, the workaround for unit testing is to &ldquo;mock&rdquo; current date and time, i.e., overriding those returned by <code>today</code> and <code>now</code> methods with some specific date values.
In Python, it can be done by using some mocking framework, such as illustrated <a href="http://www.voidspace.org.uk/python/mock/examples.html#partial-mocking">here</a>.
Fortunately, my life was made even easier with <a href="https://github.com/spulec/freezegun"><code>freezegun</code> library</a>.
To install <code>freezegun</code> on Mac OSX, simply run</p>

<pre><code class="plain   ">pip install freezegun
</code></pre>

<p>Using this <code>freezegun</code> library, I can easily specify my &ldquo;current date&rdquo; as &ldquo;July 29, 2006&rdquo; by adding the following decorator with some string &ldquo;2006-07-29&rdquo; for that date.</p>

<pre><code class="python Unit test with mocking">    @freeze_time("2006-07-29")
    def test_year544_end(self):
        """
        Mock today() at 2006-07-29
        """
        self._verify_544_methods()
</code></pre>

<p>For full usage of <code>freezegun</code>, refer to its <a href="https://github.com/spulec/freezegun">quickstart guide</a>.
It should be noted that <code>freezegun</code> can mock <code>datetime</code> calls from other modules and it works great for testing with <code>datetime</code> calls.
However, you might encounter some occasional failures in your unit tests when working with <code>time</code> module.
From my personal experience, in those cases, note that time zones must be accounted for when mocking with <code>time</code> module by specifying <code>tz_offset</code> in the decorator <code>freeze_time</code>.</p>

<h3>External Links</h3>

<ul>
<li><a href="https://github.com/spulec/freezegun">freeze_gun</a></li>
<li><a href="https://en.wikipedia.org/wiki/4%E2%80%934%E2%80%935_calendar">Retail Calendar</a></li>
<li><a href="http://www.staff.science.uu.nl/~gent0113/calendar/isocalendar.htm">ISO Calendar</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Virtual Machine for ETL Testing]]></title>
    <link href="http://tdongsi.github.io/blog/2016/01/10/find-and-replace-a-string-in-multiple-files/"/>
    <updated>2016-01-10T23:49:15-08:00</updated>
    <id>http://tdongsi.github.io/blog/2016/01/10/find-and-replace-a-string-in-multiple-files</id>
    <content type="html"><![CDATA[<p>When developing data-warehouse solutions in Vertica, you want to set up some test environment.
Ideally, you should have separate schema for each developer.
However, it is usually NOT possible in my experience: developers and test engineers have to share very few schemas in development environment.
The explanation that I usually get is that having a schema for each developer will not scale in database maintenance and administration, and there are likely some limits in Vertica&rsquo;s commercial license.
If that is the case, I recommend that we look into using Vertica Community Edition on <strong>Virtual Machines (VMs)</strong> for sandbox test environment, as a cheap alternative.</p>

<!--more-->


<h3>Vertica Virtual Machine as sandbox test environment</h3>

<p>Are VMs really necessary in data-warehouse testing? When testing Extract-Transform-Load (ETL) processes, I find that many of test cases require regular set-up and tear-down, adding mock records to force rare logical branches and corner cases, and/or running ETLs multiple times to simulate daily runs of those processes.
Regular tear-down requires dropping multiple tables regularly, which requires much greater care and drains much mental energy when working with others' data and tables.
Similarly, adding mock records into some commonly shared tables might affect others when they assume the data is production-like.
Running ETL scripts regularly, which could be computationally intensive, on a shared Vertica cluster might affect the performance or get affected by others' processes.
In short, for these tests, I cannot use the common schema that is shared with others since it might interfere others and/or destroy valuable common data.
Using a Vertica VM as the sandbox test environment helps us minimize interference to and from others' data and activities.</p>

<h3>Single-node VM and KSAFE clause</h3>

<p>I have been using a <strong>single-node</strong> Vertica VM to run tests for sometime. And it works wonderfully for testing purpose, especially when you want to isolate issues, for example, a corner case. The Vertica VM can be downloaded from HP Vertica&rsquo;s support website (NOTE: As of 2016 Jan 1st, the Vertica 7.1 VM is taken down while the Vertica 7.2 VM is not available).</p>

<p>The only minor problem is when we add <code>KSAFE 1</code> in our DDL scripts (i.e., <code>CREATE TABLE</code> statements) for production purposes. This gives error on single-node VM when running DDL scripts to set up schema.
The reason is that Vertica database with one or two hosts cannot be <em>k-safe</em> (i.e., it may lose data if it crashes) and three-node cluster is the minimum requirement to have <code>KSAFE 1</code> in <code>CREATE TABLE</code> statements to work.</p>

<p>Even then, the workaround for running those DDL scripts in tests is easy enough if all DDL scripts are all located in a single folder. The idea is that since <code>KSAFE 1</code> does not affect ETL processes' transform logics, we can remove those KSAFE clauses to set up the test schema and go ahead with our ETL testing. Specifically, in my project, my workflow for ETL testing with <strong>Git</strong> is as follows:</p>

<ul>
<li>Branch the latest code (<code>develop</code> branch) into a temporary branch (e.g., <code>local/develop</code> branch).</li>
<li>Find and remove <code>KSAFE 1</code> in all DDL files (see subsection below).</li>
<li>While still in <code>local/develop</code> branch, commit all these changes in a <strong>single</strong> commit with some unique description (e.g., &ldquo;KSAFE REMOVAL&rdquo;).</li>
<li>Add unit and functional tests to ETL scripts in this branch.</li>
<li>After tests are properly developed and checked-in, reverse the &ldquo;KSAFE REMOVAL&rdquo; commit above.

<ul>
<li>In SourceTree, it could be done by a simple right-click on that commit and selecting &ldquo;Reverse Commit&rdquo;.</li>
</ul>
</li>
<li>Merge <code>local/develop</code> branch into <code>develop</code> branch (create a pull request if needed). You will now have your tests with the latest codes in <code>develop</code> branch.</li>
</ul>


<h4>Find and replace a string in multiple files</h4>

<p>There are times and times again that you find that you have to replace every single occurrences of some string in multiple files with another string. Finding and removing <code>KSAFE 1</code> like the above workflow is an example where &ldquo;removing string&rdquo; is a special case of &ldquo;replacing string&rdquo; with nothing. This operation can be quickly done by the following bash command:</p>

<pre><code>grep -rl match_string your_dir/ | xargs sed -i 's/old_string/new_string/g'
</code></pre>

<p>If you are familiar with bash scripting, the above command is straight forward. This quick explanation is for anyone who does not understand the command:</p>

<ul>
<li><code>grep</code> command finds all files in <code>your_dir</code> directory that contain <code>match_string</code>. <code>-l</code> option makes sure it will return a list of files</li>
<li><code>sed</code> command then execute the replacement regex on all those files. A regex tip: the forward slash <code>/</code> delimiter could be another delimiter (e.g., <code>#</code>). This might be useful if you need to search HTML files.</li>
</ul>


<p>Example: In my case, all the DDL scripts are in multiple sub-directories under <code>tables</code> directory. To find and remove all <code>KSAFE 1</code> occurrences, the command is:</p>

<pre><code>grep -rl 'KSAFE 1' tables | xargs sed -i 's/KSAFE 1//g'
</code></pre>

<p>This will search for the string <code>KSAFE 1</code> in all files in the <code>tables</code> directory and replace <code>KSAFE 1</code> with nothing <code>''</code> for each occurrence of the string in each file.</p>
]]></content>
  </entry>
  
</feed>
